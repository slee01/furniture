Script started on 2020년 07월 07일 (화) 오후 02시 55분 25초
(base) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num-steps 2048 --nu m-processes 1 \
> --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 \
> --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits \
> --gail --extract-obs --gail-algo standard --expert-algo ppo \
> --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior \
> --save-date 200707 --eval-interval 1^C
(base) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num-steps 2048 --numm-processes 1 --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits --gail --extract-obs --gail-algoo standard --expert-algo ppo --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior \[A[A(base) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ [K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C^C
(base) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ clearn[K[Kr
[H[2J[3J(base) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num-steps 2048 --nu m-processes 1 \
> --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 \
> --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits \
> --gail --extract-obs --gail-algo standard --expert-algo ppo \
> --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior \
> --save-date 200707 --eval-interval 1
Traceback (most recent call last):
  File "main.py", line 4, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
(base) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ conda ac[Kctivate multi-task
(multi-task) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ conda activate multi-task[Kconda activate multi-taskpython main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num-steps 20488 --num-processes 1 --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits --gail --extract-obs --gaiil-algo standard --expert-algo ppo --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior --save-date 200707 --eval-interval 1
Logging to /tmp/openai-2020-07-07-14-57-04-706392
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
************************************************************************************************
0. task:  MountainToyCar-v1  device:  cuda:0
   observation shape:  (1,)
1. policy_lr & task_lr 0.0003, b_lr: 0.0001, discr_lr: 0.0001, postr_lr: 0.0001
2. policy: hierarchical_policy
   action distribution is Categorical
3. algorithm: ppo
4. posterior: true
   latent distribution is DiagGaussian
5. discriminator: standard_discriminator
6. task_transition_model: true
   latent distribution is DiagGaussian
************************************************************************************************
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-10.      -2.0196  -1.2432 ...  -0.0379  -0.0379  -0.0379]]
 postr_rewards:  [[ 9.8921  1.9098  1.1232 ... -0.0605 -0.0787 -0.2125]]
 discr_rewards:  [[ 0.0157  0.0349  0.0499 ... -0.004   0.0182 -0.0031]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0922 -0.0749 -0.0701 ... -0.1025 -0.0984 -0.2534]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 0, num timesteps 2048, FPS 36 
 Last 5 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.7632
                action_loss  -0.0044
                    bc_loss   0.0000
               dist_entropy   1.0947
         discriminator_loss   2.1020
                  gail_loss   1.7158
                  grad_loss   0.3862
                    ib_loss  -0.1968
                  task_loss   0.0523
                       beta   0.0000
             posterior_loss   0.5087
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0379 -0.0379 -0.0379 ... -0.0387 -0.0387 -0.0387]]
 postr_rewards:  [[-0.0715 -0.0662 -0.1455 ... -0.1272 -0.1424 -0.1375]]
 discr_rewards:  [[ 0.006   0.0498 -0.0102 ...  0.0164  0.0678  0.0132]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1033 -0.0543 -0.1936 ... -0.1495 -0.1133 -0.163 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 1, num timesteps 4096, FPS 59 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.3362
                action_loss  -0.0067
                    bc_loss   0.0000
               dist_entropy   1.0778
         discriminator_loss   1.8690
                  gail_loss   1.7730
                  grad_loss   0.0961
                    ib_loss  -0.1970
                  task_loss   0.7131
                       beta   0.0000
             posterior_loss   0.5205
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0387 -0.0387 -0.0387 ... -0.0391 -0.0391 -0.0391]]
 postr_rewards:  [[-0.0722 -0.056  -0.1051 ... -0.0652 -0.0843 -0.054 ]]
 discr_rewards:  [[ 0.057   0.0196 -0.0046 ... -0.0067  0.0034  0.0533]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0539 -0.0751 -0.1484 ... -0.111  -0.12   -0.0399]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 2, num timesteps 6144, FPS 75 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.0844
                action_loss  -0.0036
                    bc_loss   0.0000
               dist_entropy   1.0443
         discriminator_loss   1.8862
                  gail_loss   1.7987
                  grad_loss   0.0875
                    ib_loss  -0.1966
                  task_loss   0.8480
                       beta   0.0000
             posterior_loss   0.4919
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0391 -0.0391 -0.0391 ... -0.0393 -0.0393 -0.0393]]
 postr_rewards:  [[-0.0541 -0.2008 -0.0585 ... -0.0783 -0.168  -0.1769]]
 discr_rewards:  [[-0.0298  0.0403 -0.02   ... -0.0164 -0.011  -0.0013]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.123  -0.1995 -0.1175 ... -0.1339 -0.2183 -0.2175]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 3, num timesteps 8192, FPS 86 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.9659
                action_loss  -0.0138
                    bc_loss   0.0000
               dist_entropy   0.9606
         discriminator_loss   1.7944
                  gail_loss   1.7227
                  grad_loss   0.0717
                    ib_loss  -0.1968
                  task_loss   0.5729
                       beta   0.0000
             posterior_loss   0.4890
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0393 -0.0393 -0.0393 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[-0.2167 -0.0671 -0.0527 ... -0.0829 -0.1023 -0.0788]]
 discr_rewards:  [[ 0.0478 -0.0509  0.0181 ...  0.0146  0.0299  0.0473]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2081 -0.1573 -0.0739 ... -0.1077 -0.1118 -0.0709]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 4, num timesteps 10240, FPS 95 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.9008
                action_loss  -0.0106
                    bc_loss   0.0000
               dist_entropy   0.8552
         discriminator_loss   1.7895
                  gail_loss   1.7358
                  grad_loss   0.0537
                    ib_loss  -0.1969
                  task_loss   0.4478
                       beta   0.0000
             posterior_loss   0.5224
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0682 -0.1017 -0.0886 ... -0.0607 -0.0531 -0.0526]]
 discr_rewards:  [[ 0.0639  0.0358  0.0693 ... -0.068   0.0424  0.0707]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.0437 -0.1054 -0.0587 ... -0.1682 -0.0501 -0.0213]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 5, num timesteps 12288, FPS 102 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.8783
                action_loss  -0.0070
                    bc_loss   0.0000
               dist_entropy   0.7443
         discriminator_loss   1.7433
                  gail_loss   1.7019
                  grad_loss   0.0414
                    ib_loss  -0.1965
                  task_loss   0.4962
                       beta   0.0000
             posterior_loss   0.5251
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0596 -0.0564 -0.0841 ... -0.0647 -0.0542 -0.0581]]
 discr_rewards:  [[ 0.0778 -0.1014 -0.0365 ...  0.0611  0.0474 -0.0302]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.0213 -0.1973 -0.1601 ... -0.0431 -0.0463 -0.1278]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 6, num timesteps 14336, FPS 107 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.8722
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.7019
         discriminator_loss   1.6592
                  gail_loss   1.6313
                  grad_loss   0.0279
                    ib_loss  -0.1968
                  task_loss   0.5544
                       beta   0.0000
             posterior_loss   0.5184
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2547 -0.3044 -0.092  ... -0.1353 -0.104  -0.0684]]
 discr_rewards:  [[-0.0117  0.0035  0.0392 ... -0.1186  0.0238  0.0382]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.3059 -0.3404 -0.0923 ... -0.2934 -0.1197 -0.0697]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 7, num timesteps 16384, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.9818
                action_loss  -0.0066
                    bc_loss   0.0000
               dist_entropy   0.6386
         discriminator_loss   1.5495
                  gail_loss   1.5249
                  grad_loss   0.0245
                    ib_loss  -0.1968
                  task_loss   0.6143
                       beta   0.0000
             posterior_loss   0.5330
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[-0.0647 -0.1416 -0.0651 ... -0.1109 -0.0914 -0.0556]]
 discr_rewards:  [[-0.0147 -0.0412  0.011  ...  0.0263 -0.055  -0.0152]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1189 -0.2223 -0.0936 ... -0.124  -0.1857 -0.1102]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 8, num timesteps 18432, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.9573
                action_loss  -0.0041
                    bc_loss   0.0000
               dist_entropy   0.6050
         discriminator_loss   1.4502
                  gail_loss   1.4322
                  grad_loss   0.0180
                    ib_loss  -0.1971
                  task_loss   0.5645
                       beta   0.0000
             posterior_loss   0.5053
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0393 -0.0393 -0.0393]]
 postr_rewards:  [[-0.0542 -0.1563 -0.1198 ... -0.0567 -0.0572 -0.0792]]
 discr_rewards:  [[-0.0765 -0.0099 -0.1429 ... -0.007   0.0426  0.0397]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.17   -0.2055 -0.3021 ... -0.103  -0.0539 -0.0789]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 9, num timesteps 20480, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.0358
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.5711
         discriminator_loss   1.3703
                  gail_loss   1.3538
                  grad_loss   0.0165
                    ib_loss  -0.1971
                  task_loss   0.6555
                       beta   0.0000
             posterior_loss   0.5065
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0393 -0.0393 -0.0393 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[-0.1208 -0.3034 -0.0582 ... -0.1154 -0.0584 -0.0964]]
 discr_rewards:  [[ 0.0176 -0.0536  0.0433 ... -0.005  -0.0503 -0.0175]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1425 -0.3963 -0.0542 ... -0.1598 -0.148  -0.1533]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 10, num timesteps 22528, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.2730
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.5981
         discriminator_loss   1.2725
                  gail_loss   1.2555
                  grad_loss   0.0169
                    ib_loss  -0.1968
                  task_loss   0.7970
                       beta   0.0000
             posterior_loss   0.5226
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[-0.2434 -0.053  -0.0982 ... -0.0963 -0.1027 -0.1301]]
 discr_rewards:  [[-0.0235  0.0139 -0.0398 ... -0.0247 -0.0306 -0.0356]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.3064 -0.0785 -0.1773 ... -0.1604 -0.1728 -0.2051]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 11, num timesteps 24576, FPS 119 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.2889
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6106
         discriminator_loss   1.2267
                  gail_loss   1.2084
                  grad_loss   0.0183
                    ib_loss  -0.1966
                  task_loss   1.2125
                       beta   0.0000
             posterior_loss   0.5026
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0567 -0.0794 -0.1773 ... -0.1049 -0.0767 -0.061 ]]
 discr_rewards:  [[-0.0437 -0.0172 -0.0494 ... -0.0494 -0.0394 -0.0346]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1398 -0.136  -0.2661 ... -0.1938 -0.1556 -0.135 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 12, num timesteps 26624, FPS 121 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.2486
                action_loss  -0.0054
                    bc_loss   0.0000
               dist_entropy   0.6872
         discriminator_loss   1.2088
                  gail_loss   1.1907
                  grad_loss   0.0181
                    ib_loss  -0.1969
                  task_loss   1.2095
                       beta   0.0000
             posterior_loss   0.5075
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0812 -0.0583 -0.079  ... -0.2496 -0.0526 -0.0766]]
 discr_rewards:  [[-0.0387 -0.0402 -0.0647 ... -0.0764  0.0385  0.0304]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1594 -0.138  -0.1832 ... -0.3656 -0.0535 -0.0858]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 13, num timesteps 28672, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   1.1311
                action_loss  -0.0090
                    bc_loss   0.0000
               dist_entropy   0.7395
         discriminator_loss   1.2114
                  gail_loss   1.1906
                  grad_loss   0.0207
                    ib_loss  -0.1968
                  task_loss   1.1137
                       beta   0.0000
             posterior_loss   0.5160
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0536 -0.0636 -0.4295 ... -0.0594 -0.0616 -0.0568]]
 discr_rewards:  [[-0.0519 -0.0698  0.0244 ... -0.0193  0.0334 -0.0409]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.145  -0.1729 -0.4446 ... -0.1183 -0.0676 -0.1372]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 14, num timesteps 30720, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.9840
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.7507
         discriminator_loss   1.2110
                  gail_loss   1.1919
                  grad_loss   0.0191
                    ib_loss  -0.1963
                  task_loss   1.0589
                       beta   0.0000
             posterior_loss   0.5134
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1852 -0.0524 -0.0537 ... -0.0533 -0.0562 -0.074 ]]
 discr_rewards:  [[-0.0355 -0.0576  0.0085 ... -0.0426  0.0199  0.0129]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2602 -0.1495 -0.0847 ... -0.1354 -0.0758 -0.1007]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 15, num timesteps 32768, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.8837
                action_loss  -0.0092
                    bc_loss   0.0000
               dist_entropy   0.7811
         discriminator_loss   1.2200
                  gail_loss   1.2018
                  grad_loss   0.0182
                    ib_loss  -0.1972
                  task_loss   0.8794
                       beta   0.0000
             posterior_loss   0.5205
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0704 -0.4931 -0.057  ... -0.0525 -0.1939 -0.0575]]
 discr_rewards:  [[-0.0458 -0.0517 -0.0035 ... -0.0413 -0.0419 -0.0342]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1557 -0.5844 -0.1001 ... -0.1333 -0.2753 -0.1312]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 16, num timesteps 34816, FPS 124 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.8274
                action_loss  -0.0091
                    bc_loss   0.0000
               dist_entropy   0.7919
         discriminator_loss   1.2271
                  gail_loss   1.2090
                  grad_loss   0.0181
                    ib_loss  -0.1968
                  task_loss   0.8823
                       beta   0.0000
             posterior_loss   0.5282
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[-0.1839 -0.0534 -0.0533 ... -0.1107 -0.0535 -0.0559]]
 discr_rewards:  [[-0.0037 -0.0411 -0.0459 ... -0.0346 -0.0257 -0.0478]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2271 -0.1339 -0.1386 ... -0.1848 -0.1185 -0.1431]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 17, num timesteps 36864, FPS 125 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.5647
                action_loss  -0.0128
                    bc_loss   0.0000
               dist_entropy   0.7799
         discriminator_loss   1.2402
                  gail_loss   1.2223
                  grad_loss   0.0179
                    ib_loss  -0.1971
                  task_loss   0.6610
                       beta   0.0000
             posterior_loss   0.4726
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[-0.0824 -0.0749 -0.0663 ... -0.1969 -0.0843 -0.055 ]]
 discr_rewards:  [[-0.0034 -0.001  -0.0197 ... -0.0149 -0.0028  0.0021]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1252 -0.1153 -0.1254 ... -0.2513 -0.1266 -0.0923]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 18, num timesteps 38912, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.6044
                action_loss  -0.0060
                    bc_loss   0.0000
               dist_entropy   0.7441
         discriminator_loss   1.2478
                  gail_loss   1.2319
                  grad_loss   0.0159
                    ib_loss  -0.1969
                  task_loss   0.6112
                       beta   0.0000
             posterior_loss   0.5200
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0893 -0.0906 -0.0534 ... -0.1347 -0.066  -0.1874]]
 discr_rewards:  [[-0.0121 -0.0099 -0.0245 ... -0.0147 -0.0131 -0.0202]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1408 -0.1399 -0.1174 ... -0.1889 -0.1186 -0.247 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 19, num timesteps 40960, FPS 128 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.4980
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.6795
         discriminator_loss   1.2682
                  gail_loss   1.2520
                  grad_loss   0.0163
                    ib_loss  -0.1969
                  task_loss   0.6357
                       beta   0.0000
             posterior_loss   0.4861
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1842 -0.0661 -0.1153 ... -0.0927 -0.0758 -0.0533]]
 discr_rewards:  [[-0.0177 -0.0172  0.0051 ... -0.0295 -0.0293 -0.03  ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2414 -0.1227 -0.1497 ... -0.1616 -0.1447 -0.1228]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 20, num timesteps 43008, FPS 129 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.5813
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.6099
         discriminator_loss   1.2373
                  gail_loss   1.2190
                  grad_loss   0.0182
                    ib_loss  -0.1965
                  task_loss   0.5780
                       beta   0.0000
             posterior_loss   0.4978
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0541 -0.1492 -0.0561 ... -0.1505 -0.0609 -0.0524]]
 discr_rewards:  [[-0.0339 -0.0312 -0.0307 ... -0.0895 -0.0363 -0.03  ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1275 -0.2198 -0.1263 ... -0.2794 -0.1368 -0.122 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 21, num timesteps 45056, FPS 129 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.5019
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.5753
         discriminator_loss   1.2211
                  gail_loss   1.2038
                  grad_loss   0.0174
                    ib_loss  -0.1968
                  task_loss   0.5554
                       beta   0.0000
             posterior_loss   0.4956
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0909 -0.072  -0.0648 ... -0.0987 -0.0646 -0.0536]]
 discr_rewards:  [[-0.039  -0.0404 -0.0426 ... -0.0375 -0.0416 -0.0377]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1693 -0.1519 -0.1469 ... -0.1757 -0.1457 -0.1308]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 22, num timesteps 47104, FPS 130 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.5235
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.5820
         discriminator_loss   1.1976
                  gail_loss   1.1797
                  grad_loss   0.0179
                    ib_loss  -0.1968
                  task_loss   0.5454
                       beta   0.0000
             posterior_loss   0.4862
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2644 -0.1522 -0.2022 ... -0.0946 -0.206  -0.0526]]
 discr_rewards:  [[ 0.0248 -0.0429 -0.0408 ...  0.0242 -0.0424 -0.0445]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2792 -0.2347 -0.2825 ... -0.1099 -0.2879 -0.1366]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 23, num timesteps 49152, FPS 131 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.5422
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.5973
         discriminator_loss   1.1913
                  gail_loss   1.1740
                  grad_loss   0.0173
                    ib_loss  -0.1970
                  task_loss   0.5297
                       beta   0.0000
             posterior_loss   0.5024
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0969 -0.0622 -0.0734 ... -0.0528 -0.1246 -0.1169]]
 discr_rewards:  [[-0.0479 -0.055  -0.0505 ... -0.0445 -0.0402 -0.0445]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1844 -0.1568 -0.1635 ... -0.1369 -0.2044 -0.201 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 24, num timesteps 51200, FPS 132 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.4218
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.5705
         discriminator_loss   1.1890
                  gail_loss   1.1736
                  grad_loss   0.0154
                    ib_loss  -0.1968
                  task_loss   0.4633
                       beta   0.0000
             posterior_loss   0.4827
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.194  -0.0737 -0.0544 ... -0.1768 -0.0595 -0.0525]]
 discr_rewards:  [[ 0.0093 -0.0011 -0.0506 ... -0.0447  0.0259 -0.0485]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2242 -0.1143 -0.1446 ... -0.261  -0.0731 -0.1405]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 25, num timesteps 53248, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.4568
                action_loss  -0.0016
                    bc_loss   0.0000
               dist_entropy   0.6002
         discriminator_loss   1.1764
                  gail_loss   1.1641
                  grad_loss   0.0123
                    ib_loss  -0.1971
                  task_loss   0.5269
                       beta   0.0000
             posterior_loss   0.5096
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0758 -0.1399 -0.1556 ... -0.0975 -0.0998 -0.2481]]
 discr_rewards:  [[-0.0408 -0.0448 -0.0508 ...  0.0317  0.0192 -0.0378]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1561 -0.2242 -0.2459 ... -0.1053 -0.12   -0.3254]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 26, num timesteps 55296, FPS 123 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.3363
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6059
         discriminator_loss   1.1851
                  gail_loss   1.1723
                  grad_loss   0.0128
                    ib_loss  -0.1972
                  task_loss   0.4291
                       beta   0.0000
             posterior_loss   0.4673
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.3312 -0.3068 ... -0.0958 -0.0819 -0.0683]]
 discr_rewards:  [[-0.0535 -0.0356  0.0096 ... -0.055   0.0173 -0.0574]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1455 -0.4063 -0.3366 ... -0.1902 -0.104  -0.1652]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 27, num timesteps 57344, FPS 125 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.3955
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6497
         discriminator_loss   1.1902
                  gail_loss   1.1796
                  grad_loss   0.0106
                    ib_loss  -0.1969
                  task_loss   0.3964
                       beta   0.0000
             posterior_loss   0.5137
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0767 -0.0644 -0.058  ... -0.1705 -0.0631 -0.2454]]
 discr_rewards:  [[-0.0625 -0.0499  0.0066 ... -0.0834  0.0002 -0.0625]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1787 -0.1539 -0.0909 ... -0.2934 -0.1023 -0.3473]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 28, num timesteps 59392, FPS 126 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.2992
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6861
         discriminator_loss   1.1743
                  gail_loss   1.1638
                  grad_loss   0.0104
                    ib_loss  -0.1967
                  task_loss   0.3721
                       beta   0.0000
             posterior_loss   0.4793
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0926 -0.105  -0.0875 ... -0.0624 -0.0539 -0.0889]]
 discr_rewards:  [[-0.0672 -0.0079 -0.0555 ... -0.0118 -0.0572 -0.0417]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1993 -0.1524 -0.1825 ... -0.1137 -0.1507 -0.1701]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 29, num timesteps 61440, FPS 128 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.3021
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.7128
         discriminator_loss   1.1748
                  gail_loss   1.1665
                  grad_loss   0.0083
                    ib_loss  -0.1969
                  task_loss   0.2767
                       beta   0.0000
             posterior_loss   0.5285
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0561 -0.0859 -0.1001 ... -0.0915 -0.0768 -0.0738]]
 discr_rewards:  [[-0.0414 -0.0591 -0.0613 ...  0.0079 -0.0362  0.01  ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.137  -0.1845 -0.2008 ... -0.123  -0.1524 -0.1033]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 30, num timesteps 63488, FPS 129 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.2479
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6879
         discriminator_loss   1.1786
                  gail_loss   1.1702
                  grad_loss   0.0083
                    ib_loss  -0.1967
                  task_loss   0.2809
                       beta   0.0000
             posterior_loss   0.5010
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0767 -0.0536 -0.0562 ... -0.2579 -0.1216 -0.1501]]
 discr_rewards:  [[ 0.0151 -0.0434 -0.0486 ...  0.0109 -0.0512 -0.0472]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1011 -0.1365 -0.1444 ... -0.2865 -0.2124 -0.2368]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 31, num timesteps 65536, FPS 130 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.2113
                action_loss   0.0017
                    bc_loss   0.0000
               dist_entropy   0.6950
         discriminator_loss   1.1597
                  gail_loss   1.1498
                  grad_loss   0.0098
                    ib_loss  -0.1971
                  task_loss   0.2587
                       beta   0.0000
             posterior_loss   0.4795
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0821 -0.3566 -0.0538 ... -0.0604 -0.0989 -0.0589]]
 discr_rewards:  [[-0.0357 -0.0028 -0.0593 ... -0.0509 -0.0651 -0.0487]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1573 -0.3989 -0.1526 ... -0.1508 -0.2036 -0.147 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 32, num timesteps 67584, FPS 131 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.2553
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6879
         discriminator_loss   1.1554
                  gail_loss   1.1443
                  grad_loss   0.0111
                    ib_loss  -0.1969
                  task_loss   0.2587
                       beta   0.0000
             posterior_loss   0.5167
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2032 -0.0718 -0.0767 ... -0.0662 -0.2065 -0.222 ]]
 discr_rewards:  [[-0.1004 -0.0446 -0.0875 ... -0.0539 -0.0615 -0.0786]]
 task_rewards:  [[3.7253e-09 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3431 -0.1559 -0.2037 ... -0.1597 -0.3075 -0.3401]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 33, num timesteps 69632, FPS 132 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.2702
                action_loss  -0.0068
                    bc_loss   0.0000
               dist_entropy   0.6370
         discriminator_loss   1.1347
                  gail_loss   1.1231
                  grad_loss   0.0116
                    ib_loss  -0.1970
                  task_loss   0.3341
                       beta   0.0000
             posterior_loss   0.5153
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0777 -0.0554 -0.0545 ... -0.1673 -0.3361 -0.0663]]
 discr_rewards:  [[-0.0356 -0.0161 -0.115  ...  0.0221  0.0146 -0.1111]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1527 -0.111  -0.209  ... -0.1847 -0.361  -0.2168]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 34, num timesteps 71680, FPS 133 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.2469
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6703
         discriminator_loss   1.1223
                  gail_loss   1.1060
                  grad_loss   0.0163
                    ib_loss  -0.1969
                  task_loss   0.3082
                       beta   0.0000
             posterior_loss   0.5033
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1637 -0.0683 -0.1027 ... -0.0963 -0.1014 -0.0959]]
 discr_rewards:  [[-0.0833  0.0216 -0.0589 ... -0.0134 -0.014  -0.0461]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2865 -0.0862 -0.2011 ... -0.1491 -0.1549 -0.1815]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 35, num timesteps 73728, FPS 134 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1917
                action_loss  -0.0052
                    bc_loss   0.0000
               dist_entropy   0.7035
         discriminator_loss   1.1242
                  gail_loss   1.1059
                  grad_loss   0.0183
                    ib_loss  -0.1968
                  task_loss   0.1962
                       beta   0.0000
             posterior_loss   0.4969
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0756 -0.1383 -0.0706 ... -0.0578 -0.0662 -0.402 ]]
 discr_rewards:  [[ 0.0123 -0.0634  0.0053 ... -0.0126  0.0087 -0.0758]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1028 -0.2412 -0.1048 ... -0.1098 -0.0969 -0.5173]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 36, num timesteps 75776, FPS 135 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1754
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7209
         discriminator_loss   1.1229
                  gail_loss   1.0998
                  grad_loss   0.0230
                    ib_loss  -0.1970
                  task_loss   0.1629
                       beta   0.0000
             posterior_loss   0.5054
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1621 -0.2112 -0.0795 ... -0.0682 -0.1079 -0.1221]]
 discr_rewards:  [[-0.0285 -0.036  -0.0446 ... -0.0424 -0.0459 -0.0367]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2301 -0.2867 -0.1636 ... -0.1501 -0.1933 -0.1983]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 37, num timesteps 77824, FPS 136 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1670
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7356
         discriminator_loss   1.1248
                  gail_loss   1.1011
                  grad_loss   0.0238
                    ib_loss  -0.1968
                  task_loss   0.1749
                       beta   0.0000
             posterior_loss   0.5130
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.078  -0.0539 -0.4748 ... -0.0754 -0.0578 -0.0851]]
 discr_rewards:  [[-0.0624 -0.0622 -0.0847 ... -0.0634 -0.0404  0.0052]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1799 -0.1556 -0.5989 ... -0.1783 -0.1377 -0.1194]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 38, num timesteps 79872, FPS 137 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1795
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.7163
         discriminator_loss   1.1138
                  gail_loss   1.0886
                  grad_loss   0.0252
                    ib_loss  -0.1968
                  task_loss   0.2375
                       beta   0.0000
             posterior_loss   0.4964
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0793 -0.1201 -0.0764 ... -0.5185 -0.1079 -0.0965]]
 discr_rewards:  [[-0.0773 -0.0794 -0.0291 ...  0.0162 -0.0288 -0.0204]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1961 -0.2389 -0.145  ... -0.5418 -0.1763 -0.1564]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 39, num timesteps 81920, FPS 138 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1438
                action_loss   0.0032
                    bc_loss   0.0000
               dist_entropy   0.7057
         discriminator_loss   1.0918
                  gail_loss   1.0684
                  grad_loss   0.0234
                    ib_loss  -0.1971
                  task_loss   0.1881
                       beta   0.0000
             posterior_loss   0.4881
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1427 -0.0829 -0.2578 ... -0.0527 -0.0525 -0.0642]]
 discr_rewards:  [[-0.1141 -0.0202 -0.1104 ...  0.0029 -0.0321 -0.0371]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2963 -0.1426 -0.4077 ... -0.0893 -0.1241 -0.1409]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 40, num timesteps 83968, FPS 139 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1487
                action_loss  -0.0053
                    bc_loss   0.0000
               dist_entropy   0.7351
         discriminator_loss   1.0948
                  gail_loss   1.0718
                  grad_loss   0.0229
                    ib_loss  -0.1970
                  task_loss   0.1679
                       beta   0.0000
             posterior_loss   0.5118
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.1595 -0.0532 ... -0.0622 -0.1692 -0.0549]]
 discr_rewards:  [[-0.1004 -0.0096 -0.0317 ... -0.0266 -0.1109 -0.0375]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1925 -0.2086 -0.1243 ... -0.1283 -0.3195 -0.1319]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 41, num timesteps 86016, FPS 140 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1106
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7318
         discriminator_loss   1.1041
                  gail_loss   1.0770
                  grad_loss   0.0271
                    ib_loss  -0.1970
                  task_loss   0.1369
                       beta   0.0000
             posterior_loss   0.5184
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0575 -0.0757 -0.0534 ... -0.0974 -0.0754 -0.1397]]
 discr_rewards:  [[-0.0319 -0.0282 -0.023  ... -0.0472 -0.0582 -0.0627]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.129  -0.1434 -0.1159 ... -0.1841 -0.173  -0.2418]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 42, num timesteps 88064, FPS 141 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1432
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.7382
         discriminator_loss   1.0972
                  gail_loss   1.0722
                  grad_loss   0.0250
                    ib_loss  -0.1970
                  task_loss   0.1770
                       beta   0.0000
             posterior_loss   0.5035
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1176 -0.0544 -0.1071 ... -0.0985 -0.0906 -0.0915]]
 discr_rewards:  [[-0.0277 -0.0222 -0.0165 ... -0.0207 -0.0277  0.0064]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1848 -0.1161 -0.1631 ... -0.1587 -0.1578 -0.1246]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 43, num timesteps 90112, FPS 141 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0884
                action_loss   0.0027
                    bc_loss   0.0000
               dist_entropy   0.7313
         discriminator_loss   1.0829
                  gail_loss   1.0568
                  grad_loss   0.0261
                    ib_loss  -0.1968
                  task_loss   0.1575
                       beta   0.0000
             posterior_loss   0.4787
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1029 -0.2551 -0.1066 ... -0.0915 -0.1577 -0.0605]]
 discr_rewards:  [[-0.0881 -0.0161 -0.0874 ... -0.0427 -0.0097 -0.0212]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2305 -0.3107 -0.2335 ... -0.1737 -0.2069 -0.1212]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 44, num timesteps 92160, FPS 142 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1044
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.6988
         discriminator_loss   1.0754
                  gail_loss   1.0525
                  grad_loss   0.0230
                    ib_loss  -0.1966
                  task_loss   0.1097
                       beta   0.0000
             posterior_loss   0.5130
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1386 -0.0558 -0.1082 ... -0.1573 -0.0586 -0.0617]]
 discr_rewards:  [[-0.0195 -0.0029 -0.0298 ... -0.029  -0.0364 -0.0376]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1976 -0.0981 -0.1775 ... -0.2258 -0.1346 -0.1388]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 45, num timesteps 94208, FPS 143 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1279
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6787
         discriminator_loss   1.0496
                  gail_loss   1.0256
                  grad_loss   0.0241
                    ib_loss  -0.1969
                  task_loss   0.1789
                       beta   0.0000
             posterior_loss   0.5094
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1183 -0.2599 -0.0545 ... -0.1892 -0.0667 -0.0744]]
 discr_rewards:  [[-0.0556 -0.037   0.0057 ... -0.0268 -0.0508 -0.047 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2133 -0.3364 -0.0884 ... -0.2555 -0.1571 -0.1609]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 46, num timesteps 96256, FPS 143 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1071
                action_loss   0.0028
                    bc_loss   0.0000
               dist_entropy   0.6453
         discriminator_loss   1.0462
                  gail_loss   1.0206
                  grad_loss   0.0256
                    ib_loss  -0.1970
                  task_loss   0.1588
                       beta   0.0000
             posterior_loss   0.5208
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1203 -0.0546 -0.1159 ... -0.49   -0.0527 -0.0573]]
 discr_rewards:  [[-0.0299  0.03   -0.0879 ...  0.0227 -0.0334  0.0041]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1897 -0.0641 -0.2433 ... -0.5068 -0.1256 -0.0926]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 47, num timesteps 98304, FPS 144 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0861
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.6695
         discriminator_loss   1.0401
                  gail_loss   1.0139
                  grad_loss   0.0262
                    ib_loss  -0.1967
                  task_loss   0.1390
                       beta   0.0000
             posterior_loss   0.4961
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2211 -0.1125 -0.0604 ... -0.0524 -0.0634 -0.0552]]
 discr_rewards:  [[-0.0273 -0.0664 -0.12   ... -0.0621 -0.0799 -0.0712]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2879 -0.2184 -0.22   ... -0.154  -0.1828 -0.1659]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 48, num timesteps 100352, FPS 145 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0737
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.6936
         discriminator_loss   1.0241
                  gail_loss   0.9996
                  grad_loss   0.0245
                    ib_loss  -0.1969
                  task_loss   0.1080
                       beta   0.0000
             posterior_loss   0.4938
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2269 -0.0796 -0.1083 ... -0.1327 -0.0849 -0.0565]]
 discr_rewards:  [[-0.0442 -0.0366 -0.0735 ... -0.1262 -0.0407 -0.1023]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.3106 -0.1557 -0.2214 ... -0.2985 -0.1652 -0.1983]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 49, num timesteps 102400, FPS 145 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0625
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6983
         discriminator_loss   1.0356
                  gail_loss   1.0111
                  grad_loss   0.0245
                    ib_loss  -0.1971
                  task_loss   0.0900
                       beta   0.0000
             posterior_loss   0.5106
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.063  -0.2436 -0.1986 ... -0.1306 -0.0602 -0.0596]]
 discr_rewards:  [[-0.0321 -0.0433 -0.0202 ... -0.1018 -0.0884  0.0111]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1346 -0.3264 -0.2583 ... -0.2719 -0.1881 -0.0879]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 50, num timesteps 104448, FPS 139 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0539
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7095
         discriminator_loss   1.0328
                  gail_loss   1.0089
                  grad_loss   0.0238
                    ib_loss  -0.1968
                  task_loss   0.0669
                       beta   0.0000
             posterior_loss   0.5090
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0938 -0.0619 -0.0556 ... -0.0554 -0.0713 -0.0619]]
 discr_rewards:  [[-0.0351 -0.0135 -0.0401 ... -0.0339 -0.1758 -0.0356]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1684 -0.1149 -0.1352 ... -0.1288 -0.2866 -0.137 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 51, num timesteps 106496, FPS 140 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0737
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.7075
         discriminator_loss   1.0301
                  gail_loss   1.0043
                  grad_loss   0.0258
                    ib_loss  -0.1967
                  task_loss   0.1481
                       beta   0.0000
             posterior_loss   0.4901
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0532 -0.0561 -0.0914 ... -0.1076 -0.0599 -0.0657]]
 discr_rewards:  [[-0.0406 -0.0253 -0.0104 ... -0.0356 -0.0307 -0.1009]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1333 -0.1209 -0.1413 ... -0.1827 -0.1302 -0.2061]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 52, num timesteps 108544, FPS 140 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0472
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6825
         discriminator_loss   1.0153
                  gail_loss   0.9906
                  grad_loss   0.0247
                    ib_loss  -0.1971
                  task_loss   0.1091
                       beta   0.0000
             posterior_loss   0.4761
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1257 -0.0567 -0.0577 ... -0.1131 -0.095  -0.1344]]
 discr_rewards:  [[-0.0095 -0.1051 -0.0337 ... -0.0283 -0.0565 -0.0365]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1747 -0.2013 -0.1309 ... -0.1809 -0.191  -0.2104]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 53, num timesteps 110592, FPS 141 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0601
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6983
         discriminator_loss   0.9980
                  gail_loss   0.9753
                  grad_loss   0.0227
                    ib_loss  -0.1970
                  task_loss   0.0569
                       beta   0.0000
             posterior_loss   0.5169
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0617 -0.2853 -0.129  ... -0.0545 -0.0561 -0.0926]]
 discr_rewards:  [[-0.0703 -0.0261 -0.0581 ... -0.0402 -0.0516 -0.0205]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1716 -0.3509 -0.2266 ... -0.1343 -0.1472 -0.1526]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 54, num timesteps 112640, FPS 142 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0460
                action_loss   0.0023
                    bc_loss   0.0000
               dist_entropy   0.6886
         discriminator_loss   1.0039
                  gail_loss   0.9817
                  grad_loss   0.0222
                    ib_loss  -0.1971
                  task_loss   0.0653
                       beta   0.0000
             posterior_loss   0.4991
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0727 -0.0808 -0.053  ... -0.1069 -0.0525 -0.0531]]
 discr_rewards:  [[-0.0185 -0.0149 -0.0488 ... -0.053  -0.1142 -0.0492]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1307 -0.1352 -0.1413 ... -0.1994 -0.2063 -0.1418]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 55, num timesteps 114688, FPS 142 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0524
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.6987
         discriminator_loss   0.9867
                  gail_loss   0.9640
                  grad_loss   0.0226
                    ib_loss  -0.1971
                  task_loss   0.0521
                       beta   0.0000
             posterior_loss   0.4952
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0557 -0.0936 -0.0631 ... -0.082  -0.0789 -0.0619]]
 discr_rewards:  [[-0.0732 -0.0293  0.0103 ... -0.1023 -0.1281 -0.1444]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1684 -0.1624 -0.0923 ... -0.2238 -0.2465 -0.2459]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 56, num timesteps 116736, FPS 143 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0559
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7157
         discriminator_loss   0.9988
                  gail_loss   0.9740
                  grad_loss   0.0248
                    ib_loss  -0.1969
                  task_loss   0.0763
                       beta   0.0000
             posterior_loss   0.5112
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2013 -0.0529 -0.1822 ... -0.0524 -0.0826 -0.0574]]
 discr_rewards:  [[-0.0199 -0.1428 -0.0143 ... -0.0005 -0.0718 -0.0525]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2608 -0.2353 -0.2361 ... -0.0925 -0.1939 -0.1494]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 57, num timesteps 118784, FPS 143 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0459
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7156
         discriminator_loss   0.9855
                  gail_loss   0.9611
                  grad_loss   0.0244
                    ib_loss  -0.1969
                  task_loss   0.0604
                       beta   0.0000
             posterior_loss   0.5047
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1682 -0.0534 -0.1297 ... -0.1856 -0.1339 -0.0806]]
 discr_rewards:  [[-0.0107 -0.0068 -0.0272 ... -0.1945 -0.1031 -0.0241]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 7.4506e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2184 -0.0997 -0.1964 ... -0.4197 -0.2766 -0.1442]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 58, num timesteps 120832, FPS 144 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0398
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7160
         discriminator_loss   0.9962
                  gail_loss   0.9712
                  grad_loss   0.0250
                    ib_loss  -0.1970
                  task_loss   0.0518
                       beta   0.0000
             posterior_loss   0.5199
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0608 -0.0721 -0.1201 ... -0.0524 -0.1058 -0.0826]]
 discr_rewards:  [[-0.0123 -0.0016 -0.0133 ... -0.0028 -0.0396 -0.2122]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1126 -0.1132 -0.1728 ... -0.0947 -0.1849 -0.3342]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 59, num timesteps 122880, FPS 145 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0470
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.6981
         discriminator_loss   0.9976
                  gail_loss   0.9716
                  grad_loss   0.0259
                    ib_loss  -0.1969
                  task_loss   0.0494
                       beta   0.0000
             posterior_loss   0.5177
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.173  -0.0565 -0.0914 ... -0.2378 -0.1114 -0.0754]]
 discr_rewards:  [[-0.0134 -0.1658 -0.0297 ...  0.0094 -0.0551 -0.0888]]
 task_rewards:  [[ 0.0000e+00  7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2259 -0.2619 -0.1606 ... -0.2679 -0.206  -0.2037]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 60, num timesteps 124928, FPS 145 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6993
         discriminator_loss   0.9842
                  gail_loss   0.9599
                  grad_loss   0.0244
                    ib_loss  -0.1969
                  task_loss   0.0509
                       beta   0.0000
             posterior_loss   0.4605
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0737 -0.0641 -0.0541 ... -0.0525 -0.0718 -0.1812]]
 discr_rewards:  [[-0.0117  0.0053 -0.0304 ...  0.011  -0.0116 -0.0643]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1249 -0.0983 -0.1239 ... -0.081  -0.123  -0.285 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 61, num timesteps 126976, FPS 146 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.7057
         discriminator_loss   0.9855
                  gail_loss   0.9629
                  grad_loss   0.0227
                    ib_loss  -0.1969
                  task_loss   0.0368
                       beta   0.0000
             posterior_loss   0.5102
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0737 -0.0534 -0.078  ... -0.3343 -0.0554 -0.0554]]
 discr_rewards:  [[-0.0637 -0.1285 -0.0643 ... -0.0391 -0.0372 -0.1141]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1768 -0.2214 -0.1819 ... -0.4129 -0.1321 -0.2091]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 62, num timesteps 129024, FPS 146 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0277
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.6972
         discriminator_loss   0.9751
                  gail_loss   0.9500
                  grad_loss   0.0251
                    ib_loss  -0.1970
                  task_loss   0.0352
                       beta   0.0000
             posterior_loss   0.4601
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.053  -0.131  -0.1026 ... -0.0578 -0.2543 -0.0834]]
 discr_rewards:  [[-0.1059 -0.1137 -0.1297 ... -0.0072 -0.0521 -0.1454]]
 task_rewards:  [[ 3.7253e-09  3.7253e-09 -7.4506e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1984 -0.2842 -0.2718 ... -0.1045 -0.3459 -0.2683]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 63, num timesteps 131072, FPS 147 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0265
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6677
         discriminator_loss   0.9729
                  gail_loss   0.9485
                  grad_loss   0.0245
                    ib_loss  -0.1971
                  task_loss   0.0290
                       beta   0.0000
             posterior_loss   0.4968
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.0574 -0.097  ... -0.0579 -0.1086 -0.0581]]
 discr_rewards:  [[-0.0979 -0.0911 -0.0592 ... -0.1196 -0.0698 -0.0185]]
 task_rewards:  [[ 3.7253e-09  3.7253e-09  0.0000e+00 ...  3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1902 -0.188  -0.1957 ... -0.217  -0.2179 -0.1161]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 64, num timesteps 133120, FPS 147 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0344
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.6855
         discriminator_loss   0.9668
                  gail_loss   0.9439
                  grad_loss   0.0229
                    ib_loss  -0.1968
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.4828
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1076 -0.0525 -0.0853 ... -0.0527 -0.1444 -0.0903]]
 discr_rewards:  [[-0.1051 -0.0412 -0.0783 ... -0.0347  0.0048 -0.0147]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2522 -0.1332 -0.2032 ... -0.1269 -0.1791 -0.1445]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 65, num timesteps 135168, FPS 148 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0312
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6986
         discriminator_loss   0.9715
                  gail_loss   0.9490
                  grad_loss   0.0225
                    ib_loss  -0.1969
                  task_loss   0.0427
                       beta   0.0000
             posterior_loss   0.5062
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1485 -0.0957 -0.1217 ... -0.2443 -0.0778 -0.0531]]
 discr_rewards:  [[-0.0887 -0.0297 -0.0263 ... -0.0242 -0.033  -0.0342]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2767 -0.1649 -0.1875 ... -0.308  -0.1503 -0.1269]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 66, num timesteps 137216, FPS 148 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0388
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.7056
         discriminator_loss   0.9705
                  gail_loss   0.9459
                  grad_loss   0.0245
                    ib_loss  -0.1968
                  task_loss   0.0332
                       beta   0.0000
             posterior_loss   0.5217
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0873 -0.0535 -0.0776 ... -0.0748 -0.0986 -0.1875]]
 discr_rewards:  [[-0.1373 -0.005  -0.0537 ... -0.0371 -0.0662 -0.1699]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2641 -0.098  -0.1708 ... -0.1514 -0.2043 -0.397 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 67, num timesteps 139264, FPS 149 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0245
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7099
         discriminator_loss   0.9739
                  gail_loss   0.9510
                  grad_loss   0.0229
                    ib_loss  -0.1970
                  task_loss   0.0419
                       beta   0.0000
             posterior_loss   0.4927
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0919 -0.2091 -0.0559 ... -0.1691 -0.0544 -0.0743]]
 discr_rewards:  [[-0.0684 -0.0236 -0.0493 ... -0.0201 -0.0343 -0.0483]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1998 -0.2722 -0.1447 ... -0.2287 -0.1282 -0.162 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 68, num timesteps 141312, FPS 149 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0356
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7015
         discriminator_loss   0.9692
                  gail_loss   0.9482
                  grad_loss   0.0210
                    ib_loss  -0.1967
                  task_loss   0.0268
                       beta   0.0000
             posterior_loss   0.5077
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.062  -0.0592 -0.0802 ... -0.0569 -0.0591 -0.1385]]
 discr_rewards:  [[-0.0228 -0.0149 -0.1648 ... -0.0924 -0.0955 -0.1765]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1244 -0.1135 -0.2845 ... -0.1888 -0.1941 -0.3546]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 69, num timesteps 143360, FPS 149 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0341
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.7049
         discriminator_loss   0.9659
                  gail_loss   0.9437
                  grad_loss   0.0223
                    ib_loss  -0.1970
                  task_loss   0.0400
                       beta   0.0000
             posterior_loss   0.5018
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1526 -0.0524 -0.0573 ... -0.1431 -0.1515 -0.1891]]
 discr_rewards:  [[-0.0403 -0.1135 -0.0221 ... -0.1022 -0.0755 -0.0636]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2324 -0.2054 -0.1189 ... -0.2848 -0.2665 -0.2922]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 70, num timesteps 145408, FPS 150 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.7110
         discriminator_loss   0.9583
                  gail_loss   0.9340
                  grad_loss   0.0243
                    ib_loss  -0.1967
                  task_loss   0.0370
                       beta   0.0000
             posterior_loss   0.5082
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0559 -0.0542 -0.0713 ... -0.168  -0.0595 -0.1554]]
 discr_rewards:  [[-0.0617 -0.0221 -0.0528 ... -0.0991 -0.0181 -0.0414]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1571 -0.1159 -0.1636 ... -0.3066 -0.117  -0.2364]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 71, num timesteps 147456, FPS 150 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0383
                action_loss   0.0046
                    bc_loss   0.0000
               dist_entropy   0.7101
         discriminator_loss   0.9710
                  gail_loss   0.9464
                  grad_loss   0.0247
                    ib_loss  -0.1969
                  task_loss   0.0353
                       beta   0.0000
             posterior_loss   0.5188
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0585 -0.0889 ... -0.0814 -0.0702 -0.0587]]
 discr_rewards:  [[-0.0232 -0.0079 -0.0245 ... -0.0086 -0.05   -0.2   ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1151 -0.1059 -0.1529 ... -0.1295 -0.1597 -0.2983]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 72, num timesteps 149504, FPS 151 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0367
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.6995
         discriminator_loss   0.9677
                  gail_loss   0.9454
                  grad_loss   0.0223
                    ib_loss  -0.1972
                  task_loss   0.0424
                       beta   0.0000
             posterior_loss   0.5314
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0534 -0.0863 -0.1128 ... -0.1696 -0.0911 -0.3487]]
 discr_rewards:  [[-0.0816 -0.1162 -0.1225 ... -0.1612 -0.0766 -0.0022]]
 task_rewards:  [[3.7253e-09 0.0000e+00 3.7253e-09 ... 7.4506e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1745 -0.242  -0.2749 ... -0.3703 -0.2072 -0.3904]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 73, num timesteps 151552, FPS 151 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0298
                action_loss  -0.0069
                    bc_loss   0.0000
               dist_entropy   0.6734
         discriminator_loss   0.9715
                  gail_loss   0.9511
                  grad_loss   0.0204
                    ib_loss  -0.1967
                  task_loss   0.0434
                       beta   0.0000
             posterior_loss   0.4998
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.0532 -0.064  ... -0.0588 -0.0753 -0.0742]]
 discr_rewards:  [[-0.0159 -0.1397 -0.0164 ... -0.064  -0.0117 -0.0426]]
 task_rewards:  [[ 0.0000e+00  7.4506e-09  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1082 -0.2324 -0.12   ... -0.1624 -0.1265 -0.1563]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 74, num timesteps 153600, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0286
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.6938
         discriminator_loss   0.9762
                  gail_loss   0.9569
                  grad_loss   0.0193
                    ib_loss  -0.1971
                  task_loss   0.0332
                       beta   0.0000
             posterior_loss   0.4820
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2917 -0.1556 -0.1111 ... -0.1571 -0.0557 -0.1456]]
 discr_rewards:  [[-0.036   0.0043  0.0003 ... -0.0227 -0.003  -0.1072]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.3672 -0.1908 -0.1504 ... -0.2194 -0.0981 -0.2923]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 75, num timesteps 155648, FPS 147 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6979
         discriminator_loss   0.9648
                  gail_loss   0.9446
                  grad_loss   0.0203
                    ib_loss  -0.1969
                  task_loss   0.0376
                       beta   0.0000
             posterior_loss   0.5198
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1216 -0.2484 -0.1024 ... -0.154  -0.0524 -0.1097]]
 discr_rewards:  [[-0.047  -0.082  -0.0465 ... -0.0601 -0.0173 -0.0871]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2081 -0.3699 -0.1884 ... -0.2536 -0.1092 -0.2362]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 76, num timesteps 157696, FPS 147 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0357
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.6930
         discriminator_loss   0.9574
                  gail_loss   0.9359
                  grad_loss   0.0215
                    ib_loss  -0.1969
                  task_loss   0.0429
                       beta   0.0000
             posterior_loss   0.5098
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0632 -0.0616 -0.0726 ... -0.0768 -0.0673 -0.0531]]
 discr_rewards:  [[-0.0445 -0.0052 -0.0938 ... -0.0957 -0.0911 -0.0915]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1473 -0.1063 -0.2059 ... -0.212  -0.1979 -0.1841]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 77, num timesteps 159744, FPS 148 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0278
                action_loss   0.0024
                    bc_loss   0.0000
               dist_entropy   0.6885
         discriminator_loss   0.9606
                  gail_loss   0.9376
                  grad_loss   0.0230
                    ib_loss  -0.1969
                  task_loss   0.0385
                       beta   0.0000
             posterior_loss   0.5071
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0543 -0.0778 -0.0525 ... -0.1058 -0.0524 -0.0527]]
 discr_rewards:  [[-0.1027 -0.0393 -0.0192 ... -0.0221 -0.0125  0.0025]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1966 -0.1566 -0.1112 ... -0.1674 -0.1044 -0.0897]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 78, num timesteps 161792, FPS 148 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0393
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.6960
         discriminator_loss   0.9689
                  gail_loss   0.9466
                  grad_loss   0.0223
                    ib_loss  -0.1970
                  task_loss   0.0307
                       beta   0.0000
             posterior_loss   0.4974
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1056 -0.0887 -0.116  ... -0.7924 -0.0918 -0.0943]]
 discr_rewards:  [[-0.1449 -0.0714 -0.0047 ... -0.0074 -0.062  -0.0897]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.29   -0.1996 -0.1602 ... -0.8393 -0.1933 -0.2235]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 79, num timesteps 163840, FPS 149 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0381
                action_loss  -0.0057
                    bc_loss   0.0000
               dist_entropy   0.6912
         discriminator_loss   0.9708
                  gail_loss   0.9514
                  grad_loss   0.0194
                    ib_loss  -0.1966
                  task_loss   0.0501
                       beta   0.0000
             posterior_loss   0.5088
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2367 -0.0527 -0.1172 ... -0.0971 -0.0953 -0.0527]]
 discr_rewards:  [[-0.1061 -0.077  -0.0248 ... -0.0457 -0.1195 -0.0363]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3824 -0.1692 -0.1815 ... -0.1823 -0.2544 -0.1285]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 80, num timesteps 165888, FPS 149 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.6993
         discriminator_loss   0.9620
                  gail_loss   0.9416
                  grad_loss   0.0204
                    ib_loss  -0.1969
                  task_loss   0.0448
                       beta   0.0000
             posterior_loss   0.5348
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1919 -0.2806 -0.1151 ... -0.0527 -0.0584 -0.0733]]
 discr_rewards:  [[-0.0329 -0.0608 -0.0059 ... -0.1369 -0.0717 -0.0891]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 7.4506e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2643 -0.3809 -0.1605 ... -0.2292 -0.1696 -0.2019]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 81, num timesteps 167936, FPS 149 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0314
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.7042
         discriminator_loss   0.9598
                  gail_loss   0.9369
                  grad_loss   0.0229
                    ib_loss  -0.1970
                  task_loss   0.0373
                       beta   0.0000
             posterior_loss   0.5128
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0592 -0.0684 -0.0997 ... -0.1553 -0.2693 -0.3812]]
 discr_rewards:  [[-0.1106 -0.1305 -0.0195 ... -0.0904  0.0024 -0.1053]]
 task_rewards:  [[ 0.0000e+00 -7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.2093 -0.2383 -0.1587 ... -0.2852 -0.3065 -0.5261]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 82, num timesteps 169984, FPS 150 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0359
                action_loss   0.0008
                    bc_loss   0.0000
               dist_entropy   0.7104
         discriminator_loss   0.9615
                  gail_loss   0.9422
                  grad_loss   0.0193
                    ib_loss  -0.1968
                  task_loss   0.0358
                       beta   0.0000
             posterior_loss   0.4970
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0542 -0.056  -0.2286 ... -0.0551 -0.0582 -0.0539]]
 discr_rewards:  [[-0.0024  0.0035 -0.0014 ... -0.0334 -0.0467 -0.1412]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 7.4506e-09]]
 final_rewards:  [[-0.0961 -0.0921 -0.2695 ... -0.128  -0.1443 -0.2346]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 83, num timesteps 172032, FPS 150 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0316
                action_loss   0.0431
                    bc_loss   0.0000
               dist_entropy   0.7070
         discriminator_loss   0.9644
                  gail_loss   0.9442
                  grad_loss   0.0202
                    ib_loss  -0.1969
                  task_loss   0.0384
                       beta   0.0000
             posterior_loss   0.4688
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0881 -0.1377 -0.0642 ... -0.1155 -0.0722 -0.0607]]
 discr_rewards:  [[-0.0369 -0.0395 -0.1028 ... -0.0369 -0.0004 -0.0437]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1645 -0.2167 -0.2064 ... -0.1919 -0.1121 -0.1439]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 84, num timesteps 174080, FPS 150 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0360
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.6979
         discriminator_loss   0.9555
                  gail_loss   0.9345
                  grad_loss   0.0210
                    ib_loss  -0.1970
                  task_loss   0.0351
                       beta   0.0000
             posterior_loss   0.5103
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0939 -0.0527 -0.1202 ... -0.0537 -0.1395 -0.1379]]
 discr_rewards:  [[-0.061  -0.0265 -0.1094 ... -0.0332 -0.0797 -0.0721]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1944 -0.1187 -0.2691 ... -0.1265 -0.2587 -0.2494]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 85, num timesteps 176128, FPS 151 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0375
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6967
         discriminator_loss   0.9503
                  gail_loss   0.9319
                  grad_loss   0.0184
                    ib_loss  -0.1970
                  task_loss   0.0392
                       beta   0.0000
             posterior_loss   0.4880
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0749 -0.0756 -0.209  ... -0.092  -0.0877 -0.0586]]
 discr_rewards:  [[-0.041  -0.1184 -0.019  ... -0.018  -0.0668 -0.1321]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 3.7253e-09 3.7253e-09]]
 final_rewards:  [[-0.1554 -0.2335 -0.2676 ... -0.1496 -0.1941 -0.2303]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 86, num timesteps 178176, FPS 151 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0328
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.6967
         discriminator_loss   0.9636
                  gail_loss   0.9453
                  grad_loss   0.0183
                    ib_loss  -0.1968
                  task_loss   0.0419
                       beta   0.0000
             posterior_loss   0.5151
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0597 -0.0681 -0.068  ... -0.0535 -0.1125 -0.0951]]
 discr_rewards:  [[-0.0645 -0.2138 -0.1082 ... -0.0294 -0.0216 -0.1109]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.1637 -0.3214 -0.2158 ... -0.1224 -0.1736 -0.2456]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 87, num timesteps 180224, FPS 151 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0325
                action_loss  -0.0064
                    bc_loss   0.0000
               dist_entropy   0.6902
         discriminator_loss   0.9603
                  gail_loss   0.9418
                  grad_loss   0.0185
                    ib_loss  -0.1970
                  task_loss   0.0389
                       beta   0.0000
             posterior_loss   0.5041
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1111 -0.0541 -0.0873 ... -0.0542 -0.1422 -0.1007]]
 discr_rewards:  [[-0.0091 -0.0154 -0.0118 ... -0.0262 -0.0338 -0.0111]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1598 -0.109  -0.1386 ... -0.1199 -0.2155 -0.1514]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 88, num timesteps 182272, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0297
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6908
         discriminator_loss   0.9458
                  gail_loss   0.9255
                  grad_loss   0.0203
                    ib_loss  -0.1968
                  task_loss   0.0414
                       beta   0.0000
             posterior_loss   0.5116
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0654 -0.0621 -0.159  ... -0.0529 -0.0591 -0.0583]]
 discr_rewards:  [[-0.0082 -0.0291  0.0012 ... -0.0524 -0.0316 -0.0225]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1132 -0.1307 -0.1973 ... -0.1448 -0.1302 -0.1204]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 89, num timesteps 184320, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.6866
         discriminator_loss   0.9554
                  gail_loss   0.9370
                  grad_loss   0.0183
                    ib_loss  -0.1971
                  task_loss   0.0333
                       beta   0.0000
             posterior_loss   0.5179
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0885 -0.0976 -0.1366 ... -0.0561 -0.2063 -0.1545]]
 discr_rewards:  [[-0.0106 -0.1264 -0.0325 ...  0.0153 -0.0309 -0.0075]]
 task_rewards:  [[ 0.0000e+00 -7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1386 -0.2635 -0.2086 ... -0.0804 -0.2768 -0.2015]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 90, num timesteps 186368, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0354
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.6983
         discriminator_loss   0.9472
                  gail_loss   0.9264
                  grad_loss   0.0207
                    ib_loss  -0.1967
                  task_loss   0.0360
                       beta   0.0000
             posterior_loss   0.4966
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0942 -0.0531 ... -0.1663 -0.0525 -0.2481]]
 discr_rewards:  [[-0.0112 -0.0458 -0.0637 ... -0.0037 -0.024   0.0012]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1033 -0.1796 -0.1563 ... -0.2095 -0.116  -0.2865]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 91, num timesteps 188416, FPS 153 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.6937
         discriminator_loss   0.9494
                  gail_loss   0.9300
                  grad_loss   0.0194
                    ib_loss  -0.1972
                  task_loss   0.0376
                       beta   0.0000
             posterior_loss   0.4877
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0766 -0.1402 -0.0745 ... -0.057  -0.3072 -0.1466]]
 discr_rewards:  [[-0.0507 -0.0185 -0.0209 ... -0.0485  0.0073 -0.1628]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.1668 -0.1982 -0.1349 ... -0.1451 -0.3394 -0.3488]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 92, num timesteps 190464, FPS 153 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0393
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.6919
         discriminator_loss   0.9488
                  gail_loss   0.9284
                  grad_loss   0.0204
                    ib_loss  -0.1970
                  task_loss   0.0326
                       beta   0.0000
             posterior_loss   0.5445
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0574 -0.1094 -0.1129 ... -0.2882 -0.11   -0.0573]]
 discr_rewards:  [[-0.0192 -0.0041 -0.1805 ... -0.091  -0.018  -0.1353]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1161 -0.1529 -0.3329 ... -0.4188 -0.1675 -0.2321]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 93, num timesteps 192512, FPS 153 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0413
                action_loss   0.0021
                    bc_loss   0.0000
               dist_entropy   0.6853
         discriminator_loss   0.9594
                  gail_loss   0.9377
                  grad_loss   0.0216
                    ib_loss  -0.1969
                  task_loss   0.0446
                       beta   0.0000
             posterior_loss   0.5009
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1458 -0.0643 -0.1312 ... -0.0548 -0.1413 -0.055 ]]
 discr_rewards:  [[-0.0714 -0.0509 -0.0401 ... -0.0801 -0.0649 -0.0772]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2567 -0.1547 -0.2109 ... -0.1744 -0.2458 -0.1717]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 94, num timesteps 194560, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0312
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.7011
         discriminator_loss   0.9507
                  gail_loss   0.9316
                  grad_loss   0.0191
                    ib_loss  -0.1968
                  task_loss   0.0423
                       beta   0.0000
             posterior_loss   0.5025
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1005 -0.0783 -0.1689 ... -0.1034 -0.139  -0.057 ]]
 discr_rewards:  [[-0.1204 -0.1618 -0.0451 ... -0.0581 -0.0496 -0.0937]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.2604 -0.2795 -0.2535 ... -0.201  -0.2281 -0.1902]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 95, num timesteps 196608, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0328
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.7244
         discriminator_loss   0.9507
                  gail_loss   0.9289
                  grad_loss   0.0218
                    ib_loss  -0.1969
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.5222
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1369 -0.0527 -0.0856 ... -0.1519 -0.0607 -0.0524]]
 discr_rewards:  [[-0.1167 -0.1053 -0.0686 ...  0.0003 -0.0208 -0.1477]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.2931 -0.1975 -0.1937 ... -0.1911 -0.121  -0.2396]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 96, num timesteps 198656, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0338
                action_loss   0.0056
                    bc_loss   0.0000
               dist_entropy   0.7157
         discriminator_loss   0.9617
                  gail_loss   0.9413
                  grad_loss   0.0203
                    ib_loss  -0.1968
                  task_loss   0.0358
                       beta   0.0000
             posterior_loss   0.4981
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.074  -0.0657 -0.0681 ... -0.1814 -0.0592 -0.0913]]
 discr_rewards:  [[-0.0137 -0.0005 -0.0466 ... -0.0567 -0.0023 -0.1056]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1272 -0.1057 -0.1542 ... -0.2776 -0.1011 -0.2365]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 97, num timesteps 200704, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0324
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.7105
         discriminator_loss   0.9598
                  gail_loss   0.9399
                  grad_loss   0.0199
                    ib_loss  -0.1968
                  task_loss   0.0368
                       beta   0.0000
             posterior_loss   0.4923
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0582 -0.077  -0.0538 ... -0.1393 -0.1011 -0.0631]]
 discr_rewards:  [[-0.1203 -0.0508 -0.0605 ... -0.0489 -0.0573 -0.0686]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.2181 -0.1673 -0.1538 ... -0.2277 -0.1979 -0.1713]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 98, num timesteps 202752, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6905
         discriminator_loss   0.9514
                  gail_loss   0.9297
                  grad_loss   0.0216
                    ib_loss  -0.1967
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.4777
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0628 -0.0547 -0.1002 ... -0.0525 -0.1307 -0.3072]]
 discr_rewards:  [[-0.138  -0.0049 -0.2025 ... -0.0614 -0.0566 -0.0008]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2403 -0.0991 -0.3422 ... -0.1534 -0.2268 -0.3475]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 99, num timesteps 204800, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0362
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.7212
         discriminator_loss   0.9604
                  gail_loss   0.9417
                  grad_loss   0.0188
                    ib_loss  -0.1970
                  task_loss   0.0391
                       beta   0.0000
             posterior_loss   0.4939
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0604 -0.0892 -0.0589 ... -0.0882 -0.0547 -0.0532]]
 discr_rewards:  [[-0.1161 -0.0341 -0.0746 ... -0.0368 -0.0717 -0.1032]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.216  -0.1628 -0.173  ... -0.1645 -0.166  -0.1959]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 100, num timesteps 206848, FPS 151 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0369
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7112
         discriminator_loss   0.9585
                  gail_loss   0.9377
                  grad_loss   0.0208
                    ib_loss  -0.1970
                  task_loss   0.0431
                       beta   0.0000
             posterior_loss   0.5119
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0536 -0.0863 -0.0564 ... -0.0753 -0.1173 -0.25  ]]
 discr_rewards:  [[-0.0919 -0.054  -0.1764 ... -0.1493 -0.0338 -0.1043]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1849 -0.1799 -0.2723 ... -0.2641 -0.1905 -0.3937]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 101, num timesteps 208896, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0295
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7093
         discriminator_loss   0.9481
                  gail_loss   0.9291
                  grad_loss   0.0190
                    ib_loss  -0.1969
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.4796
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0555 -0.2257 ... -0.1645 -0.0536 -0.0537]]
 discr_rewards:  [[-0.008  -0.0269 -0.1187 ... -0.0256 -0.112  -0.0677]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.0999 -0.1219 -0.3839 ... -0.2296 -0.2051 -0.1609]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 102, num timesteps 210944, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0383
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.7144
         discriminator_loss   0.9599
                  gail_loss   0.9426
                  grad_loss   0.0173
                    ib_loss  -0.1971
                  task_loss   0.0340
                       beta   0.0000
             posterior_loss   0.5108
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1433 -0.0804 -0.063  ... -0.0538 -0.0581 -0.2939]]
 discr_rewards:  [[ 0.0029 -0.0358  0.0034 ... -0.0344 -0.0272 -0.0605]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1799 -0.1558 -0.0992 ... -0.1277 -0.1248 -0.394 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 103, num timesteps 212992, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0326
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7093
         discriminator_loss   0.9555
                  gail_loss   0.9362
                  grad_loss   0.0193
                    ib_loss  -0.1968
                  task_loss   0.0429
                       beta   0.0000
             posterior_loss   0.4918
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0917 -0.1036 -0.0528 ... -0.0534 -0.0542 -0.0531]]
 discr_rewards:  [[-0.0945 -0.0279 -0.1081 ... -0.0081 -0.0034 -0.1143]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2257 -0.171  -0.2004 ... -0.101  -0.0971 -0.207 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 104, num timesteps 215040, FPS 152 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.7031
         discriminator_loss   0.9463
                  gail_loss   0.9278
                  grad_loss   0.0185
                    ib_loss  -0.1972
                  task_loss   0.0351
                       beta   0.0000
             posterior_loss   0.5200
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1008 -0.0863 -0.0538 ... -0.1602 -0.0794 -0.0717]]
 discr_rewards:  [[-0.0418 -0.0126 -0.1772 ... -0.1961 -0.0441 -0.0084]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  7.4506e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1821 -0.1384 -0.2705 ... -0.3958 -0.163  -0.1196]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 105, num timesteps 217088, FPS 153 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0247
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.7013
         discriminator_loss   0.9461
                  gail_loss   0.9258
                  grad_loss   0.0204
                    ib_loss  -0.1974
                  task_loss   0.0381
                       beta   0.0000
             posterior_loss   0.4892
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2906 -0.0546 -0.0539 ... -0.0615 -0.0712 -0.0594]]
 discr_rewards:  [[-0.1581 -0.0399  0.0041 ... -0.1349 -0.1088 -0.069 ]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.4883 -0.134  -0.0893 ... -0.236  -0.2195 -0.168 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 106, num timesteps 219136, FPS 153 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7011
         discriminator_loss   0.9443
                  gail_loss   0.9222
                  grad_loss   0.0221
                    ib_loss  -0.1966
                  task_loss   0.0311
                       beta   0.0000
             posterior_loss   0.4993
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0676 -0.0532 -0.3132 ... -0.0684 -0.1177 -0.1598]]
 discr_rewards:  [[-0.0497 -0.0551 -0.0166 ... -0.0135 -0.0456 -0.0616]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1568 -0.1478 -0.3693 ... -0.1213 -0.2028 -0.2609]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 107, num timesteps 221184, FPS 153 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0422
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.7141
         discriminator_loss   0.9407
                  gail_loss   0.9217
                  grad_loss   0.0190
                    ib_loss  -0.1969
                  task_loss   0.0446
                       beta   0.0000
             posterior_loss   0.5281
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0575 -0.0847 -0.0557 ... -0.0551 -0.146  -0.1537]]
 discr_rewards:  [[-0.055  -0.0567 -0.0609 ... -0.1243 -0.0669 -0.1623]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.1521 -0.1809 -0.1561 ... -0.2189 -0.2524 -0.3555]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 108, num timesteps 223232, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0313
                action_loss   0.0046
                    bc_loss   0.0000
               dist_entropy   0.7069
         discriminator_loss   0.9421
                  gail_loss   0.9228
                  grad_loss   0.0193
                    ib_loss  -0.1969
                  task_loss   0.0486
                       beta   0.0000
             posterior_loss   0.5023
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.055  -0.0774 -0.0541 ... -0.1125 -0.0709 -0.1062]]
 discr_rewards:  [[-0.0807 -0.0703 -0.0447 ...  0.0102 -0.1182 -0.0343]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1752 -0.1873 -0.1383 ... -0.1419 -0.2287 -0.18  ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 109, num timesteps 225280, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss   0.0028
                    bc_loss   0.0000
               dist_entropy   0.7245
         discriminator_loss   0.9398
                  gail_loss   0.9207
                  grad_loss   0.0191
                    ib_loss  -0.1967
                  task_loss   0.0380
                       beta   0.0000
             posterior_loss   0.4840
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0806 -0.1408 -0.0763 ... -0.053  -0.1481 -0.0607]]
 discr_rewards:  [[-0.0281 -0.0301 -0.0457 ... -0.0643 -0.0606 -0.048 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1482 -0.2105 -0.1614 ... -0.1568 -0.2482 -0.1482]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 110, num timesteps 227328, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0345
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7130
         discriminator_loss   0.9362
                  gail_loss   0.9205
                  grad_loss   0.0157
                    ib_loss  -0.1972
                  task_loss   0.0366
                       beta   0.0000
             posterior_loss   0.5015
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1067 -0.0712 -0.087  ... -0.0579 -0.0595 -0.0821]]
 discr_rewards:  [[-0.0905  0.0066 -0.0257 ... -0.0824 -0.1029 -0.1248]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2368 -0.1041 -0.1523 ... -0.1798 -0.2019 -0.2464]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 111, num timesteps 229376, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0364
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.7237
         discriminator_loss   0.9480
                  gail_loss   0.9308
                  grad_loss   0.0173
                    ib_loss  -0.1970
                  task_loss   0.0374
                       beta   0.0000
             posterior_loss   0.5043
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.0564 -0.0692 ... -0.0561 -0.1851 -0.0577]]
 discr_rewards:  [[-0.0605 -0.066  -0.0527 ... -0.1096 -0.2047 -0.0635]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.1528 -0.1619 -0.1614 ... -0.2053 -0.4294 -0.1607]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 112, num timesteps 231424, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0334
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.7305
         discriminator_loss   0.9565
                  gail_loss   0.9382
                  grad_loss   0.0182
                    ib_loss  -0.1965
                  task_loss   0.0622
                       beta   0.0000
             posterior_loss   0.4919
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1062 -0.0864 -0.2778 ... -0.078  -0.0876 -0.0576]]
 discr_rewards:  [[-0.0861  0.001  -0.1887 ... -0.061  -0.0925 -0.114 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -7.4506e-09 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2318 -0.1249 -0.506  ... -0.1785 -0.2196 -0.2111]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 113, num timesteps 233472, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0396
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.7289
         discriminator_loss   0.9398
                  gail_loss   0.9214
                  grad_loss   0.0184
                    ib_loss  -0.1968
                  task_loss   0.0383
                       beta   0.0000
             posterior_loss   0.4948
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2207 -0.4629 -0.0527 ... -0.3706 -0.0571 -0.0948]]
 discr_rewards:  [[-0.0509 -0.1121 -0.1552 ... -0.0065 -0.0463 -0.0761]]
 task_rewards:  [[0.0000e+00 0.0000e+00 7.4506e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3111 -0.6146 -0.2473 ... -0.4167 -0.1429 -0.2104]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 114, num timesteps 235520, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0377
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.7347
         discriminator_loss   0.9520
                  gail_loss   0.9339
                  grad_loss   0.0181
                    ib_loss  -0.1968
                  task_loss   0.0488
                       beta   0.0000
             posterior_loss   0.5120
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0934 -0.0985 -0.0635 ... -0.078  -0.1271 -0.2414]]
 discr_rewards:  [[ 0.0129 -0.0231 -0.0194 ... -0.1058 -0.0608 -0.1491]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1201 -0.1611 -0.1223 ... -0.2233 -0.2275 -0.4301]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 115, num timesteps 237568, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0378
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.7352
         discriminator_loss   0.9467
                  gail_loss   0.9275
                  grad_loss   0.0192
                    ib_loss  -0.1970
                  task_loss   0.0436
                       beta   0.0000
             posterior_loss   0.5375
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0809 -0.0529 -0.0769 ... -0.0546 -0.0547 -0.0751]]
 discr_rewards:  [[-0.0684 -0.0766 -0.1435 ... -0.0049 -0.021  -0.0041]]
 task_rewards:  [[-3.7253e-09  3.7253e-09 -7.4506e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1889 -0.1691 -0.26   ... -0.0991 -0.1152 -0.1186]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 116, num timesteps 239616, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0360
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7434
         discriminator_loss   0.9447
                  gail_loss   0.9263
                  grad_loss   0.0184
                    ib_loss  -0.1967
                  task_loss   0.0426
                       beta   0.0000
             posterior_loss   0.5112
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0783 -0.0626 -0.0542 ... -0.0644 -0.1502 -0.0543]]
 discr_rewards:  [[-0.1965 -0.1534 -0.0169 ...  0.005  -0.2001 -0.0271]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.3144 -0.2555 -0.1106 ... -0.0989 -0.3898 -0.1208]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 117, num timesteps 241664, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0247
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.7180
         discriminator_loss   0.9424
                  gail_loss   0.9255
                  grad_loss   0.0169
                    ib_loss  -0.1968
                  task_loss   0.0413
                       beta   0.0000
             posterior_loss   0.5014
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0571 -0.0525 -0.0557 ... -0.0535 -0.1061 -0.0544]]
 discr_rewards:  [[-0.1025 -0.0495 -0.0443 ...  0.0107 -0.0163 -0.0037]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1991 -0.1416 -0.1395 ... -0.0823 -0.1619 -0.0975]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 118, num timesteps 243712, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0297
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.7258
         discriminator_loss   0.9355
                  gail_loss   0.9175
                  grad_loss   0.0180
                    ib_loss  -0.1971
                  task_loss   0.0279
                       beta   0.0000
             posterior_loss   0.5020
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0807 -0.2058 -0.1603 ... -0.0703 -0.0649 -0.098 ]]
 discr_rewards:  [[-0.028  -0.048  -0.0491 ... -0.015  -0.108  -0.0812]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.1482 -0.2934 -0.249  ... -0.1249 -0.2124 -0.2187]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 119, num timesteps 245760, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0249
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.7172
         discriminator_loss   0.9426
                  gail_loss   0.9245
                  grad_loss   0.0181
                    ib_loss  -0.1972
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.4766
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1303 -0.2982 -0.0532 ... -0.3205 -0.0698 -0.1935]]
 discr_rewards:  [[ 0.0004 -0.1714 -0.0443 ... -0.1528 -0.0142 -0.0128]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1694 -0.5091 -0.137  ... -0.5128 -0.1236 -0.2458]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 120, num timesteps 247808, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0798
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.7043
         discriminator_loss   0.9604
                  gail_loss   0.9422
                  grad_loss   0.0182
                    ib_loss  -0.1968
                  task_loss   0.1078
                       beta   0.0000
             posterior_loss   0.5302
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0531 -0.2729 -0.067  ... -0.2626 -0.2044 -0.0562]]
 discr_rewards:  [[-0.0159 -0.0129 -0.2114 ... -0.0798 -0.0378 -0.0218]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -7.4506e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1085 -0.3253 -0.318  ... -0.382  -0.2817 -0.1176]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 121, num timesteps 249856, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0319
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7021
         discriminator_loss   0.9409
                  gail_loss   0.9244
                  grad_loss   0.0165
                    ib_loss  -0.1969
                  task_loss   0.0736
                       beta   0.0000
             posterior_loss   0.4777
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0532 -0.096  -0.0706 ... -0.0652 -0.2593 -0.1754]]
 discr_rewards:  [[-0.0364 -0.0415 -0.0673 ... -0.1068 -0.0761 -0.0444]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1292 -0.177  -0.1774 ... -0.2115 -0.3749 -0.2593]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 122, num timesteps 251904, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0388
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7028
         discriminator_loss   0.9329
                  gail_loss   0.9147
                  grad_loss   0.0182
                    ib_loss  -0.1969
                  task_loss   0.0381
                       beta   0.0000
             posterior_loss   0.5167
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0808 -0.1629 -0.0532 ... -0.106  -0.0784 -0.1072]]
 discr_rewards:  [[-0.0511 -0.1011 -0.0244 ... -0.0141 -0.0732 -0.1037]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1715 -0.3036 -0.1171 ... -0.1596 -0.1911 -0.2504]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 123, num timesteps 253952, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0356
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.7024
         discriminator_loss   0.9259
                  gail_loss   0.9081
                  grad_loss   0.0178
                    ib_loss  -0.1968
                  task_loss   0.0438
                       beta   0.0000
             posterior_loss   0.4963
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1217 -0.0785 -0.1276 ... -0.111  -0.0706 -0.3876]]
 discr_rewards:  [[-0.0105 -0.1095 -0.0288 ... -0.0765 -0.0807 -0.0726]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1717 -0.2275 -0.1959 ... -0.2271 -0.1908 -0.4997]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 124, num timesteps 256000, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0303
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.7004
         discriminator_loss   0.9472
                  gail_loss   0.9300
                  grad_loss   0.0172
                    ib_loss  -0.1967
                  task_loss   0.0420
                       beta   0.0000
             posterior_loss   0.4783
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0559 -0.0681 -0.367  ... -0.1013 -0.1813 -0.184 ]]
 discr_rewards:  [[ 0.0009  0.0111 -0.0216 ... -0.0472 -0.103  -0.0405]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0946 -0.0965 -0.4281 ... -0.188  -0.3238 -0.2641]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 125, num timesteps 258048, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss  -0.0041
                    bc_loss   0.0000
               dist_entropy   0.7045
         discriminator_loss   0.9376
                  gail_loss   0.9194
                  grad_loss   0.0182
                    ib_loss  -0.1970
                  task_loss   0.0362
                       beta   0.0000
             posterior_loss   0.4995
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0827 -0.0846 -0.0527 ... -0.2718 -0.0639 -0.0554]]
 discr_rewards:  [[-0.1333 -0.1107 -0.0865 ... -0.0755 -0.0158 -0.081 ]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  3.7253e-09 ...  3.7253e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.2555 -0.2348 -0.1787 ... -0.3868 -0.1192 -0.176 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 126, num timesteps 260096, FPS 154 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0285
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.7016
         discriminator_loss   0.9455
                  gail_loss   0.9282
                  grad_loss   0.0173
                    ib_loss  -0.1970
                  task_loss   0.0329
                       beta   0.0000
             posterior_loss   0.4870
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1487 -0.0549 -0.1444 ... -0.1437 -0.0548 -0.1904]]
 discr_rewards:  [[-0.0167 -0.1088 -0.0276 ... -0.132  -0.0936 -0.1422]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2049 -0.2033 -0.2115 ... -0.3153 -0.1879 -0.3721]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 127, num timesteps 262144, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0268
                action_loss  -0.0035
                    bc_loss   0.0000
               dist_entropy   0.7015
         discriminator_loss   0.9329
                  gail_loss   0.9138
                  grad_loss   0.0191
                    ib_loss  -0.1970
                  task_loss   0.0302
                       beta   0.0000
             posterior_loss   0.5198
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0789 -0.1078 -0.0577 ... -0.059  -0.1977 -0.0709]]
 discr_rewards:  [[-0.141  -0.0231 -0.102  ... -0.1098 -0.0049 -0.0816]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2595 -0.1704 -0.1992 ... -0.2084 -0.2422 -0.192 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 128, num timesteps 264192, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0280
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7010
         discriminator_loss   0.9359
                  gail_loss   0.9183
                  grad_loss   0.0176
                    ib_loss  -0.1969
                  task_loss   0.0295
                       beta   0.0000
             posterior_loss   0.5059
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1586 -0.0746 -0.1309 ... -0.0547 -0.3193 -0.0823]]
 discr_rewards:  [[-0.1065 -0.027  -0.112  ... -0.0122 -0.0605 -0.201 ]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  3.7253e-09 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.3047 -0.1412 -0.2823 ... -0.1065 -0.4194 -0.3228]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 129, num timesteps 266240, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.7052
         discriminator_loss   0.9299
                  gail_loss   0.9140
                  grad_loss   0.0160
                    ib_loss  -0.1968
                  task_loss   0.0307
                       beta   0.0000
             posterior_loss   0.5066
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1366 -0.0616 -0.0555 ... -0.0557 -0.0573 -0.0663]]
 discr_rewards:  [[-0.0581 -0.004  -0.0038 ... -0.129  -0.1324 -0.0519]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2343 -0.1051 -0.0988 ... -0.2242 -0.2293 -0.1577]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 130, num timesteps 268288, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0237
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7023
         discriminator_loss   0.9282
                  gail_loss   0.9106
                  grad_loss   0.0176
                    ib_loss  -0.1969
                  task_loss   0.0356
                       beta   0.0000
             posterior_loss   0.5049
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1165 -0.2473 -0.271  ... -0.2208 -0.0525 -0.0538]]
 discr_rewards:  [[-0.1345 -0.0046 -0.0142 ... -0.1887 -0.187  -0.1783]]
 task_rewards:  [[ 7.4506e-09  0.0000e+00  0.0000e+00 ... -7.4506e-09  3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.2905 -0.2914 -0.3246 ... -0.449  -0.279  -0.2716]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 131, num timesteps 270336, FPS 155 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0272
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6955
         discriminator_loss   0.9333
                  gail_loss   0.9162
                  grad_loss   0.0171
                    ib_loss  -0.1969
                  task_loss   0.0271
                       beta   0.0000
             posterior_loss   0.4721
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0624 -0.0747 -0.0637 ... -0.0602 -0.0531 -0.1124]]
 discr_rewards:  [[ 0.0005 -0.1019 -0.003  ... -0.0712 -0.0055 -0.0537]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1014 -0.2161 -0.1062 ... -0.1709 -0.0982 -0.2056]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 132, num timesteps 272384, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0398
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6973
         discriminator_loss   0.9435
                  gail_loss   0.9262
                  grad_loss   0.0173
                    ib_loss  -0.1970
                  task_loss   0.0320
                       beta   0.0000
             posterior_loss   0.5010
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0554 -0.0533 -0.0943 ... -0.0524 -0.0524 -0.2701]]
 discr_rewards:  [[-0.1048 -0.1954 -0.0396 ... -0.1033 -0.1033 -0.0553]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1998 -0.2882 -0.1734 ... -0.1952 -0.1952 -0.365 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 133, num timesteps 274432, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0287
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.7049
         discriminator_loss   0.9377
                  gail_loss   0.9215
                  grad_loss   0.0162
                    ib_loss  -0.1969
                  task_loss   0.0461
                       beta   0.0000
             posterior_loss   0.4809
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1604 -0.0748 -0.102  ... -0.0547 -0.115  -0.1079]]
 discr_rewards:  [[-0.0534 -0.0203 -0.1142 ... -0.1239 -0.0172 -0.0082]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2533 -0.1346 -0.2557 ... -0.2181 -0.1717 -0.1556]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 134, num timesteps 276480, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0353
                action_loss   0.0015
                    bc_loss   0.0000
               dist_entropy   0.6983
         discriminator_loss   0.9528
                  gail_loss   0.9350
                  grad_loss   0.0178
                    ib_loss  -0.1970
                  task_loss   0.0321
                       beta   0.0000
             posterior_loss   0.4959
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0593 -0.1235 -0.0527 ... -0.0722 -0.1948 -0.0538]]
 discr_rewards:  [[-0.143  -0.0376  0.0137 ... -0.0592 -0.0816 -0.0822]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2418 -0.2006 -0.0785 ... -0.171  -0.3159 -0.1755]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 135, num timesteps 278528, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0221
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7022
         discriminator_loss   0.9533
                  gail_loss   0.9372
                  grad_loss   0.0162
                    ib_loss  -0.1967
                  task_loss   0.0424
                       beta   0.0000
             posterior_loss   0.4714
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1204 -0.1073 -0.0539 ... -0.0535 -0.0819 -0.0642]]
 discr_rewards:  [[-0.0692 -0.1444 -0.0458 ... -0.0265 -0.0541 -0.025 ]]
 task_rewards:  [[-3.7253e-09 -7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2291 -0.2912 -0.1392 ... -0.1194 -0.1755 -0.1287]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 136, num timesteps 280576, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0361
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6955
         discriminator_loss   0.9477
                  gail_loss   0.9324
                  grad_loss   0.0153
                    ib_loss  -0.1969
                  task_loss   0.0266
                       beta   0.0000
             posterior_loss   0.4949
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.081  -0.0551 -0.0568 ... -0.0535 -0.0838 -0.0562]]
 discr_rewards:  [[-0.1789 -0.0203 -0.1128 ... -0.0086 -0.1017 -0.1008]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00 -3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.2995 -0.1149 -0.2091 ... -0.1016 -0.225  -0.1965]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 137, num timesteps 282624, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0324
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6992
         discriminator_loss   0.9435
                  gail_loss   0.9285
                  grad_loss   0.0151
                    ib_loss  -0.1967
                  task_loss   0.0407
                       beta   0.0000
             posterior_loss   0.5245
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.058  -0.06   -0.2181 ... -0.059  -0.0694 -0.3718]]
 discr_rewards:  [[-0.1041 -0.0384 -0.0152 ... -0.0669 -0.0255 -0.0773]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2017 -0.138  -0.2728 ... -0.1653 -0.1344 -0.4886]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 138, num timesteps 284672, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0301
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.7077
         discriminator_loss   0.9457
                  gail_loss   0.9296
                  grad_loss   0.0161
                    ib_loss  -0.1968
                  task_loss   0.0399
                       beta   0.0000
             posterior_loss   0.5034
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0888 -0.0529 -0.0572 ... -0.1073 -0.1995 -0.0569]]
 discr_rewards:  [[-0.0102 -0.0283 -0.0201 ... -0.0249 -0.022  -0.0189]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1386 -0.1207 -0.1168 ... -0.1716 -0.261  -0.1153]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 139, num timesteps 286720, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0306
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7085
         discriminator_loss   0.9205
                  gail_loss   0.9040
                  grad_loss   0.0166
                    ib_loss  -0.1970
                  task_loss   0.0316
                       beta   0.0000
             posterior_loss   0.5227
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0683 -0.0768 -0.1307 ... -0.1936 -0.0608 -0.1029]]
 discr_rewards:  [[-0.0632 -0.0163 -0.1074 ... -0.0485 -0.1484 -0.0882]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.1711 -0.1326 -0.2777 ... -0.2816 -0.2487 -0.2306]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 140, num timesteps 288768, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0293
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6782
         discriminator_loss   0.9328
                  gail_loss   0.9174
                  grad_loss   0.0154
                    ib_loss  -0.1969
                  task_loss   0.0334
                       beta   0.0000
             posterior_loss   0.5016
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0642 -0.0837 -0.0559 ... -0.0632 -0.0778 -0.0563]]
 discr_rewards:  [[-0.0086 -0.0095 -0.0409 ... -0.0113 -0.0362 -0.051 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1124 -0.1327 -0.1362 ... -0.114  -0.1535 -0.1468]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 141, num timesteps 290816, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6886
         discriminator_loss   0.9319
                  gail_loss   0.9163
                  grad_loss   0.0157
                    ib_loss  -0.1965
                  task_loss   0.0311
                       beta   0.0000
             posterior_loss   0.4963
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0784 -0.4429 -0.1188 ... -0.1894 -0.0809 -0.0674]]
 discr_rewards:  [[-0.1026 -0.0705 -0.04   ... -0.1233 -0.1407 -0.0256]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2205 -0.5529 -0.1983 ... -0.3522 -0.2611 -0.1324]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 142, num timesteps 292864, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0287
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7048
         discriminator_loss   0.9387
                  gail_loss   0.9241
                  grad_loss   0.0146
                    ib_loss  -0.1969
                  task_loss   0.0379
                       beta   0.0000
             posterior_loss   0.4895
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0649 -0.1289 -0.2375 ... -0.0918 -0.0532 -0.0598]]
 discr_rewards:  [[-0.021  -0.0317 -0.0125 ... -0.0635 -0.1196 -0.0215]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1254 -0.2002 -0.2895 ... -0.1949 -0.2123 -0.1208]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 143, num timesteps 294912, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0390
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6994
         discriminator_loss   0.9339
                  gail_loss   0.9193
                  grad_loss   0.0146
                    ib_loss  -0.1968
                  task_loss   0.0368
                       beta   0.0000
             posterior_loss   0.5239
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0533 -0.0958 -0.0744 ... -0.1766 -0.1008 -0.1699]]
 discr_rewards:  [[-0.1221 -0.0037 -0.045  ... -0.0886 -0.0706 -0.0468]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2149 -0.139  -0.1589 ... -0.3047 -0.2109 -0.2562]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 144, num timesteps 296960, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0390
                action_loss  -0.0032
                    bc_loss   0.0000
               dist_entropy   0.7097
         discriminator_loss   0.9285
                  gail_loss   0.9136
                  grad_loss   0.0150
                    ib_loss  -0.1969
                  task_loss   0.0437
                       beta   0.0000
             posterior_loss   0.5140
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0669 -0.0558 -0.1053 ... -0.1272 -0.0979 -0.055 ]]
 discr_rewards:  [[-0.0108 -0.0594 -0.0345 ... -0.013   0.0057 -0.0782]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1172 -0.1548 -0.1793 ... -0.1797 -0.1317 -0.1726]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 145, num timesteps 299008, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0256
                action_loss   0.0100
                    bc_loss   0.0000
               dist_entropy   0.7222
         discriminator_loss   0.9425
                  gail_loss   0.9267
                  grad_loss   0.0158
                    ib_loss  -0.1970
                  task_loss   0.0428
                       beta   0.0000
             posterior_loss   0.4847
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1063 -0.0685 -0.0611 ... -0.0616 -0.1644 -0.0592]]
 discr_rewards:  [[-0.0725 -0.0637 -0.0918 ... -0.0125 -0.0875  0.0067]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2184 -0.1717 -0.1924 ... -0.1136 -0.2914 -0.092 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 146, num timesteps 301056, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0390
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7151
         discriminator_loss   0.9471
                  gail_loss   0.9322
                  grad_loss   0.0150
                    ib_loss  -0.1970
                  task_loss   0.0306
                       beta   0.0000
             posterior_loss   0.5101
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0536 -0.0763 -0.0536 ... -0.0527 -0.0524 -0.21  ]]
 discr_rewards:  [[-0.0521 -0.1152 -0.0874 ...  0.0121 -0.0347 -0.0249]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1452 -0.231  -0.1805 ... -0.0802 -0.1266 -0.2744]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 147, num timesteps 303104, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0285
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7029
         discriminator_loss   0.9380
                  gail_loss   0.9251
                  grad_loss   0.0129
                    ib_loss  -0.1968
                  task_loss   0.0434
                       beta   0.0000
             posterior_loss   0.4876
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.074  -0.1622 -0.0817 ... -0.1501 -0.1242 -0.1247]]
 discr_rewards:  [[-0.0163 -0.0194  0.005  ... -0.0267  0.0015 -0.0053]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1298 -0.2211 -0.1162 ... -0.2163 -0.1623 -0.1695]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 148, num timesteps 305152, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0271
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7111
         discriminator_loss   0.9486
                  gail_loss   0.9359
                  grad_loss   0.0126
                    ib_loss  -0.1968
                  task_loss   0.0358
                       beta   0.0000
             posterior_loss   0.4725
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0875 -0.1688 -0.149  ... -0.0689 -0.0596 -0.053 ]]
 discr_rewards:  [[-0.0438 -0.15   -0.0398 ... -0.0268  0.0006 -0.0991]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1708 -0.3583 -0.2284 ... -0.1352 -0.0985 -0.1916]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 149, num timesteps 307200, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0272
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7057
         discriminator_loss   0.9368
                  gail_loss   0.9227
                  grad_loss   0.0141
                    ib_loss  -0.1969
                  task_loss   0.0307
                       beta   0.0000
             posterior_loss   0.5092
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1557 -0.06   -0.0569 ... -0.1251 -0.0638 -0.053 ]]
 discr_rewards:  [[-0.075  -0.0456 -0.0201 ... -0.1016 -0.0021 -0.0853]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2702 -0.1452 -0.1164 ... -0.2662 -0.1055 -0.1777]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 150, num timesteps 309248, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0260
                action_loss   0.0008
                    bc_loss   0.0000
               dist_entropy   0.7091
         discriminator_loss   0.9305
                  gail_loss   0.9159
                  grad_loss   0.0146
                    ib_loss  -0.1970
                  task_loss   0.0302
                       beta   0.0000
             posterior_loss   0.4731
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0538 -0.0568 -0.0692 ... -0.0674 -0.1088 -0.0527]]
 discr_rewards:  [[-0.0531 -0.0346 -0.0635 ... -0.005  -0.1213 -0.0244]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1464 -0.1309 -0.1722 ... -0.112  -0.2697 -0.1166]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 151, num timesteps 311296, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0375
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.7028
         discriminator_loss   0.9404
                  gail_loss   0.9249
                  grad_loss   0.0156
                    ib_loss  -0.1969
                  task_loss   0.0285
                       beta   0.0000
             posterior_loss   0.5316
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0584 -0.0906 -0.0677 ... -0.1136 -0.0527 -0.1436]]
 discr_rewards:  [[-0.1771 -0.1713 -0.0362 ... -0.0046 -0.0957 -0.0813]]
 task_rewards:  [[3.7253e-09 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.2751 -0.3014 -0.1434 ... -0.1577 -0.1879 -0.2644]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 152, num timesteps 313344, FPS 156 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0333
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6970
         discriminator_loss   0.9369
                  gail_loss   0.9226
                  grad_loss   0.0143
                    ib_loss  -0.1970
                  task_loss   0.0419
                       beta   0.0000
             posterior_loss   0.5182
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0599 -0.0731 -0.0524 ... -0.1649 -0.1197 -0.0739]]
 discr_rewards:  [[-0.077  -0.0536 -0.0062 ... -0.0944 -0.1302 -0.0464]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1764 -0.1663 -0.0981 ... -0.2988 -0.2894 -0.1598]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 153, num timesteps 315392, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0290
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7061
         discriminator_loss   0.9355
                  gail_loss   0.9197
                  grad_loss   0.0158
                    ib_loss  -0.1967
                  task_loss   0.0346
                       beta   0.0000
             posterior_loss   0.5169
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0813 -0.0527 -0.0524 ... -0.053  -0.0776 -0.1866]]
 discr_rewards:  [[-0.0353 -0.1378 -0.0039 ... -0.1691 -0.0347 -0.0841]]
 task_rewards:  [[ 0.0000e+00  7.4506e-09  0.0000e+00 ...  7.4506e-09  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1561 -0.2301 -0.0959 ... -0.2617 -0.1518 -0.3102]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 154, num timesteps 317440, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0324
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.7034
         discriminator_loss   0.9349
                  gail_loss   0.9210
                  grad_loss   0.0140
                    ib_loss  -0.1970
                  task_loss   0.0308
                       beta   0.0000
             posterior_loss   0.5239
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1906 -0.0541 -0.0524 ... -0.0525 -0.1142 -0.0997]]
 discr_rewards:  [[-0.0489 -0.0658 -0.0287 ... -0.0146 -0.033  -0.1349]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.279  -0.1594 -0.1206 ... -0.1066 -0.1867 -0.2741]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 155, num timesteps 319488, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.7249
         discriminator_loss   0.9413
                  gail_loss   0.9267
                  grad_loss   0.0146
                    ib_loss  -0.1968
                  task_loss   0.0370
                       beta   0.0000
             posterior_loss   0.5081
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1051 -0.1018 -0.158  ... -0.0788 -0.0527 -0.087 ]]
 discr_rewards:  [[-0.0209 -0.1055 -0.0263 ... -0.0291 -0.2049 -0.0062]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1655 -0.2468 -0.2237 ... -0.1474 -0.2971 -0.1327]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 156, num timesteps 321536, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0315
                action_loss   0.0011
                    bc_loss   0.0000
               dist_entropy   0.7109
         discriminator_loss   0.9258
                  gail_loss   0.9128
                  grad_loss   0.0130
                    ib_loss  -0.1970
                  task_loss   0.0353
                       beta   0.0000
             posterior_loss   0.5146
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0603 -0.1143 -0.1339 ... -0.1162 -0.0837 -0.08  ]]
 discr_rewards:  [[-0.0569 -0.0168 -0.0671 ... -0.0145 -0.054  -0.106 ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1567 -0.1707 -0.2406 ... -0.1702 -0.1772 -0.2256]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 157, num timesteps 323584, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0378
                action_loss   0.0014
                    bc_loss   0.0000
               dist_entropy   0.7196
         discriminator_loss   0.9305
                  gail_loss   0.9147
                  grad_loss   0.0158
                    ib_loss  -0.1967
                  task_loss   0.0359
                       beta   0.0000
             posterior_loss   0.5007
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0577 -0.1098 -0.0803 ... -0.0872 -0.0648 -0.0745]]
 discr_rewards:  [[-0.0409 -0.0127 -0.0366 ...  0.0035 -0.1098 -0.037 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1382 -0.1621 -0.1565 ... -0.1232 -0.2142 -0.151 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 158, num timesteps 325632, FPS 157 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.7241
         discriminator_loss   0.9288
                  gail_loss   0.9127
                  grad_loss   0.0161
                    ib_loss  -0.1971
                  task_loss   0.0424
                       beta   0.0000
             posterior_loss   0.5168
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.0621 -0.1506 ... -0.0923 -0.0659 -0.2316]]
 discr_rewards:  [[-0.0162 -0.0738 -0.0409 ... -0.0397 -0.024  -0.0605]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1083 -0.1754 -0.231  ... -0.1716 -0.1294 -0.3317]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 159, num timesteps 327680, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7198
         discriminator_loss   0.9376
                  gail_loss   0.9227
                  grad_loss   0.0149
                    ib_loss  -0.1968
                  task_loss   0.0368
                       beta   0.0000
             posterior_loss   0.5012
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0643 -0.0835 ... -0.1142 -0.0699 -0.0535]]
 discr_rewards:  [[-0.0491 -0.0749 -0.0506 ... -0.1134 -0.1328 -0.0102]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.141  -0.1787 -0.1736 ... -0.2671 -0.2421 -0.1033]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 160, num timesteps 329728, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0378
                action_loss   0.0013
                    bc_loss   0.0000
               dist_entropy   0.7162
         discriminator_loss   0.9303
                  gail_loss   0.9149
                  grad_loss   0.0154
                    ib_loss  -0.1971
                  task_loss   0.0356
                       beta   0.0000
             posterior_loss   0.5279
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0698 -0.0567 -0.0579 ... -0.0533 -0.0639 -0.0603]]
 discr_rewards:  [[-0.1372 -0.0274 -0.0755 ... -0.024  -0.0202 -0.0937]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2466 -0.1236 -0.173  ... -0.1168 -0.1236 -0.1935]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 161, num timesteps 331776, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0299
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.7056
         discriminator_loss   0.9353
                  gail_loss   0.9210
                  grad_loss   0.0143
                    ib_loss  -0.1969
                  task_loss   0.0419
                       beta   0.0000
             posterior_loss   0.5035
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0647 -0.0525 -0.1116 ... -0.0852 -0.1576 -0.0952]]
 discr_rewards:  [[-0.057  -0.0305 -0.0047 ... -0.0348 -0.0077 -0.1262]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1611 -0.1225 -0.1559 ... -0.1595 -0.2048 -0.2609]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 162, num timesteps 333824, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0401
                action_loss  -0.0048
                    bc_loss   0.0000
               dist_entropy   0.7133
         discriminator_loss   0.9389
                  gail_loss   0.9243
                  grad_loss   0.0146
                    ib_loss  -0.1964
                  task_loss   0.0379
                       beta   0.0000
             posterior_loss   0.5036
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0576 -0.078  -0.1894 ... -0.063  -0.0801 -0.1266]]
 discr_rewards:  [[-0.1106 -0.0654 -0.0603 ... -0.0653 -0.0442 -0.0571]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2077 -0.1829 -0.2893 ... -0.1679 -0.1638 -0.2232]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 163, num timesteps 335872, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7030
         discriminator_loss   0.9267
                  gail_loss   0.9136
                  grad_loss   0.0131
                    ib_loss  -0.1968
                  task_loss   0.0454
                       beta   0.0000
             posterior_loss   0.5223
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0561 -0.0532 -0.0868 ... -0.0527 -0.0745 -0.0786]]
 discr_rewards:  [[-0.0285 -0.0457 -0.0401 ... -0.0419 -0.0418 -0.0403]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1241 -0.1384 -0.1665 ... -0.1341 -0.1558 -0.1583]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 164, num timesteps 337920, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss  -0.0036
                    bc_loss   0.0000
               dist_entropy   0.7077
         discriminator_loss   0.9410
                  gail_loss   0.9271
                  grad_loss   0.0140
                    ib_loss  -0.1969
                  task_loss   0.0402
                       beta   0.0000
             posterior_loss   0.5248
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0934 -0.0713 -0.1246 ... -0.0584 -0.0603 -0.1492]]
 discr_rewards:  [[-0.0241 -0.0924 -0.0659 ... -0.0119 -0.0891 -0.0259]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.157  -0.2031 -0.23   ... -0.1098 -0.189  -0.2146]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 165, num timesteps 339968, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0329
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.7033
         discriminator_loss   0.9249
                  gail_loss   0.9108
                  grad_loss   0.0141
                    ib_loss  -0.1969
                  task_loss   0.0360
                       beta   0.0000
             posterior_loss   0.4951
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0711 -0.0524 -0.066  ... -0.0524 -0.0721 -0.0645]]
 discr_rewards:  [[-0.0982  0.0116 -0.036  ... -0.0168 -0.0325 -0.1254]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2088 -0.0804 -0.1415 ... -0.1087 -0.1441 -0.2293]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 166, num timesteps 342016, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0374
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.7115
         discriminator_loss   0.9455
                  gail_loss   0.9318
                  grad_loss   0.0136
                    ib_loss  -0.1971
                  task_loss   0.0404
                       beta   0.0000
             posterior_loss   0.5309
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1761 -0.0683 -0.0587 ... -0.0806 -0.07   -0.0669]]
 discr_rewards:  [[-0.0287 -0.091  -0.074  ... -0.091  -0.0739 -0.1776]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  3.7253e-09  3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2443 -0.1987 -0.1722 ... -0.2111 -0.1834 -0.284 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 167, num timesteps 344064, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0402
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7197
         discriminator_loss   0.9519
                  gail_loss   0.9373
                  grad_loss   0.0145
                    ib_loss  -0.1970
                  task_loss   0.0528
                       beta   0.0000
             posterior_loss   0.5177
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.065  -0.0525 -0.0537 ... -0.0526 -0.0536 -0.0966]]
 discr_rewards:  [[-0.0091 -0.0411 -0.0758 ... -0.0138 -0.0307 -0.0878]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1136 -0.133  -0.169  ... -0.106  -0.1238 -0.2239]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 168, num timesteps 346112, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0347
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7050
         discriminator_loss   0.9271
                  gail_loss   0.9142
                  grad_loss   0.0129
                    ib_loss  -0.1968
                  task_loss   0.0471
                       beta   0.0000
             posterior_loss   0.4696
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.183  -0.071  -0.059  ... -0.0617 -0.0525 -0.1074]]
 discr_rewards:  [[-0.0726 -0.0137 -0.0837 ... -0.0995 -0.0413 -0.1074]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -3.7253e-09 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2951 -0.1242 -0.1822 ... -0.2007 -0.1333 -0.2543]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 169, num timesteps 348160, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0265
                action_loss  -0.0041
                    bc_loss   0.0000
               dist_entropy   0.7067
         discriminator_loss   0.9398
                  gail_loss   0.9260
                  grad_loss   0.0138
                    ib_loss  -0.1970
                  task_loss   0.0391
                       beta   0.0000
             posterior_loss   0.5227
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0567 -0.0667 -0.063  ... -0.0889 -0.0555 -0.0682]]
 discr_rewards:  [[-0.0468 -0.093  -0.14   ... -0.0108 -0.0774 -0.043 ]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.143  -0.1993 -0.2426 ... -0.1393 -0.1725 -0.1507]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 170, num timesteps 350208, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0281
                action_loss   0.0229
                    bc_loss   0.0000
               dist_entropy   0.7087
         discriminator_loss   0.9441
                  gail_loss   0.9283
                  grad_loss   0.0158
                    ib_loss  -0.1970
                  task_loss   0.0286
                       beta   0.0000
             posterior_loss   0.4743
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0748 -0.0723 -0.1054 ... -0.0681 -0.0858 -0.0747]]
 discr_rewards:  [[-0.039  -0.0968 -0.0348 ... -0.1316 -0.0255 -0.1544]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 3.7253e-09 0.0000e+00 7.4506e-09]]
 final_rewards:  [[-0.1533 -0.2087 -0.1797 ... -0.2393 -0.1509 -0.2686]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 171, num timesteps 352256, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0296
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6974
         discriminator_loss   0.9350
                  gail_loss   0.9226
                  grad_loss   0.0124
                    ib_loss  -0.1966
                  task_loss   0.0313
                       beta   0.0000
             posterior_loss   0.5107
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0865 -0.1068 -0.0541 ... -0.0545 -0.0607 -0.0652]]
 discr_rewards:  [[-0.1047  0.0047 -0.1417 ... -0.077  -0.0793 -0.0692]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  3.7253e-09 -3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2307 -0.1416 -0.2354 ... -0.1711 -0.1796 -0.1739]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 172, num timesteps 354304, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7021
         discriminator_loss   0.9319
                  gail_loss   0.9196
                  grad_loss   0.0123
                    ib_loss  -0.1967
                  task_loss   0.0320
                       beta   0.0000
             posterior_loss   0.5135
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0588 -0.0862 -0.0577 ... -0.1218 -0.0567 -0.0678]]
 discr_rewards:  [[-0.0581 -0.0597 -0.0364 ... -0.1577 -0.0212 -0.0886]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1564 -0.1854 -0.1336 ... -0.319  -0.1174 -0.1959]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 173, num timesteps 356352, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0371
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.7061
         discriminator_loss   0.9406
                  gail_loss   0.9280
                  grad_loss   0.0126
                    ib_loss  -0.1969
                  task_loss   0.0394
                       beta   0.0000
             posterior_loss   0.5144
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0563 -0.0681 -0.1538 ... -0.0752 -0.0524 -0.0848]]
 discr_rewards:  [[-0.1395 -0.1161 -0.0124 ... -0.0198 -0.219  -0.0671]]
 task_rewards:  [[ 3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2353 -0.2237 -0.2057 ... -0.1345 -0.311  -0.1914]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 174, num timesteps 358400, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0350
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.6985
         discriminator_loss   0.9217
                  gail_loss   0.9091
                  grad_loss   0.0126
                    ib_loss  -0.1967
                  task_loss   0.0424
                       beta   0.0000
             posterior_loss   0.4994
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0679 -0.0524 -0.1617 ... -0.1019 -0.1249 -0.0825]]
 discr_rewards:  [[-0.0383 -0.1318 -0.1035 ... -0.0472 -0.0287 -0.0472]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1458 -0.2238 -0.3048 ... -0.1886 -0.1932 -0.1691]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 175, num timesteps 360448, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0252
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7000
         discriminator_loss   0.9275
                  gail_loss   0.9145
                  grad_loss   0.0129
                    ib_loss  -0.1972
                  task_loss   0.0387
                       beta   0.0000
             posterior_loss   0.5019
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1414 -0.0571 -0.0632 ... -0.0644 -0.0783 -0.0527]]
 discr_rewards:  [[-0.1127 -0.0814 -0.0375 ... -0.0216 -0.035  -0.0526]]
 task_rewards:  [[-3.7253e-09 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2937 -0.178  -0.1402 ... -0.1255 -0.1528 -0.1448]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 176, num timesteps 362496, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0256
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7055
         discriminator_loss   0.9292
                  gail_loss   0.9156
                  grad_loss   0.0137
                    ib_loss  -0.1970
                  task_loss   0.0272
                       beta   0.0000
             posterior_loss   0.4958
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0704 -0.1203 -0.0613 ... -0.0568 -0.0575 -0.137 ]]
 discr_rewards:  [[-0.0274 -0.0431 -0.0687 ... -0.0706 -0.0682 -0.0239]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1373 -0.2028 -0.1696 ... -0.1669 -0.1652 -0.2003]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 177, num timesteps 364544, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0314
                action_loss  -0.0045
                    bc_loss   0.0000
               dist_entropy   0.6922
         discriminator_loss   0.9336
                  gail_loss   0.9188
                  grad_loss   0.0147
                    ib_loss  -0.1972
                  task_loss   0.0280
                       beta   0.0000
             posterior_loss   0.5036
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1798 -0.1483 -0.0535 ... -0.0772 -0.1139 -0.088 ]]
 discr_rewards:  [[-0.0177 -0.11   -0.1013 ... -0.0767 -0.2197 -0.0318]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2371 -0.2979 -0.1943 ... -0.1934 -0.3731 -0.1593]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 178, num timesteps 366592, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0248
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.6955
         discriminator_loss   0.9237
                  gail_loss   0.9123
                  grad_loss   0.0114
                    ib_loss  -0.1972
                  task_loss   0.0345
                       beta   0.0000
             posterior_loss   0.4915
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0527 -0.0526 -0.2315 ... -0.0581 -0.0527 -0.0524]]
 discr_rewards:  [[-0.1595 -0.0792 -0.0436 ... -0.032  -0.1337 -0.021 ]]
 task_rewards:  [[ 7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.2517 -0.1713 -0.3146 ... -0.1296 -0.2259 -0.113 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 179, num timesteps 368640, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0270
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7030
         discriminator_loss   0.9338
                  gail_loss   0.9211
                  grad_loss   0.0126
                    ib_loss  -0.1972
                  task_loss   0.0269
                       beta   0.0000
             posterior_loss   0.4822
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0723 -0.0558 -0.1157 ... -0.1813 -0.128  -0.0588]]
 discr_rewards:  [[-0.0203 -0.0594 -0.0678 ... -0.0052 -0.0499 -0.1504]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.1322 -0.1547 -0.223  ... -0.226  -0.2175 -0.2487]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 180, num timesteps 370688, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0273
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6998
         discriminator_loss   0.9295
                  gail_loss   0.9172
                  grad_loss   0.0123
                    ib_loss  -0.1969
                  task_loss   0.0294
                       beta   0.0000
             posterior_loss   0.5138
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.055  -0.0525 -0.1086 ... -0.0606 -0.0525 -0.0919]]
 discr_rewards:  [[-0.0156 -0.1213 -0.0859 ... -0.0436 -0.1453 -0.0355]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1101 -0.2133 -0.234  ... -0.1437 -0.2373 -0.167 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 181, num timesteps 372736, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0381
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7008
         discriminator_loss   0.9495
                  gail_loss   0.9333
                  grad_loss   0.0162
                    ib_loss  -0.1970
                  task_loss   0.0290
                       beta   0.0000
             posterior_loss   0.5235
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0536 -0.116  -0.0524 ... -0.1072 -0.2574 -0.0639]]
 discr_rewards:  [[-0.1083 -0.0525 -0.1046 ... -0.0367 -0.124  -0.047 ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2014 -0.208  -0.1966 ... -0.1834 -0.4209 -0.1505]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 182, num timesteps 374784, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0366
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7121
         discriminator_loss   0.9432
                  gail_loss   0.9291
                  grad_loss   0.0141
                    ib_loss  -0.1969
                  task_loss   0.0420
                       beta   0.0000
             posterior_loss   0.4806
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0591 -0.0586 -0.0524 ... -0.0638 -0.0586 -0.0769]]
 discr_rewards:  [[-0.0113 -0.0849 -0.0943 ... -0.083  -0.0625 -0.0415]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1099 -0.1831 -0.1863 ... -0.1863 -0.1605 -0.1579]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 183, num timesteps 376832, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0335
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7053
         discriminator_loss   0.9372
                  gail_loss   0.9222
                  grad_loss   0.0150
                    ib_loss  -0.1968
                  task_loss   0.0467
                       beta   0.0000
             posterior_loss   0.5005
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0601 -0.0599 -0.1304 ... -0.0524 -0.0836 -0.1543]]
 discr_rewards:  [[-0.0322 -0.0047 -0.0124 ... -0.083  -0.0878 -0.0094]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1319 -0.1041 -0.1822 ... -0.1749 -0.2109 -0.2032]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 184, num timesteps 378880, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0288
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6985
         discriminator_loss   0.9198
                  gail_loss   0.9080
                  grad_loss   0.0118
                    ib_loss  -0.1966
                  task_loss   0.0363
                       beta   0.0000
             posterior_loss   0.5134
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0929 -0.0547 -0.1111 ... -0.0524 -0.0651 -0.0818]]
 discr_rewards:  [[-0.0803 -0.0653 -0.0655 ... -0.1432 -0.0109 -0.0639]]
 task_rewards:  [[-3.7253e-09  3.7253e-09 -3.7253e-09 ...  7.4506e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.2128 -0.1595 -0.2161 ... -0.2351 -0.1155 -0.1853]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 185, num timesteps 380928, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0205
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.7047
         discriminator_loss   0.9283
                  gail_loss   0.9161
                  grad_loss   0.0122
                    ib_loss  -0.1969
                  task_loss   0.0323
                       beta   0.0000
             posterior_loss   0.4860
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.453  -0.0997 -0.0955 ... -0.0533 -0.0572 -0.0536]]
 discr_rewards:  [[-0.0563 -0.057  -0.024  ... -0.1318 -0.0431 -0.0417]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.5488 -0.1962 -0.1591 ... -0.2246 -0.1399 -0.1348]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 186, num timesteps 382976, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.7017
         discriminator_loss   0.9256
                  gail_loss   0.9125
                  grad_loss   0.0132
                    ib_loss  -0.1967
                  task_loss   0.0238
                       beta   0.0000
             posterior_loss   0.5097
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0587 -0.0547 -0.1016 ... -0.1172 -0.4033 -0.0528]]
 discr_rewards:  [[-0.0281 -0.1453 -0.0043 ... -0.1556 -0.0726 -0.0616]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  7.4506e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1263 -0.2396 -0.1453 ... -0.3124 -0.5153 -0.1539]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 187, num timesteps 385024, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0359
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7081
         discriminator_loss   0.9340
                  gail_loss   0.9210
                  grad_loss   0.0130
                    ib_loss  -0.1968
                  task_loss   0.0329
                       beta   0.0000
             posterior_loss   0.4879
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0719 -0.058  -0.0525 ... -0.0531 -0.0664 -0.0559]]
 discr_rewards:  [[-0.0816 -0.0494 -0.0255 ... -0.0974 -0.0958 -0.0239]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.193  -0.1469 -0.1175 ... -0.19   -0.2017 -0.1193]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 188, num timesteps 387072, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0361
                action_loss  -0.0052
                    bc_loss   0.0000
               dist_entropy   0.7034
         discriminator_loss   0.9212
                  gail_loss   0.9093
                  grad_loss   0.0119
                    ib_loss  -0.1968
                  task_loss   0.0438
                       beta   0.0000
             posterior_loss   0.5227
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0585 -0.2948 -0.053  ... -0.0667 -0.0531 -0.0527]]
 discr_rewards:  [[-0.0509 -0.0316 -0.0734 ... -0.0351 -0.0292 -0.0172]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1489 -0.3659 -0.1659 ... -0.1413 -0.1218 -0.1094]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 189, num timesteps 389120, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0334
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6926
         discriminator_loss   0.9132
                  gail_loss   0.9010
                  grad_loss   0.0122
                    ib_loss  -0.1969
                  task_loss   0.0406
                       beta   0.0000
             posterior_loss   0.5174
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0554 -0.3931 -0.1191 ... -0.0991 -0.1525 -0.0682]]
 discr_rewards:  [[ 0.0021 -0.0547 -0.0198 ... -0.0405 -0.1209 -0.0215]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0928 -0.4874 -0.1784 ... -0.1792 -0.3129 -0.1292]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 190, num timesteps 391168, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0338
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6952
         discriminator_loss   0.9341
                  gail_loss   0.9206
                  grad_loss   0.0135
                    ib_loss  -0.1969
                  task_loss   0.0430
                       beta   0.0000
             posterior_loss   0.5076
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1082 -0.0654 -0.1283 ... -0.0691 -0.0537 -0.1704]]
 discr_rewards:  [[-0.023  -0.0272 -0.015  ... -0.0852 -0.0377 -0.0824]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1707 -0.1322 -0.1828 ... -0.1938 -0.131  -0.2923]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 191, num timesteps 393216, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0346
                action_loss  -0.0035
                    bc_loss   0.0000
               dist_entropy   0.6984
         discriminator_loss   0.9191
                  gail_loss   0.9067
                  grad_loss   0.0124
                    ib_loss  -0.1967
                  task_loss   0.0389
                       beta   0.0000
             posterior_loss   0.5038
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0561 -0.1544 -0.0798 ... -0.0526 -0.0659 -0.0968]]
 discr_rewards:  [[-0.0241 -0.0996 -0.0369 ... -0.0473 -0.0944 -0.1198]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1197 -0.2935 -0.1562 ... -0.1394 -0.1998 -0.2561]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 192, num timesteps 395264, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.7191
         discriminator_loss   0.9274
                  gail_loss   0.9143
                  grad_loss   0.0131
                    ib_loss  -0.1969
                  task_loss   0.0392
                       beta   0.0000
             posterior_loss   0.4865
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0659 -0.1481 -0.1806 ... -0.2195 -0.0699 -0.0553]]
 discr_rewards:  [[-0.0336 -0.0875 -0.035  ... -0.1152 -0.1638 -0.2202]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 3.7253e-09 3.7253e-09]]
 final_rewards:  [[-0.139  -0.2751 -0.2551 ... -0.3743 -0.2732 -0.315 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 193, num timesteps 397312, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0335
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7223
         discriminator_loss   0.9363
                  gail_loss   0.9213
                  grad_loss   0.0150
                    ib_loss  -0.1971
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.5087
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0752 -0.1029 -0.0545 ... -0.0587 -0.0833 -0.0589]]
 discr_rewards:  [[-0.0792 -0.017  -0.0427 ... -0.0239 -0.0546 -0.0145]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1939 -0.1594 -0.1367 ... -0.1221 -0.1774 -0.113 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 194, num timesteps 399360, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0374
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7155
         discriminator_loss   0.9285
                  gail_loss   0.9148
                  grad_loss   0.0138
                    ib_loss  -0.1966
                  task_loss   0.0355
                       beta   0.0000
             posterior_loss   0.5139
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2434 -0.4698 -0.0551 ... -0.0565 -0.068  -0.2859]]
 discr_rewards:  [[-0.0246 -0.0846 -0.058  ... -0.2035 -0.1182 -0.042 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.3076 -0.5939 -0.1526 ... -0.2996 -0.2256 -0.3674]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 195, num timesteps 401408, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0221
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.7064
         discriminator_loss   0.9305
                  gail_loss   0.9168
                  grad_loss   0.0137
                    ib_loss  -0.1966
                  task_loss   0.0425
                       beta   0.0000
             posterior_loss   0.4921
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.1062 -0.0579 ... -0.0546 -0.0645 -0.0547]]
 discr_rewards:  [[-0.0589 -0.049  -0.0126 ... -0.0841  0.0088 -0.0791]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1508 -0.1947 -0.11   ... -0.1782 -0.0953 -0.1734]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 196, num timesteps 403456, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6915
         discriminator_loss   0.9358
                  gail_loss   0.9227
                  grad_loss   0.0131
                    ib_loss  -0.1969
                  task_loss   0.0248
                       beta   0.0000
             posterior_loss   0.4779
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0531 -0.0605 -0.0946 ... -0.0954 -0.0546 -0.0885]]
 discr_rewards:  [[-0.0202 -0.1104 -0.11   ... -0.0896 -0.0255 -0.1226]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1127 -0.2104 -0.2441 ... -0.2246 -0.1196 -0.2507]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 197, num timesteps 405504, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0378
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.6954
         discriminator_loss   0.9242
                  gail_loss   0.9117
                  grad_loss   0.0126
                    ib_loss  -0.1967
                  task_loss   0.0338
                       beta   0.0000
             posterior_loss   0.5008
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1125 -0.062  -0.0683 ... -0.1289 -0.0552 -0.143 ]]
 discr_rewards:  [[-0.0817 -0.063  -0.0391 ... -0.0255 -0.1163  0.0033]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2336 -0.1646 -0.1469 ... -0.1939 -0.2111 -0.1792]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 198, num timesteps 407552, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss  -0.0058
                    bc_loss   0.0000
               dist_entropy   0.7075
         discriminator_loss   0.9198
                  gail_loss   0.9077
                  grad_loss   0.0121
                    ib_loss  -0.1967
                  task_loss   0.0419
                       beta   0.0000
             posterior_loss   0.5007
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1648 -0.1936 -0.071  ... -0.1259 -0.0777 -0.0569]]
 discr_rewards:  [[-0.0442  0.0005 -0.0438 ... -0.0824 -0.026  -0.1075]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2486 -0.2326 -0.1543 ... -0.2478 -0.1432 -0.2039]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 199, num timesteps 409600, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0278
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.7083
         discriminator_loss   0.9287
                  gail_loss   0.9149
                  grad_loss   0.0138
                    ib_loss  -0.1968
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.4875
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0545 -0.0682 -0.0887 ... -0.0561 -0.0833 -0.0594]]
 discr_rewards:  [[-0.0952 -0.0869 -0.0915 ... -0.159  -0.1112 -0.0771]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09 -3.7253e-09 ...  3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1891 -0.1946 -0.2197 ... -0.2546 -0.2341 -0.1761]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 200, num timesteps 411648, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0358
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7083
         discriminator_loss   0.9297
                  gail_loss   0.9170
                  grad_loss   0.0127
                    ib_loss  -0.1971
                  task_loss   0.0310
                       beta   0.0000
             posterior_loss   0.5213
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0835 -0.0642 -0.0591 ... -0.065  -0.0639 -0.1265]]
 discr_rewards:  [[-0.1381 -0.1896 -0.0113 ... -0.0458 -0.1121 -0.0604]]
 task_rewards:  [[-3.7253e-09 -7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2611 -0.2934 -0.1099 ... -0.1503 -0.2156 -0.2264]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 201, num timesteps 413696, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0312
                action_loss   0.0013
                    bc_loss   0.0000
               dist_entropy   0.7203
         discriminator_loss   0.9291
                  gail_loss   0.9168
                  grad_loss   0.0123
                    ib_loss  -0.1966
                  task_loss   0.0409
                       beta   0.0000
             posterior_loss   0.5201
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0712 -0.124  -0.0586 ... -0.1037 -0.0529 -0.0886]]
 discr_rewards:  [[-0.1724 -0.0815 -0.0483 ... -0.0387 -0.1328 -0.0411]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.2831 -0.2449 -0.1464 ... -0.1819 -0.2252 -0.1692]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 202, num timesteps 415744, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0285
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7120
         discriminator_loss   0.9237
                  gail_loss   0.9102
                  grad_loss   0.0135
                    ib_loss  -0.1967
                  task_loss   0.0357
                       beta   0.0000
             posterior_loss   0.4907
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1159 -0.0796 -0.0683 ... -0.0564 -0.0794 -0.0746]]
 discr_rewards:  [[-0.0485 -0.0339 -0.0376 ... -0.0714 -0.0331 -0.0608]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2039 -0.1531 -0.1455 ... -0.1673 -0.152  -0.1749]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 203, num timesteps 417792, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0309
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.7102
         discriminator_loss   0.9333
                  gail_loss   0.9211
                  grad_loss   0.0122
                    ib_loss  -0.1970
                  task_loss   0.0352
                       beta   0.0000
             posterior_loss   0.4804
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0652 -0.4943 -0.0664 ... -0.2043 -0.117  -0.0571]]
 discr_rewards:  [[-0.1223 -0.1761 -0.091  ... -0.104  -0.0546 -0.0196]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09 -3.7253e-09 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.227  -0.7099 -0.1969 ... -0.3478 -0.2111 -0.1163]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 204, num timesteps 419840, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0466
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.7089
         discriminator_loss   0.9356
                  gail_loss   0.9216
                  grad_loss   0.0140
                    ib_loss  -0.1967
                  task_loss   0.0597
                       beta   0.0000
             posterior_loss   0.5034
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1379 -0.063  -0.0598 ... -0.053  -0.0526 -0.1185]]
 discr_rewards:  [[-0.068   0.0006 -0.1303 ... -0.0984 -0.092  -0.0413]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2454 -0.1018 -0.2296 ... -0.1909 -0.1842 -0.1993]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 205, num timesteps 421888, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.7016
         discriminator_loss   0.9328
                  gail_loss   0.9210
                  grad_loss   0.0118
                    ib_loss  -0.1971
                  task_loss   0.0579
                       beta   0.0000
             posterior_loss   0.4904
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1942 -0.0823 -0.1036 ... -0.0575 -0.0946 -0.3079]]
 discr_rewards:  [[-0.0401 -0.0977 -0.035  ... -0.0601 -0.0372 -0.0525]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2738 -0.2195 -0.178  ... -0.1571 -0.1713 -0.3999]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 206, num timesteps 423936, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0312
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.7060
         discriminator_loss   0.9318
                  gail_loss   0.9168
                  grad_loss   0.0150
                    ib_loss  -0.1969
                  task_loss   0.0347
                       beta   0.0000
             posterior_loss   0.4988
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0536 -0.0524 -0.0579 ... -0.3121 -0.061  -0.0708]]
 discr_rewards:  [[-0.1522 -0.0409 -0.1148 ... -0.0345 -0.0302 -0.0097]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2453 -0.1329 -0.2123 ... -0.3862 -0.1307 -0.12  ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 207, num timesteps 425984, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0346
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.7237
         discriminator_loss   0.9533
                  gail_loss   0.9370
                  grad_loss   0.0163
                    ib_loss  -0.1970
                  task_loss   0.0379
                       beta   0.0000
             posterior_loss   0.5168
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0815 -0.0926 -0.0524 ... -0.0615 -0.0626 -0.0771]]
 discr_rewards:  [[-0.0319 -0.0302 -0.0523 ... -0.0224 -0.1711 -0.0384]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.153  -0.1624 -0.1442 ... -0.1234 -0.2733 -0.155 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 208, num timesteps 428032, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0352
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7155
         discriminator_loss   0.9226
                  gail_loss   0.9103
                  grad_loss   0.0123
                    ib_loss  -0.1968
                  task_loss   0.0422
                       beta   0.0000
             posterior_loss   0.5289
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0569 -0.1256 -0.0564 ... -0.0529 -0.0555 -0.1627]]
 discr_rewards:  [[-0.0463 -0.0467 -0.0462 ...  0.0058 -0.2025 -0.0153]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1427 -0.2118 -0.1422 ... -0.0866 -0.2975 -0.2175]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 209, num timesteps 430080, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0294
                action_loss   0.0008
                    bc_loss   0.0000
               dist_entropy   0.7046
         discriminator_loss   0.9222
                  gail_loss   0.9102
                  grad_loss   0.0120
                    ib_loss  -0.1968
                  task_loss   0.0398
                       beta   0.0000
             posterior_loss   0.5111
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.0821 -0.0526 ... -0.0642 -0.0567 -0.0524]]
 discr_rewards:  [[-0.0574  0.0055 -0.1013 ... -0.1164 -0.0925 -0.0425]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1495 -0.1162 -0.1934 ... -0.2202 -0.1887 -0.1344]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 210, num timesteps 432128, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0268
                action_loss   0.0027
                    bc_loss   0.0000
               dist_entropy   0.7067
         discriminator_loss   0.9288
                  gail_loss   0.9163
                  grad_loss   0.0125
                    ib_loss  -0.1971
                  task_loss   0.0327
                       beta   0.0000
             posterior_loss   0.4905
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0656 -0.0648 -0.0545 ... -0.0828 -0.0529 -0.1833]]
 discr_rewards:  [[-0.0807 -0.0148 -0.0482 ... -0.003  -0.0965 -0.0109]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1858 -0.1191 -0.1423 ... -0.1254 -0.1889 -0.2337]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 211, num timesteps 434176, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0299
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.7166
         discriminator_loss   0.9241
                  gail_loss   0.9114
                  grad_loss   0.0127
                    ib_loss  -0.1969
                  task_loss   0.0320
                       beta   0.0000
             posterior_loss   0.5485
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0595 -0.2042 -0.1583 ... -0.0922 -0.1101 -0.0871]]
 discr_rewards:  [[-0.0981 -0.1571 -0.149  ... -0.063  -0.0237 -0.0274]]
 task_rewards:  [[-3.7253e-09 -3.7253e-09  7.4506e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1972 -0.4008 -0.3468 ... -0.1947 -0.1734 -0.154 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 212, num timesteps 436224, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0385
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.7229
         discriminator_loss   0.9335
                  gail_loss   0.9181
                  grad_loss   0.0154
                    ib_loss  -0.1969
                  task_loss   0.0365
                       beta   0.0000
             posterior_loss   0.5070
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0835 -0.1026 -0.0524 ... -0.3327 -0.0532 -0.0575]]
 discr_rewards:  [[-0.0039 -0.1759 -0.062  ... -0.0131 -0.048  -0.1419]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.127  -0.3181 -0.1539 ... -0.3853 -0.1407 -0.2389]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 213, num timesteps 438272, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0331
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7115
         discriminator_loss   0.9281
                  gail_loss   0.9137
                  grad_loss   0.0143
                    ib_loss  -0.1969
                  task_loss   0.0415
                       beta   0.0000
             posterior_loss   0.4940
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1005 -0.0533 -0.0647 ... -0.1415 -0.3641 -0.134 ]]
 discr_rewards:  [[-0.1674 -0.0028 -0.0429 ... -0.087  -0.2384 -0.0028]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.3074 -0.0956 -0.1471 ... -0.268  -0.6421 -0.1763]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 214, num timesteps 440320, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0297
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.7115
         discriminator_loss   0.9181
                  gail_loss   0.9047
                  grad_loss   0.0134
                    ib_loss  -0.1965
                  task_loss   0.0358
                       beta   0.0000
             posterior_loss   0.4984
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0627 -0.0863 -0.0712 ... -0.0659 -0.1488 -0.0534]]
 discr_rewards:  [[-0.0246 -0.0343 -0.0403 ... -0.0262 -0.0782 -0.1051]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1269 -0.1601 -0.1511 ... -0.1317 -0.2665 -0.1981]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 215, num timesteps 442368, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0284
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.7108
         discriminator_loss   0.9342
                  gail_loss   0.9227
                  grad_loss   0.0116
                    ib_loss  -0.1969
                  task_loss   0.0335
                       beta   0.0000
             posterior_loss   0.5035
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.071  -0.2019 -0.0524 ... -0.1086 -0.2378 -0.2812]]
 discr_rewards:  [[-0.097  -0.0228  0.008  ... -0.0554 -0.1123 -0.1043]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2075 -0.2642 -0.084  ... -0.2036 -0.3896 -0.425 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 216, num timesteps 444416, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7131
         discriminator_loss   0.9215
                  gail_loss   0.9089
                  grad_loss   0.0126
                    ib_loss  -0.1965
                  task_loss   0.0306
                       beta   0.0000
             posterior_loss   0.4927
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2201 -0.085  -0.1776 ... -0.0977 -0.1594 -0.1771]]
 discr_rewards:  [[-0.0061 -0.0815 -0.0644 ... -0.0906 -0.1313 -0.033 ]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.2658 -0.2061 -0.2815 ... -0.2278 -0.3301 -0.2496]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 217, num timesteps 446464, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0282
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.7008
         discriminator_loss   0.9305
                  gail_loss   0.9187
                  grad_loss   0.0117
                    ib_loss  -0.1971
                  task_loss   0.0362
                       beta   0.0000
             posterior_loss   0.4922
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0749 -0.0531 -0.3875 ... -0.0704 -0.054  -0.0664]]
 discr_rewards:  [[-0.0373 -0.1084 -0.0094 ... -0.0776 -0.0854 -0.059 ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1517 -0.201  -0.4364 ... -0.1875 -0.1789 -0.1649]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 218, num timesteps 448512, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0383
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.7082
         discriminator_loss   0.9272
                  gail_loss   0.9127
                  grad_loss   0.0145
                    ib_loss  -0.1968
                  task_loss   0.0296
                       beta   0.0000
             posterior_loss   0.5056
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1214 -0.0604 -0.078  ... -0.0538 -0.0605 -0.1002]]
 discr_rewards:  [[-0.0415 -0.2234 -0.0064 ... -0.13   -0.0636 -0.0807]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2024 -0.3233 -0.1239 ... -0.2233 -0.1636 -0.2204]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 219, num timesteps 450560, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0310
                action_loss   0.0029
                    bc_loss   0.0000
               dist_entropy   0.7064
         discriminator_loss   0.9324
                  gail_loss   0.9199
                  grad_loss   0.0125
                    ib_loss  -0.1969
                  task_loss   0.0437
                       beta   0.0000
             posterior_loss   0.4976
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0895 -0.0958 -0.1671 ... -0.0625 -0.1444 -0.1617]]
 discr_rewards:  [[-0.0055 -0.1118 -0.0188 ... -0.011  -0.0176 -0.0861]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1345 -0.2471 -0.2254 ... -0.113  -0.2016 -0.2874]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 220, num timesteps 452608, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0439
                action_loss  -0.0016
                    bc_loss   0.0000
               dist_entropy   0.6990
         discriminator_loss   0.9240
                  gail_loss   0.9114
                  grad_loss   0.0126
                    ib_loss  -0.1966
                  task_loss   0.0358
                       beta   0.0000
             posterior_loss   0.5077
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1731 -0.3829 -0.1127 ... -0.0524 -0.0526 -0.089 ]]
 discr_rewards:  [[-0.0627 -0.0966 -0.0215 ... -0.0693 -0.1822 -0.0968]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2754 -0.5191 -0.1737 ... -0.1613 -0.2744 -0.2254]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 221, num timesteps 454656, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.6900
         discriminator_loss   0.9279
                  gail_loss   0.9153
                  grad_loss   0.0127
                    ib_loss  -0.1969
                  task_loss   0.0554
                       beta   0.0000
             posterior_loss   0.4914
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0884 -0.1615 ... -0.1695 -0.056  -0.0526]]
 discr_rewards:  [[-0.124  -0.1903 -0.0623 ... -0.095  -0.1122 -0.1853]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   7.4506e-09]]
 final_rewards:  [[-0.2159 -0.3181 -0.2633 ... -0.304  -0.2078 -0.2773]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 222, num timesteps 456704, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0410
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.7062
         discriminator_loss   0.9231
                  gail_loss   0.9101
                  grad_loss   0.0131
                    ib_loss  -0.1970
                  task_loss   0.0338
                       beta   0.0000
             posterior_loss   0.5043
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0818 -0.0754 -0.0526 ... -0.2188 -0.135  -0.2632]]
 discr_rewards:  [[-0.0649 -0.0485 -0.1369 ... -0.0577 -0.049  -0.0893]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1862 -0.1635 -0.2291 ... -0.316  -0.2235 -0.392 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 223, num timesteps 458752, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0336
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.7073
         discriminator_loss   0.9319
                  gail_loss   0.9197
                  grad_loss   0.0123
                    ib_loss  -0.1969
                  task_loss   0.0471
                       beta   0.0000
             posterior_loss   0.4985
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.093  -0.1346 -0.0894 ... -0.1259 -0.1776 -0.093 ]]
 discr_rewards:  [[-0.0063 -0.0933 -0.0305 ... -0.0154 -0.0572 -0.0073]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1388 -0.2674 -0.1594 ... -0.1808 -0.2743 -0.1398]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 224, num timesteps 460800, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0246
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.7050
         discriminator_loss   0.9249
                  gail_loss   0.9121
                  grad_loss   0.0127
                    ib_loss  -0.1968
                  task_loss   0.0365
                       beta   0.0000
             posterior_loss   0.4808
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0704 -0.0625 -0.0913 ... -0.0627 -0.058  -0.0529]]
 discr_rewards:  [[-0.0789 -0.064  -0.0357 ... -0.0795 -0.0204 -0.0253]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1888 -0.166  -0.1665 ... -0.1817 -0.118  -0.1177]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 225, num timesteps 462848, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0288
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7189
         discriminator_loss   0.9278
                  gail_loss   0.9153
                  grad_loss   0.0125
                    ib_loss  -0.1968
                  task_loss   0.0299
                       beta   0.0000
             posterior_loss   0.5111
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1078 -0.065  -0.0562 ... -0.0869 -0.0584 -0.1304]]
 discr_rewards:  [[-0.0924 -0.0409 -0.0727 ... -0.0543 -0.1014 -0.0147]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2398 -0.1454 -0.1684 ... -0.1808 -0.1993 -0.1846]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 226, num timesteps 464896, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0270
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7070
         discriminator_loss   0.9267
                  gail_loss   0.9148
                  grad_loss   0.0119
                    ib_loss  -0.1972
                  task_loss   0.0318
                       beta   0.0000
             posterior_loss   0.5095
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1784 -0.0653 -0.0698 ... -0.0528 -0.1434 -0.0571]]
 discr_rewards:  [[-0.0789 -0.0333 -0.024  ... -0.071  -0.069  -0.0492]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2969 -0.1381 -0.1333 ... -0.1633 -0.252  -0.1459]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 227, num timesteps 466944, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0340
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7122
         discriminator_loss   0.9249
                  gail_loss   0.9114
                  grad_loss   0.0135
                    ib_loss  -0.1969
                  task_loss   0.0316
                       beta   0.0000
             posterior_loss   0.5043
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0693 -0.295  ... -0.0706 -0.0565 -0.0599]]
 discr_rewards:  [[-0.0196 -0.0811 -0.0872 ... -0.1031 -0.0127 -0.0695]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1116 -0.19   -0.4217 ... -0.2133 -0.1088 -0.1689]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 228, num timesteps 468992, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0299
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7012
         discriminator_loss   0.9304
                  gail_loss   0.9184
                  grad_loss   0.0120
                    ib_loss  -0.1971
                  task_loss   0.0390
                       beta   0.0000
             posterior_loss   0.5205
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0964 -0.0748 -0.0638 ... -0.0708 -0.0605 -0.0534]]
 discr_rewards:  [[-0.0753 -0.0079 -0.0135 ...  0.0025 -0.0608 -0.1445]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2112 -0.1222 -0.1169 ... -0.1079 -0.1609 -0.2374]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 229, num timesteps 471040, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0318
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7160
         discriminator_loss   0.9294
                  gail_loss   0.9158
                  grad_loss   0.0137
                    ib_loss  -0.1971
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.4834
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1111 -0.0533 -0.0543 ... -0.0537 -0.0528 -0.0543]]
 discr_rewards:  [[-0.0068 -0.077  -0.0919 ... -0.0955 -0.0206 -0.021 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1575 -0.1698 -0.1857 ... -0.1888 -0.1129 -0.1148]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 230, num timesteps 473088, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0284
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.7081
         discriminator_loss   0.9095
                  gail_loss   0.8954
                  grad_loss   0.0141
                    ib_loss  -0.1969
                  task_loss   0.0380
                       beta   0.0000
             posterior_loss   0.5182
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0542 -0.0979 -0.0599 ... -0.056  -0.1146 -0.0524]]
 discr_rewards:  [[-0.0647 -0.0491 -0.1839 ... -0.1137 -0.0798 -0.0195]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  3.7253e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1584 -0.1865 -0.2834 ... -0.2092 -0.2339 -0.1114]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 231, num timesteps 475136, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0336
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.7025
         discriminator_loss   0.9220
                  gail_loss   0.9103
                  grad_loss   0.0116
                    ib_loss  -0.1970
                  task_loss   0.0312
                       beta   0.0000
             posterior_loss   0.4802
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0842 -0.3989 -0.314  ... -0.2826 -0.1239 -0.1003]]
 discr_rewards:  [[-0.0394 -0.0164 -0.0371 ... -0.04   -0.0337 -0.1102]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.163  -0.4548 -0.3906 ... -0.3622 -0.1972 -0.25  ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 232, num timesteps 477184, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0436
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7072
         discriminator_loss   0.9254
                  gail_loss   0.9136
                  grad_loss   0.0118
                    ib_loss  -0.1971
                  task_loss   0.0472
                       beta   0.0000
             posterior_loss   0.5062
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0709 -0.0981 -0.0525 ... -0.053  -0.1124 -0.0525]]
 discr_rewards:  [[-0.0703 -0.0844 -0.103  ... -0.1177 -0.0616 -0.0584]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1807 -0.2221 -0.195  ... -0.2103 -0.2135 -0.1504]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 233, num timesteps 479232, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss  -0.0016
                    bc_loss   0.0000
               dist_entropy   0.6998
         discriminator_loss   0.9083
                  gail_loss   0.8977
                  grad_loss   0.0105
                    ib_loss  -0.1967
                  task_loss   0.0627
                       beta   0.0000
             posterior_loss   0.5147
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0922 -0.0597 -0.0524 ... -0.0788 -0.1119 -0.2821]]
 discr_rewards:  [[-0.0826 -0.0752 -0.0385 ... -0.118  -0.092  -0.1071]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.2143 -0.1744 -0.1305 ... -0.2363 -0.2434 -0.4288]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 234, num timesteps 481280, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0335
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.6988
         discriminator_loss   0.9172
                  gail_loss   0.9052
                  grad_loss   0.0119
                    ib_loss  -0.1966
                  task_loss   0.0345
                       beta   0.0000
             posterior_loss   0.4761
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2175 -0.0896 -0.0611 ... -0.0525 -0.0669 -0.0526]]
 discr_rewards:  [[-0.0283 -0.065  -0.0797 ... -0.1894 -0.0308 -0.0144]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2854 -0.1941 -0.1803 ... -0.2813 -0.1372 -0.1065]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 235, num timesteps 483328, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.6804
         discriminator_loss   0.9165
                  gail_loss   0.9051
                  grad_loss   0.0114
                    ib_loss  -0.1968
                  task_loss   0.0386
                       beta   0.0000
             posterior_loss   0.4836
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1052 -0.0605 -0.0539 ... -0.0798 -0.0609 -0.0533]]
 discr_rewards:  [[ 0.0016 -0.0234 -0.0447 ... -0.026  -0.146  -0.1377]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.1431 -0.1234 -0.1381 ... -0.1452 -0.2463 -0.2305]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 236, num timesteps 485376, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0263
                action_loss  -0.0060
                    bc_loss   0.0000
               dist_entropy   0.6916
         discriminator_loss   0.9265
                  gail_loss   0.9158
                  grad_loss   0.0107
                    ib_loss  -0.1969
                  task_loss   0.0356
                       beta   0.0000
             posterior_loss   0.4994
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0673 -0.1183 -0.0524 ... -0.0644 -0.0792 -0.2702]]
 discr_rewards:  [[-0.0358 -0.0734 -0.0705 ... -0.0818 -0.1427 -0.0529]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.1427 -0.2312 -0.1624 ... -0.1857 -0.2614 -0.3626]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 237, num timesteps 487424, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0253
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.6893
         discriminator_loss   0.9173
                  gail_loss   0.9057
                  grad_loss   0.0117
                    ib_loss  -0.1969
                  task_loss   0.0313
                       beta   0.0000
             posterior_loss   0.4885
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0788 -0.072  -0.0528 ... -0.0609 -0.0768 -0.0531]]
 discr_rewards:  [[-0.0793 -0.0857 -0.0581 ... -0.1109 -0.052  -0.1184]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1976 -0.1973 -0.1504 ... -0.2113 -0.1683 -0.2111]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 238, num timesteps 489472, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0373
                action_loss  -0.0068
                    bc_loss   0.0000
               dist_entropy   0.7008
         discriminator_loss   0.9384
                  gail_loss   0.9268
                  grad_loss   0.0116
                    ib_loss  -0.1967
                  task_loss   0.0317
                       beta   0.0000
             posterior_loss   0.5219
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0563 -0.0647 -0.0529 ... -0.2718 -0.0924 -0.0706]]
 discr_rewards:  [[-0.1185 -0.0253 -0.0789 ... -0.0074 -0.1519 -0.047 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.2143 -0.1295 -0.1713 ... -0.3188 -0.2839 -0.1571]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 239, num timesteps 491520, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0307
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6968
         discriminator_loss   0.9321
                  gail_loss   0.9200
                  grad_loss   0.0121
                    ib_loss  -0.1965
                  task_loss   0.0494
                       beta   0.0000
             posterior_loss   0.4915
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2047 -0.0628 -0.0551 ... -0.1298 -0.0685 -0.0528]]
 discr_rewards:  [[-0.0426 -0.124  -0.087  ... -0.0751 -0.0073 -0.0096]]
 task_rewards:  [[0.0000e+00 3.7253e-09 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2868 -0.2264 -0.1816 ... -0.2445 -0.1153 -0.1019]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 240, num timesteps 493568, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0460
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6918
         discriminator_loss   0.9323
                  gail_loss   0.9198
                  grad_loss   0.0125
                    ib_loss  -0.1967
                  task_loss   0.0498
                       beta   0.0000
             posterior_loss   0.5094
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0531 -0.0592 -0.073  ... -0.1493 -0.0809 -0.0611]]
 discr_rewards:  [[-0.0873 -0.0525 -0.0189 ... -0.0431 -0.118  -0.0994]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1799 -0.1512 -0.1314 ... -0.232  -0.2385 -0.1999]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 241, num timesteps 495616, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0407
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.7051
         discriminator_loss   0.9341
                  gail_loss   0.9204
                  grad_loss   0.0137
                    ib_loss  -0.1969
                  task_loss   0.0616
                       beta   0.0000
             posterior_loss   0.5045
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1816 -0.1099 -0.1442 ... -0.1023 -0.0524 -0.1068]]
 discr_rewards:  [[-0.086  -0.0615 -0.0636 ... -0.0453 -0.0742 -0.1017]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.307  -0.211  -0.2473 ... -0.1872 -0.1662 -0.248 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 242, num timesteps 497664, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0309
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.7031
         discriminator_loss   0.9331
                  gail_loss   0.9208
                  grad_loss   0.0123
                    ib_loss  -0.1966
                  task_loss   0.0558
                       beta   0.0000
             posterior_loss   0.4833
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.064  -0.0524 -0.0556 ... -0.1882 -0.0587 -0.0918]]
 discr_rewards:  [[-0.0443 -0.014  -0.0124 ... -0.0124 -0.0231 -0.1333]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.1478 -0.1059 -0.1076 ... -0.2401 -0.1214 -0.2645]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 243, num timesteps 499712, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0397
                action_loss   0.0069
                    bc_loss   0.0000
               dist_entropy   0.7072
         discriminator_loss   0.9439
                  gail_loss   0.9309
                  grad_loss   0.0130
                    ib_loss  -0.1968
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.5085
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0607 -0.0773 -0.0556 ... -0.0534 -0.198  -0.0705]]
 discr_rewards:  [[-0.0135 -0.1064 -0.0421 ... -0.0126 -0.1108 -0.0353]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1137 -0.2232 -0.1372 ... -0.1056 -0.3483 -0.1454]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 244, num timesteps 501760, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0221
                action_loss   0.0011
                    bc_loss   0.0000
               dist_entropy   0.6949
         discriminator_loss   0.9245
                  gail_loss   0.9137
                  grad_loss   0.0108
                    ib_loss  -0.1970
                  task_loss   0.0487
                       beta   0.0000
             posterior_loss   0.5032
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0676 -0.0538 -0.0564 ... -0.0545 -0.1068 -0.0748]]
 discr_rewards:  [[-0.0328 -0.0371 -0.0407 ... -0.1602 -0.075  -0.116 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -7.4506e-09 -3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.1399 -0.1305 -0.1366 ... -0.2542 -0.2213 -0.2302]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 245, num timesteps 503808, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0279
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.6950
         discriminator_loss   0.9289
                  gail_loss   0.9155
                  grad_loss   0.0134
                    ib_loss  -0.1973
                  task_loss   0.0225
                       beta   0.0000
             posterior_loss   0.4802
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.1527 -0.0614 ... -0.0545 -0.0886 -0.0879]]
 discr_rewards:  [[-0.0712 -0.0691 -0.0626 ... -0.0233 -0.0308 -0.0729]]
 task_rewards:  [[-3.7253e-09 -3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1631 -0.2613 -0.1635 ... -0.1173 -0.159  -0.2003]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 246, num timesteps 505856, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss  -0.0058
                    bc_loss   0.0000
               dist_entropy   0.6952
         discriminator_loss   0.9168
                  gail_loss   0.9055
                  grad_loss   0.0113
                    ib_loss  -0.1969
                  task_loss   0.0304
                       beta   0.0000
             posterior_loss   0.4948
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0777 -0.0596 -0.0637 ... -0.3877 -0.0524 -0.0567]]
 discr_rewards:  [[-0.0673 -0.1224 -0.0313 ... -0.093  -0.0426 -0.0436]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1845 -0.2215 -0.1346 ... -0.5202 -0.1346 -0.1399]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 247, num timesteps 507904, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0336
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6989
         discriminator_loss   0.9265
                  gail_loss   0.9151
                  grad_loss   0.0114
                    ib_loss  -0.1970
                  task_loss   0.0352
                       beta   0.0000
             posterior_loss   0.4945
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2058 -0.0952 -0.104  ... -0.1437 -0.0525 -0.0533]]
 discr_rewards:  [[-0.0398 -0.009  -0.0145 ... -0.1199 -0.0659 -0.0893]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 3.7253e-09]]
 final_rewards:  [[-0.2851 -0.1437 -0.158  ... -0.3031 -0.1579 -0.182 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 248, num timesteps 509952, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0468
                action_loss   0.0009
                    bc_loss   0.0000
               dist_entropy   0.6826
         discriminator_loss   0.9146
                  gail_loss   0.9034
                  grad_loss   0.0112
                    ib_loss  -0.1968
                  task_loss   0.0441
                       beta   0.0000
             posterior_loss   0.5222
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2171 -0.2536 -0.0812 ... -0.1744 -0.1569 -0.0939]]
 discr_rewards:  [[-0.1308  0.0109 -0.1104 ... -0.1707 -0.0968 -0.0259]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3874 -0.2823 -0.2311 ... -0.3846 -0.2932 -0.1593]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 249, num timesteps 512000, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0429
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.6903
         discriminator_loss   0.9241
                  gail_loss   0.9124
                  grad_loss   0.0117
                    ib_loss  -0.1969
                  task_loss   0.0536
                       beta   0.0000
             posterior_loss   0.5218
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0869 -0.0857 -0.1412 ... -0.1007 -0.0524 -0.1206]]
 discr_rewards:  [[-0.0283 -0.087  -0.0169 ... -0.0324 -0.0102 -0.0095]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1547 -0.2122 -0.1977 ... -0.1726 -0.1021 -0.1696]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 250, num timesteps 514048, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0328
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6950
         discriminator_loss   0.9277
                  gail_loss   0.9167
                  grad_loss   0.0109
                    ib_loss  -0.1972
                  task_loss   0.0498
                       beta   0.0000
             posterior_loss   0.4991
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0598 -0.0607 -0.0533 ... -0.1224 -0.0619 -0.0866]]
 discr_rewards:  [[-0.0063 -0.131  -0.0433 ... -0.0443 -0.056  -0.0799]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1056 -0.2312 -0.1361 ... -0.2063 -0.1574 -0.2061]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 251, num timesteps 516096, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0251
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6861
         discriminator_loss   0.9197
                  gail_loss   0.9081
                  grad_loss   0.0116
                    ib_loss  -0.1970
                  task_loss   0.0337
                       beta   0.0000
             posterior_loss   0.4966
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1165 -0.0585 -0.0979 ... -0.0663 -0.0555 -0.4494]]
 discr_rewards:  [[-0.0186 -0.0395 -0.0397 ... -0.0178 -0.0313 -0.1   ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1746 -0.1375 -0.1771 ... -0.1236 -0.1264 -0.5889]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 252, num timesteps 518144, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0366
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6826
         discriminator_loss   0.9346
                  gail_loss   0.9236
                  grad_loss   0.0111
                    ib_loss  -0.1968
                  task_loss   0.0311
                       beta   0.0000
             posterior_loss   0.5217
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.053  -0.053  ... -0.063  -0.0549 -0.1131]]
 discr_rewards:  [[-0.0951 -0.0367 -0.0146 ... -0.0221 -0.0665 -0.0219]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1873 -0.1292 -0.1071 ... -0.1246 -0.1609 -0.1745]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 253, num timesteps 520192, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6793
         discriminator_loss   0.9278
                  gail_loss   0.9163
                  grad_loss   0.0115
                    ib_loss  -0.1972
                  task_loss   0.0395
                       beta   0.0000
             posterior_loss   0.5051
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1764 -0.267  -0.0797 ... -0.1144 -0.0599 -0.1846]]
 discr_rewards:  [[-0.0944 -0.0385 -0.0966 ... -0.0714 -0.0284 -0.023 ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3103 -0.345  -0.2158 ... -0.2253 -0.1278 -0.2472]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 254, num timesteps 522240, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0353
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6699
         discriminator_loss   0.9193
                  gail_loss   0.9083
                  grad_loss   0.0109
                    ib_loss  -0.1968
                  task_loss   0.0332
                       beta   0.0000
             posterior_loss   0.5082
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0888 -0.0932 -0.1286 ... -0.2693 -0.0725 -0.0621]]
 discr_rewards:  [[-0.1138 -0.058  -0.0429 ... -0.0278 -0.0292 -0.136 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2421 -0.1907 -0.211  ... -0.3366 -0.1412 -0.2376]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 255, num timesteps 524288, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0341
                action_loss  -0.0041
                    bc_loss   0.0000
               dist_entropy   0.6809
         discriminator_loss   0.9182
                  gail_loss   0.9076
                  grad_loss   0.0105
                    ib_loss  -0.1967
                  task_loss   0.0377
                       beta   0.0000
             posterior_loss   0.4969
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0724 -0.0954 -0.0874 ... -0.0687 -0.0574 -0.1172]]
 discr_rewards:  [[-0.0936 -0.0363 -0.0738 ... -0.168  -0.016  -0.0055]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2056 -0.1712 -0.2007 ... -0.2763 -0.1129 -0.1623]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 256, num timesteps 526336, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss  -0.0038
                    bc_loss   0.0000
               dist_entropy   0.6978
         discriminator_loss   0.9193
                  gail_loss   0.9085
                  grad_loss   0.0108
                    ib_loss  -0.1970
                  task_loss   0.0410
                       beta   0.0000
             posterior_loss   0.5220
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.0652 -0.0834 ... -0.0921 -0.0662 -0.0533]]
 discr_rewards:  [[-0.0064  0.0192 -0.0297 ... -0.2114 -0.0316 -0.0396]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -7.4506e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.0988 -0.0856 -0.1527 ... -0.343  -0.1374 -0.1324]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 257, num timesteps 528384, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0264
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.7035
         discriminator_loss   0.9311
                  gail_loss   0.9199
                  grad_loss   0.0113
                    ib_loss  -0.1973
                  task_loss   0.0360
                       beta   0.0000
             posterior_loss   0.4638
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0576 -0.0535 -0.0564 ... -0.0524 -0.0561 -0.0747]]
 discr_rewards:  [[-1.0082e-01  6.0052e-06 -1.4162e-03 ... -7.9778e-02 -3.2876e-02
  -1.1513e-01]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.198  -0.093  -0.0973 ... -0.1717 -0.1285 -0.2293]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 258, num timesteps 530432, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6889
         discriminator_loss   0.9230
                  gail_loss   0.9099
                  grad_loss   0.0131
                    ib_loss  -0.1970
                  task_loss   0.0303
                       beta   0.0000
             posterior_loss   0.5194
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0821 -0.0764 -0.2492 ... -0.0528 -0.0561 -0.2612]]
 discr_rewards:  [[-0.0177 -0.0791 -0.0728 ... -0.1449 -0.0124 -0.0194]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1393 -0.1951 -0.3615 ... -0.2372 -0.108  -0.3202]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 259, num timesteps 532480, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6969
         discriminator_loss   0.9517
                  gail_loss   0.9370
                  grad_loss   0.0146
                    ib_loss  -0.1967
                  task_loss   0.0333
                       beta   0.0000
             posterior_loss   0.5008
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2327 -0.0564 -0.0702 ... -0.0531 -0.0524 -0.5973]]
 discr_rewards:  [[-0.0219 -0.0428 -0.0145 ... -0.0227 -0.0666 -0.0469]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.294  -0.1387 -0.1242 ... -0.1153 -0.1585 -0.6837]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 260, num timesteps 534528, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss   0.0042
                    bc_loss   0.0000
               dist_entropy   0.7020
         discriminator_loss   0.9355
                  gail_loss   0.9229
                  grad_loss   0.0126
                    ib_loss  -0.1968
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.5023
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0958 -0.0789 -0.0678 ... -0.063  -0.1022 -0.0589]]
 discr_rewards:  [[-0.0298 -0.0167 -0.019  ... -0.0416 -0.0123 -0.0564]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1651 -0.1351 -0.1264 ... -0.1441 -0.154  -0.1548]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 261, num timesteps 536576, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0358
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7071
         discriminator_loss   0.9381
                  gail_loss   0.9274
                  grad_loss   0.0107
                    ib_loss  -0.1970
                  task_loss   0.0422
                       beta   0.0000
             posterior_loss   0.4863
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1704 -0.0626 -0.0755 ... -0.061  -0.0542 -0.0724]]
 discr_rewards:  [[-0.0762 -0.0627 -0.1141 ... -0.1082 -0.0514 -0.0362]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2862 -0.1648 -0.2292 ... -0.2087 -0.1451 -0.1482]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 262, num timesteps 538624, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0276
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.6907
         discriminator_loss   0.9298
                  gail_loss   0.9177
                  grad_loss   0.0120
                    ib_loss  -0.1971
                  task_loss   0.0464
                       beta   0.0000
             posterior_loss   0.5060
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0686 -0.0535 -0.0611 ... -0.0811 -0.0807 -0.098 ]]
 discr_rewards:  [[-0.0015 -0.0824 -0.0512 ... -0.0219  0.001  -0.0816]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1096 -0.1755 -0.1518 ... -0.1426 -0.1193 -0.2191]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 263, num timesteps 540672, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0233
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6912
         discriminator_loss   0.9196
                  gail_loss   0.9094
                  grad_loss   0.0102
                    ib_loss  -0.1969
                  task_loss   0.0298
                       beta   0.0000
             posterior_loss   0.4953
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1887 -0.0613 -0.0527 ... -0.056  -0.0559 -0.055 ]]
 discr_rewards:  [[-0.0045 -0.0816 -0.1505 ... -0.016  -0.1086 -0.0743]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -7.4506e-09 ...  0.0000e+00  3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2328 -0.1824 -0.2428 ... -0.1116 -0.204  -0.1689]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 264, num timesteps 542720, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0305
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6977
         discriminator_loss   0.9345
                  gail_loss   0.9228
                  grad_loss   0.0116
                    ib_loss  -0.1969
                  task_loss   0.0323
                       beta   0.0000
             posterior_loss   0.4794
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0697 -0.2243 -0.0524 ... -0.0533 -0.053  -0.0993]]
 discr_rewards:  [[-0.0305 -0.0172 -0.0495 ... -0.1231 -0.0152 -0.0977]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1397 -0.281  -0.1414 ... -0.2159 -0.1078 -0.2366]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 265, num timesteps 544768, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0271
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6902
         discriminator_loss   0.9299
                  gail_loss   0.9192
                  grad_loss   0.0107
                    ib_loss  -0.1967
                  task_loss   0.0467
                       beta   0.0000
             posterior_loss   0.4656
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0699 -0.1618 -0.2531 ... -0.2313 -0.0533 -0.1849]]
 discr_rewards:  [[-0.1254 -0.0367  0.0002 ... -0.1225 -0.0379 -0.088 ]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.2348 -0.238  -0.2924 ... -0.3933 -0.1307 -0.3124]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 266, num timesteps 546816, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0373
                action_loss  -0.0058
                    bc_loss   0.0000
               dist_entropy   0.6939
         discriminator_loss   0.9208
                  gail_loss   0.9100
                  grad_loss   0.0108
                    ib_loss  -0.1971
                  task_loss   0.0317
                       beta   0.0000
             posterior_loss   0.5260
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1026 -0.0939 -0.0772 ... -0.0586 -0.0864 -0.156 ]]
 discr_rewards:  [[-0.0624 -0.115  -0.0351 ... -0.0674 -0.1001 -0.1432]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2045 -0.2483 -0.1518 ... -0.1655 -0.226  -0.3388]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 267, num timesteps 548864, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0364
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6977
         discriminator_loss   0.9268
                  gail_loss   0.9162
                  grad_loss   0.0106
                    ib_loss  -0.1967
                  task_loss   0.0464
                       beta   0.0000
             posterior_loss   0.5204
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.066  -0.0547 ... -0.0553 -0.1049 -0.0561]]
 discr_rewards:  [[-0.1303 -0.0199 -0.0476 ... -0.0595 -0.0636 -0.0028]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2225 -0.1254 -0.1418 ... -0.1543 -0.208  -0.0984]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 268, num timesteps 550912, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0326
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.6951
         discriminator_loss   0.9240
                  gail_loss   0.9135
                  grad_loss   0.0105
                    ib_loss  -0.1968
                  task_loss   0.0418
                       beta   0.0000
             posterior_loss   0.5203
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0834 -0.0594 -0.0525 ... -0.0526 -0.0609 -0.1684]]
 discr_rewards:  [[-0.1014 -0.0179 -0.0223 ... -0.058  -0.0254 -0.025 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2243 -0.1168 -0.1143 ... -0.1501 -0.1259 -0.233 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 269, num timesteps 552960, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0371
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.6933
         discriminator_loss   0.9326
                  gail_loss   0.9215
                  grad_loss   0.0111
                    ib_loss  -0.1970
                  task_loss   0.0359
                       beta   0.0000
             posterior_loss   0.5194
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0527 -0.1105 -0.0525 ... -0.0759 -0.0763 -0.1089]]
 discr_rewards:  [[-0.0628 -0.0874 -0.0444 ... -0.2062 -0.0732 -0.0234]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.155  -0.2374 -0.1365 ... -0.3217 -0.189  -0.1718]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 270, num timesteps 555008, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0379
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.6972
         discriminator_loss   0.9259
                  gail_loss   0.9154
                  grad_loss   0.0105
                    ib_loss  -0.1968
                  task_loss   0.0457
                       beta   0.0000
             posterior_loss   0.5269
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0648 -0.0589 -0.0528 ... -0.2105 -0.0976 -0.0642]]
 discr_rewards:  [[-0.134  -0.0215 -0.064  ... -0.0807 -0.0293 -0.0557]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2382 -0.1199 -0.1564 ... -0.3307 -0.1664 -0.1595]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 271, num timesteps 557056, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0404
                action_loss  -0.0016
                    bc_loss   0.0000
               dist_entropy   0.6991
         discriminator_loss   0.9280
                  gail_loss   0.9163
                  grad_loss   0.0117
                    ib_loss  -0.1966
                  task_loss   0.0500
                       beta   0.0000
             posterior_loss   0.5321
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1008 -0.0537 -0.0715 ... -0.0972 -0.0525 -0.058 ]]
 discr_rewards:  [[-0.0556 -0.0239 -0.0091 ... -0.0343 -0.0209 -0.0518]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1959 -0.1171 -0.1201 ... -0.171  -0.1129 -0.1493]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 272, num timesteps 559104, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0294
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6894
         discriminator_loss   0.9325
                  gail_loss   0.9216
                  grad_loss   0.0109
                    ib_loss  -0.1969
                  task_loss   0.0459
                       beta   0.0000
             posterior_loss   0.5019
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1404 -0.0524 -0.0559 ... -0.0526 -0.0816 -0.0597]]
 discr_rewards:  [[-0.0521 -0.0014 -0.0014 ... -0.0392 -0.0119 -0.0493]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.232  -0.0934 -0.0968 ... -0.1313 -0.1331 -0.1486]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 273, num timesteps 561152, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0340
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6744
         discriminator_loss   0.9384
                  gail_loss   0.9261
                  grad_loss   0.0123
                    ib_loss  -0.1969
                  task_loss   0.0332
                       beta   0.0000
             posterior_loss   0.5086
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0562 -0.1877 -0.0623 ... -0.0752 -0.0535 -0.0902]]
 discr_rewards:  [[-0.0726 -0.0209 -0.1114 ... -0.1397 -0.1266 -0.0196]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -7.4506e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1683 -0.2481 -0.2132 ... -0.2545 -0.2196 -0.1493]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 274, num timesteps 563200, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0319
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6826
         discriminator_loss   0.9285
                  gail_loss   0.9145
                  grad_loss   0.0140
                    ib_loss  -0.1969
                  task_loss   0.0396
                       beta   0.0000
             posterior_loss   0.5000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2323 -0.2209 -0.0564 ... -0.2128 -0.2849 -0.1129]]
 discr_rewards:  [[-0.0236 -0.0236 -0.1672 ... -0.1171 -0.1384 -0.0642]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  7.4506e-09 ... -3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2954 -0.284  -0.2631 ... -0.3695 -0.4628 -0.2165]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 275, num timesteps 565248, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0404
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6941
         discriminator_loss   0.9261
                  gail_loss   0.9154
                  grad_loss   0.0107
                    ib_loss  -0.1969
                  task_loss   0.0400
                       beta   0.0000
             posterior_loss   0.5028
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0805 -0.0535 -0.088  ... -0.1317 -0.0525 -0.1007]]
 discr_rewards:  [[-0.0336 -0.0373 -0.1354 ... -0.056  -0.0528 -0.0141]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1537 -0.1303 -0.2629 ... -0.2272 -0.1448 -0.1543]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 276, num timesteps 567296, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0235
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6957
         discriminator_loss   0.9247
                  gail_loss   0.9147
                  grad_loss   0.0100
                    ib_loss  -0.1969
                  task_loss   0.0437
                       beta   0.0000
             posterior_loss   0.4976
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0667 -0.0695 -0.0703 ... -0.2026 -0.0524 -0.1514]]
 discr_rewards:  [[-0.0239 -0.0495 -0.0342 ... -0.0209 -0.0383 -0.0534]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1301 -0.1584 -0.144  ... -0.263  -0.1302 -0.2443]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 277, num timesteps 569344, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0329
                action_loss  -0.0036
                    bc_loss   0.0000
               dist_entropy   0.6971
         discriminator_loss   0.9194
                  gail_loss   0.9082
                  grad_loss   0.0112
                    ib_loss  -0.1968
                  task_loss   0.0287
                       beta   0.0000
             posterior_loss   0.4737
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0587 -0.1058 -0.057  ... -0.1019 -0.3511 -0.0582]]
 discr_rewards:  [[-0.0212 -0.0314 -0.0358 ... -0.0292 -0.0176 -0.0735]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1195 -0.1767 -0.1322 ... -0.1706 -0.4081 -0.1713]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 278, num timesteps 571392, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0249
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7000
         discriminator_loss   0.9310
                  gail_loss   0.9189
                  grad_loss   0.0121
                    ib_loss  -0.1965
                  task_loss   0.0369
                       beta   0.0000
             posterior_loss   0.5172
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.4555 -0.0603 -0.0848 ... -0.0577 -0.1892 -0.0824]]
 discr_rewards:  [[-0.07   -0.0316 -0.1245 ... -0.0651 -0.0025 -0.0023]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.565  -0.1314 -0.2489 ... -0.1624 -0.2312 -0.1242]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 279, num timesteps 573440, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0281
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.7016
         discriminator_loss   0.9199
                  gail_loss   0.9087
                  grad_loss   0.0112
                    ib_loss  -0.1970
                  task_loss   0.0311
                       beta   0.0000
             posterior_loss   0.5150
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.4597 -0.07   -0.3408 ... -0.1434 -0.0602 -0.0527]]
 discr_rewards:  [[-0.0616 -0.0954 -0.0659 ... -0.028  -0.177  -0.0243]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 7.4506e-09 0.0000e+00]]
 final_rewards:  [[-0.5608 -0.2049 -0.4462 ... -0.211  -0.2767 -0.1165]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 280, num timesteps 575488, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0254
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.7021
         discriminator_loss   0.9062
                  gail_loss   0.8944
                  grad_loss   0.0118
                    ib_loss  -0.1970
                  task_loss   0.0326
                       beta   0.0000
             posterior_loss   0.4935
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0753 -0.188  -0.1186 ... -0.1391 -0.053  -0.0795]]
 discr_rewards:  [[-0.1105 -0.0581 -0.0567 ... -0.0289 -0.0256 -0.0365]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2253 -0.2856 -0.2148 ... -0.2075 -0.1181 -0.1555]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 281, num timesteps 577536, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0258
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7051
         discriminator_loss   0.9211
                  gail_loss   0.9105
                  grad_loss   0.0106
                    ib_loss  -0.1969
                  task_loss   0.0290
                       beta   0.0000
             posterior_loss   0.5001
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.071  -0.0835 -0.2003 ... -0.1083 -0.0607 -0.0599]]
 discr_rewards:  [[-0.01   -0.0161 -0.0321 ... -0.0269 -0.0252 -0.0426]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1205 -0.1391 -0.272  ... -0.1747 -0.1254 -0.1421]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 282, num timesteps 579584, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0336
                action_loss   0.0046
                    bc_loss   0.0000
               dist_entropy   0.7051
         discriminator_loss   0.9263
                  gail_loss   0.9139
                  grad_loss   0.0123
                    ib_loss  -0.1969
                  task_loss   0.0321
                       beta   0.0000
             posterior_loss   0.5005
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1144 -0.166  -0.2853 ... -0.0559 -0.0524 -0.0524]]
 discr_rewards:  [[-0.0193 -0.066  -0.0227 ... -0.0345 -0.0667 -0.0169]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1733 -0.2715 -0.3476 ... -0.1299 -0.1586 -0.1088]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 283, num timesteps 581632, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0334
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7042
         discriminator_loss   0.9236
                  gail_loss   0.9113
                  grad_loss   0.0124
                    ib_loss  -0.1968
                  task_loss   0.0375
                       beta   0.0000
             posterior_loss   0.5004
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0548 -0.0798 ... -0.0595 -0.2359 -0.0589]]
 discr_rewards:  [[-0.1115 -0.0003 -0.038  ... -0.0771 -0.0282 -0.1373]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.2035 -0.0947 -0.1572 ... -0.1761 -0.3036 -0.2357]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 284, num timesteps 583680, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss   0.0020
                    bc_loss   0.0000
               dist_entropy   0.7166
         discriminator_loss   0.9364
                  gail_loss   0.9245
                  grad_loss   0.0119
                    ib_loss  -0.1969
                  task_loss   0.0385
                       beta   0.0000
             posterior_loss   0.5179
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1374 -0.0567 -0.0911 ... -0.1353 -0.0832 -0.0548]]
 discr_rewards:  [[-0.027  -0.0642 -0.0134 ... -0.0737 -0.0471 -0.084 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2039 -0.1604 -0.1441 ... -0.2485 -0.1698 -0.1783]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 285, num timesteps 585728, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.6973
         discriminator_loss   0.9064
                  gail_loss   0.8948
                  grad_loss   0.0116
                    ib_loss  -0.1965
                  task_loss   0.0341
                       beta   0.0000
             posterior_loss   0.5138
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2126 -0.0572 -0.0533 ... -0.0524 -0.0537 -0.0572]]
 discr_rewards:  [[-0.0777 -0.1553 -0.0966 ... -0.0564 -0.0103 -0.0217]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3297 -0.252  -0.1895 ... -0.1483 -0.1035 -0.1184]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 286, num timesteps 587776, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0333
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.7019
         discriminator_loss   0.9242
                  gail_loss   0.9120
                  grad_loss   0.0122
                    ib_loss  -0.1969
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.5038
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1672 -0.0769 -0.1423 ... -0.0525 -0.0525 -0.1153]]
 discr_rewards:  [[-0.1501 -0.0918 -0.0231 ... -0.1089 -0.0825 -0.0208]]
 task_rewards:  [[-3.7253e-09 -3.7253e-09  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.3568 -0.2082 -0.205  ... -0.2009 -0.1746 -0.1756]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 287, num timesteps 589824, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.7026
         discriminator_loss   0.9199
                  gail_loss   0.9093
                  grad_loss   0.0105
                    ib_loss  -0.1966
                  task_loss   0.0364
                       beta   0.0000
             posterior_loss   0.5027
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0762 -0.0565 -0.0738 ... -0.0617 -0.1483 -0.1293]]
 discr_rewards:  [[-0.0988 -0.0159 -0.0441 ... -0.0278 -0.0882 -0.0653]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2145 -0.1119 -0.1574 ... -0.1289 -0.276  -0.2341]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 288, num timesteps 591872, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0380
                action_loss   0.0025
                    bc_loss   0.0000
               dist_entropy   0.7074
         discriminator_loss   0.9309
                  gail_loss   0.9150
                  grad_loss   0.0159
                    ib_loss  -0.1966
                  task_loss   0.0407
                       beta   0.0000
             posterior_loss   0.4909
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0696 -0.1804 -0.0793 ... -0.0756 -0.5295 -0.1544]]
 discr_rewards:  [[-0.0557 -0.041  -0.1627 ... -0.1397 -0.0004 -0.0492]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1647 -0.261  -0.2815 ... -0.2549 -0.5694 -0.2431]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 289, num timesteps 593920, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0310
                action_loss   0.0023
                    bc_loss   0.0000
               dist_entropy   0.7107
         discriminator_loss   0.9348
                  gail_loss   0.9234
                  grad_loss   0.0115
                    ib_loss  -0.1968
                  task_loss   0.0582
                       beta   0.0000
             posterior_loss   0.5207
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1081 -0.0752 -0.0536 ... -0.1134 -0.0613 -0.111 ]]
 discr_rewards:  [[-0.0515 -0.0496 -0.0944 ... -0.0457 -0.1302 -0.0106]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.1991 -0.1642 -0.1875 ... -0.1987 -0.231  -0.1611]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 290, num timesteps 595968, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.7094
         discriminator_loss   0.9268
                  gail_loss   0.9158
                  grad_loss   0.0110
                    ib_loss  -0.1967
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.5184
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0542 -0.1269 -0.0588 ... -0.0616 -0.0669 -0.0639]]
 discr_rewards:  [[-0.0587 -0.0816 -0.0256 ... -0.0605 -0.0469 -0.0433]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1524 -0.2481 -0.1238 ... -0.1617 -0.1533 -0.1467]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 291, num timesteps 598016, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0283
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.7061
         discriminator_loss   0.9218
                  gail_loss   0.9098
                  grad_loss   0.0119
                    ib_loss  -0.1970
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.5027
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0705 -0.0613 -0.0527 ... -0.1192 -0.203  -0.057 ]]
 discr_rewards:  [[-0.0625 -0.047  -0.0801 ... -0.061  -0.0643 -0.2534]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1726 -0.1478 -0.1723 ... -0.2197 -0.3068 -0.3499]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 292, num timesteps 600064, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0294
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7051
         discriminator_loss   0.9280
                  gail_loss   0.9173
                  grad_loss   0.0107
                    ib_loss  -0.1968
                  task_loss   0.0302
                       beta   0.0000
             posterior_loss   0.4985
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0833 -0.1051 -0.0558 ... -0.0524 -0.069  -0.0549]]
 discr_rewards:  [[-0.0533 -0.1041 -0.0351 ... -0.073  -0.0261 -0.1311]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1761 -0.2487 -0.1304 ... -0.1649 -0.1347 -0.2256]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 293, num timesteps 602112, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0366
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7087
         discriminator_loss   0.9252
                  gail_loss   0.9119
                  grad_loss   0.0133
                    ib_loss  -0.1966
                  task_loss   0.0470
                       beta   0.0000
             posterior_loss   0.4955
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0585 -0.0531 -0.0542 ... -0.0555 -0.0541 -0.0572]]
 discr_rewards:  [[-0.0667 -0.032  -0.014  ... -0.1228 -0.0689 -0.1322]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 3.7253e-09]]
 final_rewards:  [[-0.1647 -0.1246 -0.1078 ... -0.2179 -0.1626 -0.2289]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 294, num timesteps 604160, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0275
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7014
         discriminator_loss   0.9142
                  gail_loss   0.9039
                  grad_loss   0.0103
                    ib_loss  -0.1970
                  task_loss   0.0524
                       beta   0.0000
             posterior_loss   0.4967
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1762 -0.0532 -0.1733 ... -0.06   -0.0612 -0.0646]]
 discr_rewards:  [[-0.1338 -0.0226 -0.062  ... -0.0262 -0.03   -0.0873]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.3496 -0.1153 -0.2748 ... -0.1256 -0.1307 -0.1914]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 295, num timesteps 606208, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0269
                action_loss   0.0015
                    bc_loss   0.0000
               dist_entropy   0.7081
         discriminator_loss   0.9203
                  gail_loss   0.9065
                  grad_loss   0.0138
                    ib_loss  -0.1969
                  task_loss   0.0341
                       beta   0.0000
             posterior_loss   0.4907
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0737 -0.0643 -0.269  ... -0.0607 -0.1091 -0.1854]]
 discr_rewards:  [[-0.0467 -0.0764 -0.0281 ... -0.0355 -0.0674 -0.0446]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1599 -0.1802 -0.3366 ... -0.1358 -0.2159 -0.2696]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 296, num timesteps 608256, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0369
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.6970
         discriminator_loss   0.9238
                  gail_loss   0.9129
                  grad_loss   0.0109
                    ib_loss  -0.1972
                  task_loss   0.0305
                       beta   0.0000
             posterior_loss   0.5135
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.062  -0.1306 -0.1318 ... -0.1401 -0.0612 -0.1161]]
 discr_rewards:  [[-0.0228 -0.1032 -0.022  ... -0.1043 -0.0128 -0.1677]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1244 -0.2734 -0.1933 ... -0.2838 -0.1135 -0.3234]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 297, num timesteps 610304, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.6739
         discriminator_loss   0.9143
                  gail_loss   0.9028
                  grad_loss   0.0116
                    ib_loss  -0.1970
                  task_loss   0.0389
                       beta   0.0000
             posterior_loss   0.5186
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0539 -0.0542 -0.084  ... -0.0758 -0.0949 -0.0666]]
 discr_rewards:  [[-0.1146 -0.0341 -0.1576 ... -0.0743 -0.0152 -0.1086]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -7.4506e-09 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.208  -0.1278 -0.2811 ... -0.1897 -0.1497 -0.2147]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 298, num timesteps 612352, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6782
         discriminator_loss   0.9136
                  gail_loss   0.9017
                  grad_loss   0.0119
                    ib_loss  -0.1966
                  task_loss   0.0355
                       beta   0.0000
             posterior_loss   0.4972
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.122  -0.1304 -0.0524 ... -0.1746 -0.0557 -0.0992]]
 discr_rewards:  [[-0.0808 -0.0919 -0.0789 ...  0.0018 -0.0289 -0.0549]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2422 -0.2619 -0.1709 ... -0.2123 -0.1241 -0.1936]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 299, num timesteps 614400, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0310
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6953
         discriminator_loss   0.9170
                  gail_loss   0.9065
                  grad_loss   0.0105
                    ib_loss  -0.1968
                  task_loss   0.0382
                       beta   0.0000
             posterior_loss   0.5170
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1377 -0.1939 -0.0692 ... -0.1219 -0.1265 -0.1922]]
 discr_rewards:  [[-0.0522 -0.1981 -0.1113 ... -0.0207 -0.0619 -0.0795]]
 task_rewards:  [[0.0000e+00 7.4506e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2294 -0.4315 -0.22   ... -0.1821 -0.228  -0.3112]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 300, num timesteps 616448, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0331
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.7114
         discriminator_loss   0.9230
                  gail_loss   0.9127
                  grad_loss   0.0104
                    ib_loss  -0.1969
                  task_loss   0.0347
                       beta   0.0000
             posterior_loss   0.4990
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0704 -0.0753 -0.1001 ... -0.0601 -0.0662 -0.1118]]
 discr_rewards:  [[-0.0572 -0.1257 -0.0749 ... -0.0975 -0.0255 -0.1082]]
 task_rewards:  [[0.0000e+00 3.7253e-09 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1671 -0.2406 -0.2144 ... -0.1971 -0.1313 -0.2595]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 301, num timesteps 618496, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0243
                action_loss   0.0026
                    bc_loss   0.0000
               dist_entropy   0.7058
         discriminator_loss   0.9168
                  gail_loss   0.9069
                  grad_loss   0.0099
                    ib_loss  -0.1969
                  task_loss   0.0383
                       beta   0.0000
             posterior_loss   0.4936
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1838 -0.0529 -0.1049 ... -0.2064 -0.0715 -0.0524]]
 discr_rewards:  [[-0.1276 -0.1802 -0.046  ... -0.1047 -0.0271 -0.0573]]
 task_rewards:  [[7.4506e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3509 -0.2726 -0.1903 ... -0.3506 -0.1381 -0.1492]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 302, num timesteps 620544, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0326
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7141
         discriminator_loss   0.9242
                  gail_loss   0.9106
                  grad_loss   0.0135
                    ib_loss  -0.1970
                  task_loss   0.0294
                       beta   0.0000
             posterior_loss   0.4821
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0953 -0.0546 ... -0.151  -0.0782 -0.0553]]
 discr_rewards:  [[-0.0581 -0.0998 -0.0907 ... -0.0042 -0.1553 -0.0518]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.15   -0.2346 -0.1848 ... -0.1947 -0.273  -0.1466]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 303, num timesteps 622592, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0364
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7151
         discriminator_loss   0.9160
                  gail_loss   0.9056
                  grad_loss   0.0105
                    ib_loss  -0.1967
                  task_loss   0.0412
                       beta   0.0000
             posterior_loss   0.4892
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0877 -0.2812 -0.0525 ... -0.0533 -0.0598 -0.1341]]
 discr_rewards:  [[-0.0857 -0.0009 -0.006  ... -0.0691 -0.048  -0.0586]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2129 -0.3216 -0.0981 ... -0.162  -0.1473 -0.2323]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 304, num timesteps 624640, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7110
         discriminator_loss   0.9197
                  gail_loss   0.9086
                  grad_loss   0.0111
                    ib_loss  -0.1968
                  task_loss   0.0440
                       beta   0.0000
             posterior_loss   0.4954
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1513 -0.0676 -0.0744 ... -0.0646 -0.0541 -0.0525]]
 discr_rewards:  [[-0.1154 -0.0916 -0.0326 ... -0.1118 -0.0265 -0.1367]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.3063 -0.1987 -0.1466 ... -0.216  -0.1201 -0.2287]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 305, num timesteps 626688, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0306
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7173
         discriminator_loss   0.9293
                  gail_loss   0.9162
                  grad_loss   0.0131
                    ib_loss  -0.1969
                  task_loss   0.0456
                       beta   0.0000
             posterior_loss   0.4963
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0666 -0.0674 -0.0649 ... -0.0551 -0.0585 -0.0744]]
 discr_rewards:  [[-0.0911 -0.0162 -0.0575 ... -0.0592 -0.0397 -0.0373]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1972 -0.1231 -0.1619 ... -0.1539 -0.1378 -0.1513]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 306, num timesteps 628736, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.7042
         discriminator_loss   0.9197
                  gail_loss   0.9079
                  grad_loss   0.0118
                    ib_loss  -0.1970
                  task_loss   0.0362
                       beta   0.0000
             posterior_loss   0.4781
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0701 -0.0838 -0.0866 ... -0.0526 -0.058  -0.0644]]
 discr_rewards:  [[-0.1168 -0.1798 -0.14   ... -0.0896 -0.0529 -0.0158]]
 task_rewards:  [[ 3.7253e-09  3.7253e-09 -3.7253e-09 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2264 -0.3032 -0.2661 ... -0.1817 -0.1504 -0.1198]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 307, num timesteps 630784, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0365
                action_loss   0.0304
                    bc_loss   0.0000
               dist_entropy   0.7106
         discriminator_loss   0.9210
                  gail_loss   0.9077
                  grad_loss   0.0133
                    ib_loss  -0.1966
                  task_loss   0.0343
                       beta   0.0000
             posterior_loss   0.4928
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0703 -0.0541 -0.0803 ... -0.0585 -0.0994 -0.0761]]
 discr_rewards:  [[-0.0356 -0.0983 -0.0848 ... -0.1641 -0.0341 -0.0248]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1455 -0.1919 -0.2046 ... -0.2621 -0.173  -0.1404]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 308, num timesteps 632832, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0292
                action_loss   0.0019
                    bc_loss   0.0000
               dist_entropy   0.7209
         discriminator_loss   0.9328
                  gail_loss   0.9207
                  grad_loss   0.0121
                    ib_loss  -0.1969
                  task_loss   0.0423
                       beta   0.0000
             posterior_loss   0.4954
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1046 -0.0775 -0.0645 ... -0.0766 -0.0557 -0.1015]]
 discr_rewards:  [[-0.1026 -0.0218 -0.0942 ... -0.0778 -0.0361 -0.0323]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2467 -0.1388 -0.1982 ... -0.1939 -0.1312 -0.1732]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 309, num timesteps 634880, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7526
         discriminator_loss   0.9358
                  gail_loss   0.9229
                  grad_loss   0.0129
                    ib_loss  -0.1971
                  task_loss   0.0376
                       beta   0.0000
             posterior_loss   0.5150
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.132  -0.0552 ... -0.2641 -0.1236 -0.0543]]
 discr_rewards:  [[-0.0905 -0.0618 -0.1995 ... -0.0009 -0.051  -0.0613]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1825 -0.2333 -0.2942 ... -0.3045 -0.2141 -0.1552]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 310, num timesteps 636928, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0287
                action_loss   0.0070
                    bc_loss   0.0000
               dist_entropy   0.7651
         discriminator_loss   0.9348
                  gail_loss   0.9177
                  grad_loss   0.0171
                    ib_loss  -0.1974
                  task_loss   0.0396
                       beta   0.0000
             posterior_loss   0.5134
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0588 -0.1125 ... -0.0525 -0.0714 -0.1148]]
 discr_rewards:  [[-0.0589 -0.0326 -0.0901 ... -0.0946 -0.117  -0.0103]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1508 -0.1309 -0.2422 ... -0.1865 -0.2279 -0.1646]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 311, num timesteps 638976, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0367
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7525
         discriminator_loss   0.9158
                  gail_loss   0.9050
                  grad_loss   0.0108
                    ib_loss  -0.1971
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.5126
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0695 -0.0697 -0.0905 ... -0.0596 -0.0535 -0.0526]]
 discr_rewards:  [[-0.0527 -0.1144 -0.0189 ... -0.1393 -0.0648 -0.0491]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1617 -0.2237 -0.1489 ... -0.2384 -0.1579 -0.1412]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 312, num timesteps 641024, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0419
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7903
         discriminator_loss   0.9332
                  gail_loss   0.9195
                  grad_loss   0.0137
                    ib_loss  -0.1965
                  task_loss   0.0644
                       beta   0.0000
             posterior_loss   0.5181
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.0631 -0.2218 ... -0.1502 -0.054  -0.2457]]
 discr_rewards:  [[-0.0026 -0.0662 -0.0014 ... -0.0598 -0.0153 -0.0831]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0949 -0.1688 -0.2628 ... -0.2494 -0.1088 -0.3684]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 313, num timesteps 643072, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0346
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7792
         discriminator_loss   0.9117
                  gail_loss   0.8992
                  grad_loss   0.0125
                    ib_loss  -0.1969
                  task_loss   0.0606
                       beta   0.0000
             posterior_loss   0.4979
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0678 -0.0531 -0.078  ... -0.2141 -0.057  -0.0545]]
 discr_rewards:  [[-0.0055 -0.1249 -0.1101 ... -0.0234 -0.0303 -0.0886]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1128 -0.2175 -0.2277 ... -0.277  -0.1268 -0.1827]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 314, num timesteps 645120, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0277
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.7790
         discriminator_loss   0.9266
                  gail_loss   0.9119
                  grad_loss   0.0147
                    ib_loss  -0.1971
                  task_loss   0.0375
                       beta   0.0000
             posterior_loss   0.5012
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2546 -0.0549 -0.0743 ... -0.19   -0.075  -0.0979]]
 discr_rewards:  [[-0.0901 -0.1146 -0.0112 ... -0.0069 -0.0907 -0.0194]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.3843 -0.209  -0.1251 ... -0.2364 -0.2052 -0.1568]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 315, num timesteps 647168, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0415
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7860
         discriminator_loss   0.9189
                  gail_loss   0.9047
                  grad_loss   0.0142
                    ib_loss  -0.1968
                  task_loss   0.0324
                       beta   0.0000
             posterior_loss   0.5198
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0725 -0.0636 ... -0.0528 -0.1037 -0.1315]]
 discr_rewards:  [[-0.1117 -0.0351 -0.0495 ... -0.0163 -0.0489 -0.0514]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2036 -0.1471 -0.1526 ... -0.1086 -0.1921 -0.2224]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 316, num timesteps 649216, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0290
                action_loss   0.0021
                    bc_loss   0.0000
               dist_entropy   0.8019
         discriminator_loss   0.9279
                  gail_loss   0.9145
                  grad_loss   0.0134
                    ib_loss  -0.1969
                  task_loss   0.0463
                       beta   0.0000
             posterior_loss   0.4935
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0877 -0.0703 -0.0628 ... -0.1358 -0.1387 -0.0524]]
 discr_rewards:  [[-0.0871 -0.0328 -0.013  ... -0.0347 -0.0096 -0.1618]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2143 -0.1426 -0.1153 ... -0.21   -0.1878 -0.2537]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 317, num timesteps 651264, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0378
                action_loss   0.0032
                    bc_loss   0.0000
               dist_entropy   0.7964
         discriminator_loss   0.9324
                  gail_loss   0.9170
                  grad_loss   0.0154
                    ib_loss  -0.1969
                  task_loss   0.0354
                       beta   0.0000
             posterior_loss   0.4878
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.057  -0.0613 -0.0565 ... -0.0859 -0.1364 -0.1653]]
 discr_rewards:  [[ 0.0058 -0.0306 -0.0823 ... -0.0014 -0.0862 -0.0223]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.0907 -0.1314 -0.1783 ... -0.1268 -0.2622 -0.2271]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 318, num timesteps 653312, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0272
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.8002
         discriminator_loss   0.9221
                  gail_loss   0.9087
                  grad_loss   0.0134
                    ib_loss  -0.1970
                  task_loss   0.0455
                       beta   0.0000
             posterior_loss   0.4800
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0592 -0.0776 -0.1068 ... -0.0692 -0.1208 -0.0629]]
 discr_rewards:  [[-0.0708 -0.2049 -0.153  ... -0.0645 -0.0818 -0.0146]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  3.7253e-09 ... -3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1696 -0.322  -0.2993 ... -0.1732 -0.2422 -0.117 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 319, num timesteps 655360, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0229
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.8194
         discriminator_loss   0.9269
                  gail_loss   0.9142
                  grad_loss   0.0128
                    ib_loss  -0.1970
                  task_loss   0.0307
                       beta   0.0000
             posterior_loss   0.4825
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0662 -0.1037 -0.0803 ... -0.1207 -0.0524 -0.057 ]]
 discr_rewards:  [[-0.0866 -0.0548 -0.0924 ... -0.0285 -0.1195 -0.0315]]
 task_rewards:  [[-3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1923 -0.198  -0.2122 ... -0.1887 -0.2114 -0.1281]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 320, num timesteps 657408, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0296
                action_loss   0.0037
                    bc_loss   0.0000
               dist_entropy   0.8309
         discriminator_loss   0.9276
                  gail_loss   0.9133
                  grad_loss   0.0143
                    ib_loss  -0.1967
                  task_loss   0.0243
                       beta   0.0000
             posterior_loss   0.4983
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1014 -0.0596 -0.0527 ... -0.0532 -0.2009 -0.0563]]
 discr_rewards:  [[-0.0847 -0.1307 -0.0296 ... -0.1304 -0.0162 -0.1377]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.2257 -0.2298 -0.1219 ... -0.2232 -0.2566 -0.2335]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 321, num timesteps 659456, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0291
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.8361
         discriminator_loss   0.9358
                  gail_loss   0.9200
                  grad_loss   0.0158
                    ib_loss  -0.1973
                  task_loss   0.0355
                       beta   0.0000
             posterior_loss   0.4938
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0559 -0.0534 -0.0528 ... -0.0541 -0.1139 -0.0556]]
 discr_rewards:  [[-0.1157 -0.1471 -0.0956 ... -0.1201 -0.025  -0.0423]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2112 -0.24   -0.1879 ... -0.2138 -0.1785 -0.1374]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 322, num timesteps 661504, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0284
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.8483
         discriminator_loss   0.9337
                  gail_loss   0.9193
                  grad_loss   0.0144
                    ib_loss  -0.1967
                  task_loss   0.0325
                       beta   0.0000
             posterior_loss   0.5102
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0567 -0.1279 -0.1139 ... -0.0834 -0.0591 -0.0791]]
 discr_rewards:  [[-0.1273 -0.0565 -0.0626 ... -0.0509 -0.0914 -0.0139]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2236 -0.2238 -0.2161 ... -0.1738 -0.19   -0.1325]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 323, num timesteps 663552, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0264
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.8391
         discriminator_loss   0.9394
                  gail_loss   0.9263
                  grad_loss   0.0131
                    ib_loss  -0.1971
                  task_loss   0.0327
                       beta   0.0000
             posterior_loss   0.4952
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0791 -0.1311 -0.0818 ... -0.2065 -0.1764 -0.182 ]]
 discr_rewards:  [[-0.0362 -0.0627 -0.0627 ... -0.0054 -0.0678 -0.0308]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1549 -0.2334 -0.184  ... -0.2514 -0.2837 -0.2524]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 324, num timesteps 665600, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0328
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.8315
         discriminator_loss   0.9354
                  gail_loss   0.9201
                  grad_loss   0.0153
                    ib_loss  -0.1971
                  task_loss   0.0290
                       beta   0.0000
             posterior_loss   0.4789
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0871 -0.0712 -0.0929 ... -0.0526 -0.0651 -0.0859]]
 discr_rewards:  [[-0.0544 -0.0181 -0.0161 ... -0.0353 -0.024  -0.1429]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 7.4506e-09]]
 final_rewards:  [[-0.181  -0.1289 -0.1485 ... -0.1274 -0.1286 -0.2684]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 325, num timesteps 667648, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0261
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.8095
         discriminator_loss   0.9258
                  gail_loss   0.9128
                  grad_loss   0.0131
                    ib_loss  -0.1969
                  task_loss   0.0383
                       beta   0.0000
             posterior_loss   0.4841
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0626 -0.1508 -0.1556 ... -0.0536 -0.1271 -0.2707]]
 discr_rewards:  [[-0.0792 -0.0841 -0.0468 ... -0.0421 -0.0974 -0.0647]]
 task_rewards:  [[3.7253e-09 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1813 -0.2744 -0.2419 ... -0.1352 -0.264  -0.3749]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 326, num timesteps 669696, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0372
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.8271
         discriminator_loss   0.9294
                  gail_loss   0.9166
                  grad_loss   0.0128
                    ib_loss  -0.1967
                  task_loss   0.0363
                       beta   0.0000
             posterior_loss   0.5124
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0583 -0.1064 -0.2031 ... -0.0685 -0.108  -0.0622]]
 discr_rewards:  [[-0.118  -0.0727 -0.0294 ... -0.1229 -0.0749 -0.0399]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ... -3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2159 -0.2186 -0.272  ... -0.231  -0.2224 -0.1415]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 327, num timesteps 671744, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0369
                action_loss   0.0022
                    bc_loss   0.0000
               dist_entropy   0.8340
         discriminator_loss   0.9344
                  gail_loss   0.9199
                  grad_loss   0.0145
                    ib_loss  -0.1966
                  task_loss   0.0400
                       beta   0.0000
             posterior_loss   0.5026
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0708 -0.0617 ... -0.1779 -0.2551 -0.1566]]
 discr_rewards:  [[-0.0314 -0.06   -0.1232 ... -0.0333  0.0068 -0.0044]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1234 -0.1703 -0.2245 ... -0.2507 -0.2879 -0.2005]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 328, num timesteps 673792, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0330
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.8200
         discriminator_loss   0.9215
                  gail_loss   0.9071
                  grad_loss   0.0144
                    ib_loss  -0.1970
                  task_loss   0.0420
                       beta   0.0000
             posterior_loss   0.5291
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1103 -0.0611 -0.1539 ... -0.0648 -0.0667 -0.0979]]
 discr_rewards:  [[-0.0424 -0.0347 -0.009  ... -0.0269 -0.1243 -0.1018]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1923 -0.1353 -0.2024 ... -0.1312 -0.2305 -0.2392]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 329, num timesteps 675840, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0319
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.8267
         discriminator_loss   0.9329
                  gail_loss   0.9183
                  grad_loss   0.0147
                    ib_loss  -0.1969
                  task_loss   0.0355
                       beta   0.0000
             posterior_loss   0.5002
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0614 -0.0555 -0.0684 ... -0.1706 -0.053  -0.0872]]
 discr_rewards:  [[-0.0587 -0.0877 -0.0792 ... -0.0306 -0.0981 -0.0822]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00 -3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.1596 -0.1827 -0.1871 ... -0.2407 -0.1906 -0.2089]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 330, num timesteps 677888, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss   0.0013
                    bc_loss   0.0000
               dist_entropy   0.8339
         discriminator_loss   0.9165
                  gail_loss   0.9019
                  grad_loss   0.0146
                    ib_loss  -0.1967
                  task_loss   0.0358
                       beta   0.0000
             posterior_loss   0.4936
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0873 -0.0629 -0.125  ... -0.0559 -0.0553 -0.1408]]
 discr_rewards:  [[-0.065  -0.0092 -0.0195 ... -0.048   0.0079 -0.0194]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1918 -0.1116 -0.1841 ... -0.1434 -0.0868 -0.1997]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 331, num timesteps 679936, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.8609
         discriminator_loss   0.9252
                  gail_loss   0.9104
                  grad_loss   0.0148
                    ib_loss  -0.1968
                  task_loss   0.0315
                       beta   0.0000
             posterior_loss   0.4622
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.1654 -0.0681 ... -0.0748 -0.0526 -0.0721]]
 discr_rewards:  [[-0.1327 -0.0549 -0.0106 ... -0.0035 -0.0727 -0.1123]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.2248 -0.2598 -0.1182 ... -0.1179 -0.1648 -0.224 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 332, num timesteps 681984, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0290
                action_loss   0.0054
                    bc_loss   0.0000
               dist_entropy   0.8538
         discriminator_loss   0.9372
                  gail_loss   0.9223
                  grad_loss   0.0148
                    ib_loss  -0.1970
                  task_loss   0.0343
                       beta   0.0000
             posterior_loss   0.4705
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1126 -0.0808 -0.0553 ... -0.0612 -0.0851 -0.1213]]
 discr_rewards:  [[-0.0324 -0.0877 -0.0227 ... -0.0421 -0.1037 -0.1025]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1845 -0.208  -0.1176 ... -0.1429 -0.2283 -0.2633]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 333, num timesteps 684032, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0314
                action_loss   0.0026
                    bc_loss   0.0000
               dist_entropy   0.8467
         discriminator_loss   0.9225
                  gail_loss   0.9065
                  grad_loss   0.0160
                    ib_loss  -0.1968
                  task_loss   0.0311
                       beta   0.0000
             posterior_loss   0.4986
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1288 -0.0545 -0.2142 ... -0.2281 -0.0596 -0.0567]]
 discr_rewards:  [[-0.0684 -0.057  -0.1247 ... -0.0503 -0.0489 -0.0254]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2367 -0.151  -0.3784 ... -0.3179 -0.1481 -0.1216]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 334, num timesteps 686080, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0324
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.8621
         discriminator_loss   0.9352
                  gail_loss   0.9183
                  grad_loss   0.0169
                    ib_loss  -0.1965
                  task_loss   0.0387
                       beta   0.0000
             posterior_loss   0.4905
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0737 -0.1006 -0.0804 ... -0.1114 -0.1484 -0.0532]]
 discr_rewards:  [[-0.1246 -0.0912 -0.097  ... -0.2057 -0.0301 -0.0309]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  3.7253e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2378 -0.2314 -0.2169 ... -0.3566 -0.218  -0.1236]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 335, num timesteps 688128, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0375
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.8562
         discriminator_loss   0.9272
                  gail_loss   0.9114
                  grad_loss   0.0157
                    ib_loss  -0.1970
                  task_loss   0.0373
                       beta   0.0000
             posterior_loss   0.5225
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0534 -0.3318 -0.1341 ... -0.0906 -0.0688 -0.0566]]
 discr_rewards:  [[-0.015  -0.0152 -0.0223 ... -0.1056 -0.0385 -0.0248]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1079 -0.3865 -0.1959 ... -0.2357 -0.1468 -0.1209]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 336, num timesteps 690176, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0352
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.8545
         discriminator_loss   0.9344
                  gail_loss   0.9184
                  grad_loss   0.0160
                    ib_loss  -0.1969
                  task_loss   0.0424
                       beta   0.0000
             posterior_loss   0.5031
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1231 -0.0562 -0.2444 ... -0.1433 -0.0668 -0.1303]]
 discr_rewards:  [[-0.0818 -0.1345 -0.0524 ... -0.1432 -0.0853 -0.0013]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2444 -0.2303 -0.3364 ... -0.326  -0.1916 -0.1711]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 337, num timesteps 692224, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0314
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.8366
         discriminator_loss   0.9188
                  gail_loss   0.9038
                  grad_loss   0.0150
                    ib_loss  -0.1971
                  task_loss   0.0377
                       beta   0.0000
             posterior_loss   0.4831
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.3432 -0.333  -0.0793 ... -0.0535 -0.0525 -0.1574]]
 discr_rewards:  [[-0.1833 -0.1329 -0.1049 ... -0.0448 -0.0443 -0.0648]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.566  -0.5054 -0.2237 ... -0.1378 -0.1364 -0.2617]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 338, num timesteps 694272, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0305
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.8398
         discriminator_loss   0.9191
                  gail_loss   0.9042
                  grad_loss   0.0150
                    ib_loss  -0.1968
                  task_loss   0.0363
                       beta   0.0000
             posterior_loss   0.4981
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1478 -0.1165 -0.11   ... -0.0539 -0.0645 -0.0655]]
 discr_rewards:  [[-0.0969 -0.0817 -0.0122 ... -0.0826 -0.053  -0.0983]]
 task_rewards:  [[-3.7253e-09 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2843 -0.2377 -0.1617 ... -0.176  -0.157  -0.2033]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 339, num timesteps 696320, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0244
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.8222
         discriminator_loss   0.9322
                  gail_loss   0.9170
                  grad_loss   0.0152
                    ib_loss  -0.1970
                  task_loss   0.0334
                       beta   0.0000
             posterior_loss   0.4847
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0575 -0.1523 -0.1579 ... -0.0713 -0.1258 -0.1395]]
 discr_rewards:  [[-0.0285 -0.0856 -0.1424 ... -0.0871 -0.0139 -0.0188]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1255 -0.2775 -0.3398 ... -0.198  -0.1792 -0.1978]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 340, num timesteps 698368, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0249
                action_loss   0.0018
                    bc_loss   0.0000
               dist_entropy   0.8271
         discriminator_loss   0.9149
                  gail_loss   0.9017
                  grad_loss   0.0131
                    ib_loss  -0.1966
                  task_loss   0.0277
                       beta   0.0000
             posterior_loss   0.5007
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0987 -0.289  -0.1323 ... -0.0526 -0.1267 -0.0525]]
 discr_rewards:  [[-0.0707 -0.0175 -0.0469 ... -0.0638 -0.0828 -0.1606]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.2089 -0.346  -0.2187 ... -0.1559 -0.2491 -0.2527]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 341, num timesteps 700416, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0345
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.8318
         discriminator_loss   0.9327
                  gail_loss   0.9174
                  grad_loss   0.0153
                    ib_loss  -0.1971
                  task_loss   0.0303
                       beta   0.0000
             posterior_loss   0.5148
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0797 -0.5171 -0.0817 ... -0.0544 -0.0525 -0.1538]]
 discr_rewards:  [[-0.0259 -0.0594 -0.0646 ... -0.0171 -0.0121 -0.1131]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1451 -0.616  -0.1858 ... -0.111  -0.1041 -0.3063]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 342, num timesteps 702464, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0357
                action_loss   0.0013
                    bc_loss   0.0000
               dist_entropy   0.8113
         discriminator_loss   0.9247
                  gail_loss   0.9101
                  grad_loss   0.0146
                    ib_loss  -0.1970
                  task_loss   0.0398
                       beta   0.0000
             posterior_loss   0.5274
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2347 -0.1038 -0.2072 ... -0.0548 -0.0754 -0.0582]]
 discr_rewards:  [[-0.1107 -0.1176 -0.0139 ... -0.0718 -0.1008 -0.1547]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.385  -0.2609 -0.2607 ... -0.166  -0.2158 -0.2524]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 343, num timesteps 704512, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0347
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.8016
         discriminator_loss   0.9244
                  gail_loss   0.9102
                  grad_loss   0.0142
                    ib_loss  -0.1972
                  task_loss   0.0394
                       beta   0.0000
             posterior_loss   0.5092
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0704 -0.1971 -0.0697 ... -0.152  -0.0622 -0.058 ]]
 discr_rewards:  [[-0.0271 -0.0523 -0.0886 ... -0.0766 -0.0231 -0.1417]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.137  -0.2889 -0.1978 ... -0.2681 -0.1248 -0.2393]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 344, num timesteps 706560, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0257
                action_loss   0.0019
                    bc_loss   0.0000
               dist_entropy   0.8075
         discriminator_loss   0.9312
                  gail_loss   0.9175
                  grad_loss   0.0137
                    ib_loss  -0.1970
                  task_loss   0.0385
                       beta   0.0000
             posterior_loss   0.5124
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0534 -0.0593 -0.0574 ... -0.225  -0.0632 -0.0617]]
 discr_rewards:  [[-0.0249 -0.1748 -0.1151 ... -0.0286 -0.0595 -0.033 ]]
 task_rewards:  [[0.0000e+00 7.4506e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1179 -0.2736 -0.212  ... -0.2931 -0.1622 -0.1343]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 345, num timesteps 708608, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0288
                action_loss   0.0020
                    bc_loss   0.0000
               dist_entropy   0.8125
         discriminator_loss   0.9247
                  gail_loss   0.9100
                  grad_loss   0.0147
                    ib_loss  -0.1967
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.5102
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0656 -0.0531 -0.0526 ... -0.1286 -0.0677 -0.0951]]
 discr_rewards:  [[-0.0294 -0.0337 -0.0646 ... -0.0609 -0.0426 -0.0377]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1345 -0.1263 -0.1568 ... -0.229  -0.1498 -0.1723]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 346, num timesteps 710656, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0264
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.8036
         discriminator_loss   0.9292
                  gail_loss   0.9154
                  grad_loss   0.0138
                    ib_loss  -0.1970
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.4954
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1363 -0.0595 -0.1426 ... -0.0585 -0.0525 -0.0791]]
 discr_rewards:  [[-0.0038 -0.0401 -0.0512 ... -0.0566 -0.1384 -0.0068]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1796 -0.1391 -0.2333 ... -0.1546 -0.2305 -0.1255]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 347, num timesteps 712704, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0314
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7947
         discriminator_loss   0.9211
                  gail_loss   0.9067
                  grad_loss   0.0144
                    ib_loss  -0.1969
                  task_loss   0.0298
                       beta   0.0000
             posterior_loss   0.4897
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0902 -0.2541 -0.33   ... -0.0688 -0.0598 -0.0711]]
 discr_rewards:  [[-0.0427 -0.1435 -0.0483 ... -0.0627 -0.1417 -0.0272]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1724 -0.4371 -0.4179 ... -0.171  -0.2411 -0.1378]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 348, num timesteps 714752, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0348
                action_loss   0.0020
                    bc_loss   0.0000
               dist_entropy   0.8161
         discriminator_loss   0.9265
                  gail_loss   0.9120
                  grad_loss   0.0146
                    ib_loss  -0.1969
                  task_loss   0.0422
                       beta   0.0000
             posterior_loss   0.4881
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1796 -0.073  -0.1466 ... -0.0683 -0.0546 -0.0926]]
 discr_rewards:  [[-0.0254 -0.148  -0.1057 ... -0.0309 -0.0532 -0.0325]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2445 -0.2605 -0.2918 ... -0.1387 -0.1473 -0.1646]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 349, num timesteps 716800, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0318
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.8163
         discriminator_loss   0.9327
                  gail_loss   0.9180
                  grad_loss   0.0147
                    ib_loss  -0.1968
                  task_loss   0.0396
                       beta   0.0000
             posterior_loss   0.4747
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.1003 -0.054  ... -0.0696 -0.0544 -0.0842]]
 discr_rewards:  [[-0.0037 -0.1365 -0.0256 ... -0.0434 -0.1278 -0.0171]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.0959 -0.2763 -0.1192 ... -0.1525 -0.2217 -0.1408]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 350, num timesteps 718848, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.7968
         discriminator_loss   0.9293
                  gail_loss   0.9163
                  grad_loss   0.0131
                    ib_loss  -0.1972
                  task_loss   0.0425
                       beta   0.0000
             posterior_loss   0.5261
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.082  -0.0573 -0.092  ... -0.1196 -0.0588 -0.0524]]
 discr_rewards:  [[-0.0462 -0.0463 -0.0204 ... -0.0694 -0.0812 -0.0323]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1677 -0.1431 -0.1519 ... -0.2285 -0.1795 -0.1242]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 351, num timesteps 720896, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0368
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7896
         discriminator_loss   0.9228
                  gail_loss   0.9094
                  grad_loss   0.0135
                    ib_loss  -0.1971
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.5085
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.6021 -0.0524 -0.2849 ... -0.055  -0.0661 -0.0553]]
 discr_rewards:  [[-0.0102 -0.0754 -0.0534 ... -0.0976 -0.1618 -0.0243]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.6518 -0.1673 -0.3779 ... -0.1921 -0.2675 -0.1192]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 352, num timesteps 722944, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0315
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7854
         discriminator_loss   0.9333
                  gail_loss   0.9214
                  grad_loss   0.0118
                    ib_loss  -0.1968
                  task_loss   0.0407
                       beta   0.0000
             posterior_loss   0.5062
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0558 -0.0524 -0.1402 ... -0.1567 -0.0553 -0.1971]]
 discr_rewards:  [[-0.2144 -0.0136 -0.1256 ... -0.0212 -0.1202 -0.1479]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00 -3.7253e-09
   7.4506e-09]]
 final_rewards:  [[-0.3097 -0.1055 -0.3053 ... -0.2174 -0.215  -0.3846]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 353, num timesteps 724992, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7486
         discriminator_loss   0.9211
                  gail_loss   0.9084
                  grad_loss   0.0127
                    ib_loss  -0.1973
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.5013
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1007 -0.0525 -0.1562 ... -0.062  -0.077  -0.1257]]
 discr_rewards:  [[-0.1639 -0.1278 -0.0699 ... -0.0637 -0.0106 -0.0091]]
 task_rewards:  [[ 0.0000e+00 -7.4506e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.3041 -0.2198 -0.2656 ... -0.1652 -0.1271 -0.1743]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 354, num timesteps 727040, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0284
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.7547
         discriminator_loss   0.9377
                  gail_loss   0.9248
                  grad_loss   0.0129
                    ib_loss  -0.1966
                  task_loss   0.0338
                       beta   0.0000
             posterior_loss   0.5094
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0545 -0.108  -0.0547 ... -0.0524 -0.0584 -0.0647]]
 discr_rewards:  [[-0.0377 -0.0846 -0.103  ... -0.0094 -0.1302 -0.0459]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1318 -0.2322 -0.1972 ... -0.1013 -0.2281 -0.1502]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 355, num timesteps 729088, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0255
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7573
         discriminator_loss   0.9284
                  gail_loss   0.9154
                  grad_loss   0.0130
                    ib_loss  -0.1965
                  task_loss   0.0318
                       beta   0.0000
             posterior_loss   0.4673
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0999 -0.1396 -0.072  ... -0.0604 -0.0687 -0.1575]]
 discr_rewards:  [[-1.7490e-01 -3.8052e-02 -5.6033e-02 ... -5.0858e-02  1.6546e-04
  -8.8320e-02]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3143 -0.2172 -0.1675 ... -0.1508 -0.108  -0.2853]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 356, num timesteps 731136, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0335
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7461
         discriminator_loss   0.9331
                  gail_loss   0.9213
                  grad_loss   0.0118
                    ib_loss  -0.1971
                  task_loss   0.0334
                       beta   0.0000
             posterior_loss   0.4898
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.28   -0.0998 -0.061  ... -0.102  -0.191  -0.0689]]
 discr_rewards:  [[-0.1203 -0.0575 -0.1305 ... -0.044  -0.1744 -0.0145]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  3.7253e-09 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.4398 -0.1967 -0.2311 ... -0.1855 -0.4049 -0.1229]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 357, num timesteps 733184, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0347
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7401
         discriminator_loss   0.9448
                  gail_loss   0.9341
                  grad_loss   0.0107
                    ib_loss  -0.1970
                  task_loss   0.0362
                       beta   0.0000
             posterior_loss   0.5063
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0797 -0.0534 -0.1814 ... -0.1448 -0.0655 -0.0941]]
 discr_rewards:  [[-0.0014 -0.1015 -0.0363 ... -0.1135 -0.0789 -0.0849]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1207 -0.1945 -0.2572 ... -0.2978 -0.184  -0.2185]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 358, num timesteps 735232, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0252
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7303
         discriminator_loss   0.9143
                  gail_loss   0.9035
                  grad_loss   0.0108
                    ib_loss  -0.1972
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.4754
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0563 -0.0528 ... -0.0755 -0.0563 -0.1368]]
 discr_rewards:  [[-0.0702 -0.0638 -0.0562 ... -0.0621 -0.0594 -0.0699]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1622 -0.1595 -0.1486 ... -0.1771 -0.1552 -0.2462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 359, num timesteps 737280, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7340
         discriminator_loss   0.9244
                  gail_loss   0.9122
                  grad_loss   0.0122
                    ib_loss  -0.1971
                  task_loss   0.0280
                       beta   0.0000
             posterior_loss   0.5201
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1392 -0.0829 -0.1062 ... -0.0559 -0.0645 -0.0553]]
 discr_rewards:  [[-0.155  -0.058  -0.0651 ... -0.043  -0.0406 -0.0951]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.3338 -0.1804 -0.2108 ... -0.1384 -0.1446 -0.1899]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 360, num timesteps 739328, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0315
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7387
         discriminator_loss   0.9263
                  gail_loss   0.9148
                  grad_loss   0.0115
                    ib_loss  -0.1968
                  task_loss   0.0337
                       beta   0.0000
             posterior_loss   0.5219
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2386 -0.0526 -0.1073 ... -0.0524 -0.0622 -0.0678]]
 discr_rewards:  [[ 0.0023 -0.1469 -0.0258 ... -0.0764 -0.0457 -0.0352]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2758 -0.239  -0.1726 ... -0.1683 -0.1474 -0.1425]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 361, num timesteps 741376, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0296
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7263
         discriminator_loss   0.9243
                  gail_loss   0.9121
                  grad_loss   0.0123
                    ib_loss  -0.1967
                  task_loss   0.0365
                       beta   0.0000
             posterior_loss   0.5197
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0651 -0.0528 -0.0652 ... -0.0721 -0.1146 -0.0638]]
 discr_rewards:  [[-0.0304 -0.1271 -0.0333 ... -0.0321 -0.0464 -0.0182]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1351 -0.2194 -0.138  ... -0.1438 -0.2005 -0.1215]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 362, num timesteps 743424, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0339
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.7227
         discriminator_loss   0.9171
                  gail_loss   0.9060
                  grad_loss   0.0111
                    ib_loss  -0.1971
                  task_loss   0.0319
                       beta   0.0000
             posterior_loss   0.4951
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0555 -0.1797 -0.134  ... -0.0812 -0.0531 -0.0566]]
 discr_rewards:  [[-0.0461 -0.0423 -0.0775 ... -0.0927 -0.0474 -0.0412]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1412 -0.2615 -0.251  ... -0.2134 -0.14   -0.1373]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 363, num timesteps 745472, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0273
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7324
         discriminator_loss   0.9154
                  gail_loss   0.9041
                  grad_loss   0.0114
                    ib_loss  -0.1969
                  task_loss   0.0367
                       beta   0.0000
             posterior_loss   0.5015
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0696 -0.0927 -0.4407 ... -0.0529 -0.0551 -0.1992]]
 discr_rewards:  [[-0.0868 -0.0232 -0.0508 ... -0.0015 -0.0258 -0.0182]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1958 -0.1554 -0.5311 ... -0.0939 -0.1204 -0.2569]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 364, num timesteps 747520, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0286
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7232
         discriminator_loss   0.9111
                  gail_loss   0.8998
                  grad_loss   0.0113
                    ib_loss  -0.1970
                  task_loss   0.0328
                       beta   0.0000
             posterior_loss   0.4948
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0726 -0.0622 -0.0901 ... -0.0551 -0.1099 -0.0683]]
 discr_rewards:  [[-0.017  -0.1255 -0.0928 ... -0.0469 -0.0353 -0.0422]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1291 -0.2272 -0.2225 ... -0.1416 -0.1847 -0.1499]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 365, num timesteps 749568, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0272
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7260
         discriminator_loss   0.9187
                  gail_loss   0.9075
                  grad_loss   0.0113
                    ib_loss  -0.1969
                  task_loss   0.0316
                       beta   0.0000
             posterior_loss   0.5172
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0654 -0.2188 -0.0529 ... -0.057  -0.1135 -0.285 ]]
 discr_rewards:  [[-0.035  -0.0814 -0.0169 ... -0.0017 -0.0494 -0.0613]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.14   -0.3397 -0.1094 ... -0.0982 -0.2024 -0.3858]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 366, num timesteps 751616, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0327
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7278
         discriminator_loss   0.9227
                  gail_loss   0.9115
                  grad_loss   0.0112
                    ib_loss  -0.1969
                  task_loss   0.0335
                       beta   0.0000
             posterior_loss   0.5035
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0545 -0.1152 -0.0607 ... -0.1427 -0.0555 -0.0531]]
 discr_rewards:  [[-0.0168 -0.0468 -0.1392 ... -0.1704 -0.0743 -0.0464]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -7.4506e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1108 -0.2015 -0.2394 ... -0.3526 -0.1693 -0.139 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 367, num timesteps 753664, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.7212
         discriminator_loss   0.9228
                  gail_loss   0.9118
                  grad_loss   0.0110
                    ib_loss  -0.1968
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.5128
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0755 -0.0649 -0.0675 ... -0.0632 -0.2063 -0.0526]]
 discr_rewards:  [[-0.1007 -0.0886 -0.1525 ... -0.0401 -0.0714 -0.0415]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2158 -0.193  -0.2595 ... -0.1428 -0.3172 -0.1335]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 368, num timesteps 755712, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0256
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.7233
         discriminator_loss   0.9235
                  gail_loss   0.9133
                  grad_loss   0.0102
                    ib_loss  -0.1969
                  task_loss   0.0321
                       beta   0.0000
             posterior_loss   0.5088
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1184 -0.0533 -0.0602 ... -0.0533 -0.056  -0.0536]]
 discr_rewards:  [[-0.1006 -0.0123 -0.0598 ... -0.012  -0.0819 -0.1017]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.2585 -0.1051 -0.1595 ... -0.1048 -0.1774 -0.1948]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 369, num timesteps 757760, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0278
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.7187
         discriminator_loss   0.9169
                  gail_loss   0.9065
                  grad_loss   0.0104
                    ib_loss  -0.1969
                  task_loss   0.0293
                       beta   0.0000
             posterior_loss   0.5132
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0613 -0.1253 -0.1781 ... -0.0749 -0.056  -0.1775]]
 discr_rewards:  [[-0.02   -0.0306 -0.0115 ... -0.149  -0.1848 -0.0244]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -7.4506e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1209 -0.1955 -0.2291 ... -0.2634 -0.2804 -0.2414]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 370, num timesteps 759808, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0270
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.7230
         discriminator_loss   0.9268
                  gail_loss   0.9151
                  grad_loss   0.0116
                    ib_loss  -0.1967
                  task_loss   0.0334
                       beta   0.0000
             posterior_loss   0.4935
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0632 -0.1263 -0.1693 ... -0.0761 -0.2353 -0.1274]]
 discr_rewards:  [[-0.1016 -0.0473 -0.0635 ... -0.0396 -0.098  -0.0153]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2043 -0.2131 -0.2724 ... -0.1553 -0.3728 -0.1822]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 371, num timesteps 761856, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0248
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7301
         discriminator_loss   0.9252
                  gail_loss   0.9151
                  grad_loss   0.0101
                    ib_loss  -0.1968
                  task_loss   0.0328
                       beta   0.0000
             posterior_loss   0.4656
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0825 -0.0525 -0.0545 ... -0.0791 -0.2644 -0.0551]]
 discr_rewards:  [[-0.1707 -0.0985 -0.01   ... -0.0314 -0.001  -0.0381]]
 task_rewards:  [[3.7253e-09 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2927 -0.1905 -0.1041 ... -0.15   -0.3049 -0.1328]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 372, num timesteps 763904, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0348
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.7262
         discriminator_loss   0.9240
                  gail_loss   0.9132
                  grad_loss   0.0107
                    ib_loss  -0.1970
                  task_loss   0.0291
                       beta   0.0000
             posterior_loss   0.5206
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.3065 -0.214  -0.0534 ... -0.0535 -0.0803 -0.0839]]
 discr_rewards:  [[-0.0812 -0.0907 -0.062  ... -0.1224 -0.0075 -0.0776]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  3.7253e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.4272 -0.3443 -0.1549 ... -0.2154 -0.1273 -0.201 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 373, num timesteps 765952, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0363
                action_loss   0.0015
                    bc_loss   0.0000
               dist_entropy   0.7172
         discriminator_loss   0.9373
                  gail_loss   0.9260
                  grad_loss   0.0113
                    ib_loss  -0.1966
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.5008
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0607 -0.0687 -0.287  ... -0.0555 -0.0609 -0.3388]]
 discr_rewards:  [[-0.1318 -0.1309 -0.1569 ... -0.1074 -0.1379 -0.0394]]
 task_rewards:  [[-3.7253e-09  3.7253e-09 -3.7253e-09 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2319 -0.2392 -0.4834 ... -0.2024 -0.2383 -0.4177]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 374, num timesteps 768000, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0312
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7117
         discriminator_loss   0.9315
                  gail_loss   0.9199
                  grad_loss   0.0116
                    ib_loss  -0.1969
                  task_loss   0.0406
                       beta   0.0000
             posterior_loss   0.5002
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1361 -0.091  -0.0796 ... -0.0646 -0.0768 -0.1785]]
 discr_rewards:  [[-0.0384 -0.0853 -0.1108 ... -0.052  -0.0166 -0.0281]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.214  -0.2158 -0.2299 ... -0.1561 -0.1329 -0.2462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 375, num timesteps 770048, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0447
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.7139
         discriminator_loss   0.9247
                  gail_loss   0.9145
                  grad_loss   0.0102
                    ib_loss  -0.1969
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.5043
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1315 -0.0867 -0.0772 ... -0.0544 -0.0642 -0.0559]]
 discr_rewards:  [[-0.0892 -0.1215 -0.0783 ... -0.1117 -0.0383 -0.052 ]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2602 -0.2478 -0.1951 ... -0.2057 -0.142  -0.1474]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 376, num timesteps 772096, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss   0.0053
                    bc_loss   0.0000
               dist_entropy   0.7144
         discriminator_loss   0.9240
                  gail_loss   0.9133
                  grad_loss   0.0107
                    ib_loss  -0.1969
                  task_loss   0.0514
                       beta   0.0000
             posterior_loss   0.4981
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0975 -0.0554 -0.0558 ... -0.3593 -0.1151 -0.0665]]
 discr_rewards:  [[-0.0601 -0.037  -0.0739 ... -0.0174 -0.1322 -0.0166]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.1971 -0.132  -0.1692 ... -0.4162 -0.2869 -0.1226]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 377, num timesteps 774144, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0357
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7124
         discriminator_loss   0.9289
                  gail_loss   0.9185
                  grad_loss   0.0104
                    ib_loss  -0.1971
                  task_loss   0.0363
                       beta   0.0000
             posterior_loss   0.5043
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0975 -0.0864 -0.0595 ... -0.0524 -0.0637 -0.135 ]]
 discr_rewards:  [[-0.0753 -0.0551 -0.0957 ... -0.0519 -0.1037 -0.0929]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2123 -0.181  -0.1947 ... -0.1438 -0.2069 -0.2675]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 378, num timesteps 776192, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0333
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.7278
         discriminator_loss   0.9247
                  gail_loss   0.9137
                  grad_loss   0.0110
                    ib_loss  -0.1966
                  task_loss   0.0456
                       beta   0.0000
             posterior_loss   0.4747
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0956 -0.0704 -0.0899 ... -0.0524 -0.0525 -0.0551]]
 discr_rewards:  [[ 0.0026 -0.0165 -0.0179 ... -0.1054 -0.0804 -0.1175]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.1326 -0.1264 -0.1473 ... -0.1974 -0.1725 -0.2122]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 379, num timesteps 778240, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0253
                action_loss   0.0028
                    bc_loss   0.0000
               dist_entropy   0.7219
         discriminator_loss   0.9182
                  gail_loss   0.9059
                  grad_loss   0.0123
                    ib_loss  -0.1969
                  task_loss   0.0403
                       beta   0.0000
             posterior_loss   0.5038
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0655 -0.0668 -0.0681 ... -0.0781 -0.1128 -0.0527]]
 discr_rewards:  [[-0.0196 -0.0338 -0.2102 ...  0.0004 -0.1034 -0.0469]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1247 -0.1401 -0.3178 ... -0.1173 -0.2557 -0.1391]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 380, num timesteps 780288, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0223
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.7154
         discriminator_loss   0.9168
                  gail_loss   0.9064
                  grad_loss   0.0104
                    ib_loss  -0.1970
                  task_loss   0.0269
                       beta   0.0000
             posterior_loss   0.4854
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.055  -0.0575 -0.2952 ... -0.148  -0.0524 -0.3277]]
 discr_rewards:  [[-0.0291 -0.0558 -0.0534 ... -0.0252 -0.0362 -0.0714]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1237 -0.1528 -0.3881 ... -0.2127 -0.1281 -0.4386]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 381, num timesteps 782336, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0303
                action_loss   0.0008
                    bc_loss   0.0000
               dist_entropy   0.7278
         discriminator_loss   0.9252
                  gail_loss   0.9134
                  grad_loss   0.0118
                    ib_loss  -0.1968
                  task_loss   0.0256
                       beta   0.0000
             posterior_loss   0.5060
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2085 -0.1231 -0.0545 ... -0.058  -0.1591 -0.0579]]
 discr_rewards:  [[-0.0281 -0.0063 -0.0573 ... -0.0661 -0.0792 -0.0943]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2761 -0.1689 -0.1513 ... -0.1636 -0.2779 -0.1918]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 382, num timesteps 784384, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0360
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.7258
         discriminator_loss   0.9274
                  gail_loss   0.9169
                  grad_loss   0.0106
                    ib_loss  -0.1971
                  task_loss   0.0377
                       beta   0.0000
             posterior_loss   0.5128
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0758 -0.0646 -0.0809 ... -0.1777 -0.13   -0.1152]]
 discr_rewards:  [[-0.0244 -0.0132 -0.082  ... -0.0166 -0.0346 -0.0007]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1398 -0.1173 -0.2024 ... -0.2339 -0.2042 -0.1554]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 383, num timesteps 786432, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0315
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.7222
         discriminator_loss   0.9085
                  gail_loss   0.8978
                  grad_loss   0.0108
                    ib_loss  -0.1965
                  task_loss   0.0458
                       beta   0.0000
             posterior_loss   0.5078
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0532 -0.0884 -0.0739 ... -0.0687 -0.0524 -0.1339]]
 discr_rewards:  [[-0.0162 -0.1021 -0.0169 ... -0.032  -0.0373 -0.1423]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1089 -0.23   -0.1304 ... -0.1402 -0.1292 -0.3157]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 384, num timesteps 788480, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0314
                action_loss   0.0019
                    bc_loss   0.0000
               dist_entropy   0.7167
         discriminator_loss   0.9197
                  gail_loss   0.9084
                  grad_loss   0.0113
                    ib_loss  -0.1971
                  task_loss   0.0340
                       beta   0.0000
             posterior_loss   0.5097
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0587 -0.1764 -0.1651 ... -0.0823 -0.161  -0.1882]]
 discr_rewards:  [[-0.0917 -0.0351 -0.0037 ... -0.0634 -0.0056 -0.0422]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.19   -0.251  -0.2083 ... -0.1852 -0.2062 -0.2699]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 385, num timesteps 790528, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0280
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7295
         discriminator_loss   0.9265
                  gail_loss   0.9150
                  grad_loss   0.0115
                    ib_loss  -0.1969
                  task_loss   0.0365
                       beta   0.0000
             posterior_loss   0.5004
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0602 -0.0816 -0.0759 ... -0.0524 -0.0712 -0.0819]]
 discr_rewards:  [[-0.1087 -0.0421 -0.0312 ... -0.0396 -0.0615 -0.0789]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2083 -0.1632 -0.1466 ... -0.1315 -0.1722 -0.2003]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 386, num timesteps 792576, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0387
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7162
         discriminator_loss   0.9199
                  gail_loss   0.9080
                  grad_loss   0.0119
                    ib_loss  -0.1970
                  task_loss   0.0350
                       beta   0.0000
             posterior_loss   0.5129
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0563 -0.0581 -0.0543 ... -0.1025 -0.135  -0.0525]]
 discr_rewards:  [[-0.0295 -0.0471 -0.0597 ... -0.065  -0.0147 -0.105 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.1253 -0.1448 -0.1536 ... -0.207  -0.1892 -0.197 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 387, num timesteps 794624, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7193
         discriminator_loss   0.9273
                  gail_loss   0.9163
                  grad_loss   0.0110
                    ib_loss  -0.1966
                  task_loss   0.0439
                       beta   0.0000
             posterior_loss   0.5264
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0546 -0.1203 -0.0524 ... -0.0715 -0.0532 -0.0599]]
 discr_rewards:  [[-0.0871 -0.0543 -0.0986 ... -0.1784 -0.1536 -0.0315]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 7.4506e-09 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1811 -0.2141 -0.1905 ... -0.2895 -0.2463 -0.1309]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 388, num timesteps 796672, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0311
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.7162
         discriminator_loss   0.9247
                  gail_loss   0.9126
                  grad_loss   0.0120
                    ib_loss  -0.1969
                  task_loss   0.0397
                       beta   0.0000
             posterior_loss   0.5000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.071  -0.0629 -0.0598 ... -0.1487 -0.227  -0.0528]]
 discr_rewards:  [[-0.0478 -0.0966 -0.074  ... -0.0724 -0.1094 -0.1382]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.1583 -0.199  -0.1733 ... -0.2607 -0.3759 -0.2305]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 389, num timesteps 798720, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0344
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.7229
         discriminator_loss   0.9126
                  gail_loss   0.9013
                  grad_loss   0.0113
                    ib_loss  -0.1968
                  task_loss   0.0334
                       beta   0.0000
             posterior_loss   0.5009
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0701 -0.0577 -0.1155 ... -0.0541 -0.073  -0.1436]]
 discr_rewards:  [[ 0.0011 -0.0341  0.0009 ... -0.1843 -0.0516 -0.0295]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1085 -0.1314 -0.1541 ... -0.2779 -0.1641 -0.2125]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 390, num timesteps 800768, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0360
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.7262
         discriminator_loss   0.9219
                  gail_loss   0.9099
                  grad_loss   0.0121
                    ib_loss  -0.1969
                  task_loss   0.0398
                       beta   0.0000
             posterior_loss   0.5130
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.064  -0.1525 -0.0708 ... -0.1594 -0.0896 -0.0524]]
 discr_rewards:  [[-0.098  -0.018  -0.0647 ... -0.077  -0.1922 -0.0869]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.2016 -0.21   -0.1751 ... -0.2759 -0.3213 -0.1788]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 391, num timesteps 802816, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0317
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7198
         discriminator_loss   0.9212
                  gail_loss   0.9103
                  grad_loss   0.0109
                    ib_loss  -0.1971
                  task_loss   0.0453
                       beta   0.0000
             posterior_loss   0.4810
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0589 -0.1576 -0.3443 ... -0.3933 -0.0576 -0.2118]]
 discr_rewards:  [[-0.022  -0.0628 -0.0399 ... -0.126  -0.0864 -0.041 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1204 -0.26   -0.4237 ... -0.5588 -0.1835 -0.2924]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 392, num timesteps 804864, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0357
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.7119
         discriminator_loss   0.9108
                  gail_loss   0.9000
                  grad_loss   0.0108
                    ib_loss  -0.1968
                  task_loss   0.0404
                       beta   0.0000
             posterior_loss   0.5302
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1028 -0.0596 -0.0709 ... -0.0562 -0.0689 -0.3495]]
 discr_rewards:  [[-0.0392 -0.0578 -0.0363 ... -0.0287 -0.0841 -0.1555]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1816 -0.1569 -0.1467 ... -0.1244 -0.1925 -0.5445]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 393, num timesteps 806912, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0341
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.7173
         discriminator_loss   0.9255
                  gail_loss   0.9144
                  grad_loss   0.0111
                    ib_loss  -0.1970
                  task_loss   0.0399
                       beta   0.0000
             posterior_loss   0.5392
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1072 -0.0945 -0.0671 ... -0.2541 -0.0572 -0.2659]]
 discr_rewards:  [[-0.0642  0.0001 -0.0785 ... -0.0929 -0.1251 -0.0247]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2109 -0.1338 -0.1851 ... -0.3865 -0.2218 -0.3301]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 394, num timesteps 808960, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0257
                action_loss   0.0018
                    bc_loss   0.0000
               dist_entropy   0.7302
         discriminator_loss   0.9181
                  gail_loss   0.9068
                  grad_loss   0.0112
                    ib_loss  -0.1968
                  task_loss   0.0401
                       beta   0.0000
             posterior_loss   0.4975
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1953 -0.1003 -0.1274 ... -0.1169 -0.0529 -0.0579]]
 discr_rewards:  [[-0.071  -0.0125 -0.0998 ... -0.1359 -0.0846 -0.0656]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ... -7.4506e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.3058 -0.1523 -0.2667 ... -0.2923 -0.1771 -0.163 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 395, num timesteps 811008, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0339
                action_loss   0.0027
                    bc_loss   0.0000
               dist_entropy   0.7207
         discriminator_loss   0.9095
                  gail_loss   0.8994
                  grad_loss   0.0102
                    ib_loss  -0.1972
                  task_loss   0.0320
                       beta   0.0000
             posterior_loss   0.4822
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0537 -0.1999 -0.1825 ... -0.1115 -0.1042 -0.3661]]
 discr_rewards:  [[-0.0727 -0.0276 -0.0881 ... -0.0879 -0.1117 -0.0226]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1659 -0.2671 -0.3101 ... -0.239  -0.2555 -0.4282]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 396, num timesteps 813056, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0318
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.7261
         discriminator_loss   0.9165
                  gail_loss   0.9068
                  grad_loss   0.0097
                    ib_loss  -0.1967
                  task_loss   0.0379
                       beta   0.0000
             posterior_loss   0.5076
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.071  -0.0654 -0.0528 ... -0.2673 -0.0532 -0.0687]]
 discr_rewards:  [[-0.1611 -0.0125 -0.1208 ... -0.1239 -0.0395 -0.0337]]
 task_rewards:  [[-7.4506e-09  0.0000e+00  3.7253e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2716 -0.1174 -0.2132 ... -0.4307 -0.1323 -0.142 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 397, num timesteps 815104, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7387
         discriminator_loss   0.9345
                  gail_loss   0.9239
                  grad_loss   0.0106
                    ib_loss  -0.1967
                  task_loss   0.0397
                       beta   0.0000
             posterior_loss   0.5305
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1242 -0.0552 -0.0527 ... -0.0525 -0.1041 -0.1086]]
 discr_rewards:  [[-0.0897 -0.0764 -0.0604 ... -0.0246 -0.0241  0.0006]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2534 -0.1711 -0.1527 ... -0.1166 -0.1677 -0.1475]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 398, num timesteps 817152, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0296
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.7375
         discriminator_loss   0.9190
                  gail_loss   0.9079
                  grad_loss   0.0110
                    ib_loss  -0.1971
                  task_loss   0.0402
                       beta   0.0000
             posterior_loss   0.5233
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0635 -0.078  -0.0525 ... -0.0868 -0.1465 -0.1252]]
 discr_rewards:  [[-0.0157 -0.0303 -0.0627 ... -0.1123 -0.028  -0.1724]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.1188 -0.1478 -0.1547 ... -0.2387 -0.214  -0.3372]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 399, num timesteps 819200, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0354
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7314
         discriminator_loss   0.9253
                  gail_loss   0.9133
                  grad_loss   0.0120
                    ib_loss  -0.1968
                  task_loss   0.0316
                       beta   0.0000
             posterior_loss   0.5158
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.161  -0.2571 -0.0576 ... -0.1401 -0.0643 -0.5154]]
 discr_rewards:  [[-0.0717 -0.0411 -0.016  ... -0.0101 -0.0333 -0.062 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2722 -0.3377 -0.1131 ... -0.1897 -0.1372 -0.6169]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 400, num timesteps 821248, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0413
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7392
         discriminator_loss   0.9382
                  gail_loss   0.9260
                  grad_loss   0.0122
                    ib_loss  -0.1969
                  task_loss   0.0577
                       beta   0.0000
             posterior_loss   0.4959
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1256 -0.0644 -0.1404 ... -0.0547 -0.0603 -0.0696]]
 discr_rewards:  [[-0.0641 -0.0816 -0.0884 ... -0.0499 -0.0576 -0.2044]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.2292 -0.1856 -0.2683 ... -0.1441 -0.1575 -0.3136]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 401, num timesteps 823296, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0295
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7288
         discriminator_loss   0.9136
                  gail_loss   0.9027
                  grad_loss   0.0110
                    ib_loss  -0.1969
                  task_loss   0.0586
                       beta   0.0000
             posterior_loss   0.5101
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0548 -0.113  -0.1972 ... -0.0552 -0.0524 -0.1542]]
 discr_rewards:  [[-0.069  -0.1643 -0.0586 ... -0.1375 -0.02   -0.0893]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1633 -0.3168 -0.2953 ... -0.2322 -0.1119 -0.2831]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 402, num timesteps 825344, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0382
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.7425
         discriminator_loss   0.9273
                  gail_loss   0.9154
                  grad_loss   0.0119
                    ib_loss  -0.1968
                  task_loss   0.0420
                       beta   0.0000
             posterior_loss   0.5037
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1863 -0.0885 -0.0597 ... -0.0536 -0.0537 -0.1085]]
 discr_rewards:  [[-0.0896 -0.0907 -0.0674 ... -0.0114 -0.0723 -0.0234]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.3154 -0.2188 -0.1666 ... -0.1046 -0.1655 -0.1714]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 403, num timesteps 827392, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0342
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.7305
         discriminator_loss   0.9098
                  gail_loss   0.8989
                  grad_loss   0.0109
                    ib_loss  -0.1969
                  task_loss   0.0456
                       beta   0.0000
             posterior_loss   0.4804
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0566 -0.0712 -0.0544 ... -0.0976 -0.0524 -0.1026]]
 discr_rewards:  [[-0.065  -0.0641 -0.1478 ... -0.1565 -0.0368 -0.024 ]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.161  -0.1748 -0.2417 ... -0.2937 -0.1287 -0.1661]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 404, num timesteps 829440, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0258
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7213
         discriminator_loss   0.9140
                  gail_loss   0.9024
                  grad_loss   0.0115
                    ib_loss  -0.1969
                  task_loss   0.0362
                       beta   0.0000
             posterior_loss   0.5090
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0531 -0.2109 -0.169  ... -0.0547 -0.0654 -0.1219]]
 discr_rewards:  [[-0.0817 -0.1258 -0.136  ... -0.0819 -0.0568 -0.0228]]
 task_rewards:  [[ 3.7253e-09  3.7253e-09  3.7253e-09 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1743 -0.3762 -0.3445 ... -0.1761 -0.1617 -0.1842]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 405, num timesteps 831488, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0282
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7172
         discriminator_loss   0.9147
                  gail_loss   0.9047
                  grad_loss   0.0100
                    ib_loss  -0.1971
                  task_loss   0.0273
                       beta   0.0000
             posterior_loss   0.5069
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0542 -0.1158 -0.072  ... -0.0567 -0.0585 -0.208 ]]
 discr_rewards:  [[-0.1344 -0.099  -0.0413 ... -0.0058 -0.074  -0.1135]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2281 -0.2543 -0.1529 ... -0.102  -0.1719 -0.361 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 406, num timesteps 833536, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0246
                action_loss   0.0011
                    bc_loss   0.0000
               dist_entropy   0.7237
         discriminator_loss   0.9195
                  gail_loss   0.9086
                  grad_loss   0.0109
                    ib_loss  -0.1970
                  task_loss   0.0308
                       beta   0.0000
             posterior_loss   0.4926
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0856 -0.0662 -0.2086 ... -0.2244 -0.1188 -0.0557]]
 discr_rewards:  [[-0.0299 -0.0836 -0.1036 ... -0.1705 -0.0779 -0.1509]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09 -3.7253e-09 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.155  -0.1893 -0.3517 ... -0.4344 -0.2363 -0.2461]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 407, num timesteps 835584, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0306
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7260
         discriminator_loss   0.9169
                  gail_loss   0.9059
                  grad_loss   0.0109
                    ib_loss  -0.1969
                  task_loss   0.0273
                       beta   0.0000
             posterior_loss   0.5062
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0528 -0.069  -0.059  ... -0.2315 -0.2421 -0.0782]]
 discr_rewards:  [[-0.0855 -0.1191 -0.0307 ... -0.0053 -0.0904 -0.0494]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1778 -0.2275 -0.1292 ... -0.2763 -0.372  -0.167 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 408, num timesteps 837632, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0344
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.7367
         discriminator_loss   0.9197
                  gail_loss   0.9091
                  grad_loss   0.0105
                    ib_loss  -0.1970
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.5323
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1353 -0.0912 -0.0886 ... -0.0635 -0.118  -0.0533]]
 discr_rewards:  [[-0.0101 -0.0841 -0.0617 ... -0.0423 -0.0974 -0.1217]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1849 -0.2149 -0.1898 ... -0.1453 -0.2549 -0.2145]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 409, num timesteps 839680, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0269
                action_loss   0.0011
                    bc_loss   0.0000
               dist_entropy   0.7344
         discriminator_loss   0.9241
                  gail_loss   0.9132
                  grad_loss   0.0109
                    ib_loss  -0.1968
                  task_loss   0.0355
                       beta   0.0000
             posterior_loss   0.4882
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.1334 -0.0565 ... -0.054  -0.0698 -0.057 ]]
 discr_rewards:  [[-0.0401 -0.1297 -0.0884 ... -0.0536 -0.068  -0.0178]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1322 -0.3026 -0.1844 ... -0.1472 -0.1774 -0.1143]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 410, num timesteps 841728, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss   0.0011
                    bc_loss   0.0000
               dist_entropy   0.7355
         discriminator_loss   0.9332
                  gail_loss   0.9200
                  grad_loss   0.0132
                    ib_loss  -0.1970
                  task_loss   0.0293
                       beta   0.0000
             posterior_loss   0.4975
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0732 -0.0592 -0.0873 ... -0.0746 -0.0536 -0.2022]]
 discr_rewards:  [[-0.0025 -0.0062 -0.0822 ... -0.1075 -0.0248 -0.1584]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1152 -0.105  -0.209  ... -0.2217 -0.1179 -0.4001]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 411, num timesteps 843776, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0375
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7334
         discriminator_loss   0.9295
                  gail_loss   0.9181
                  grad_loss   0.0114
                    ib_loss  -0.1970
                  task_loss   0.0330
                       beta   0.0000
             posterior_loss   0.5093
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2094 -0.0527 -0.0815 ... -0.2468 -0.0528 -0.1087]]
 discr_rewards:  [[-0.0214 -0.0621 -0.0045 ... -0.1714 -0.0493 -0.0219]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2703 -0.1543 -0.1255 ... -0.4577 -0.1417 -0.1701]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 412, num timesteps 845824, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0327
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7201
         discriminator_loss   0.9250
                  gail_loss   0.9146
                  grad_loss   0.0104
                    ib_loss  -0.1966
                  task_loss   0.0446
                       beta   0.0000
             posterior_loss   0.5248
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0669 -0.1278 -0.0532 ... -0.1511 -0.0701 -0.0709]]
 discr_rewards:  [[-0.0311 -0.0423 -0.0695 ... -0.0889 -0.0742 -0.049 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1375 -0.2096 -0.1622 ... -0.2795 -0.1839 -0.1594]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 413, num timesteps 847872, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.7240
         discriminator_loss   0.9225
                  gail_loss   0.9111
                  grad_loss   0.0114
                    ib_loss  -0.1967
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.4767
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0674 -0.14   ... -0.1416 -0.0964 -0.2001]]
 discr_rewards:  [[-0.1429 -0.0305 -0.0842 ... -0.0412 -0.1856 -0.1035]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00 -3.7253e-09 ...  0.0000e+00 -3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2348 -0.1374 -0.2637 ... -0.2223 -0.3216 -0.3431]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 414, num timesteps 849920, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.7133
         discriminator_loss   0.9130
                  gail_loss   0.9007
                  grad_loss   0.0123
                    ib_loss  -0.1969
                  task_loss   0.0376
                       beta   0.0000
             posterior_loss   0.5302
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0552 -0.0696 -0.1122 ... -0.0692 -0.0795 -0.1164]]
 discr_rewards:  [[-0.0401 -0.0568 -0.0936 ... -0.0831 -0.0197 -0.0181]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1348 -0.1659 -0.2454 ... -0.1919 -0.1388 -0.174 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 415, num timesteps 851968, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0418
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7259
         discriminator_loss   0.9136
                  gail_loss   0.9030
                  grad_loss   0.0106
                    ib_loss  -0.1967
                  task_loss   0.0351
                       beta   0.0000
             posterior_loss   0.5024
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0534 -0.0719 -0.0916 ... -0.075  -0.1559 -0.0939]]
 discr_rewards:  [[-0.0335 -0.0389 -0.0784 ... -0.0181 -0.1194 -0.0807]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1264 -0.1504 -0.2096 ... -0.1325 -0.3148 -0.2141]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 416, num timesteps 854016, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0323
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.7296
         discriminator_loss   0.9359
                  gail_loss   0.9234
                  grad_loss   0.0124
                    ib_loss  -0.1966
                  task_loss   0.0493
                       beta   0.0000
             posterior_loss   0.4947
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.088  -0.0581 -0.0862 ... -0.0715 -0.059  -0.0548]]
 discr_rewards:  [[-0.0491 -0.1988 -0.0203 ... -0.0337 -0.0068 -0.0763]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1767 -0.2965 -0.146  ... -0.1447 -0.1053 -0.1706]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 417, num timesteps 856064, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0328
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7301
         discriminator_loss   0.9426
                  gail_loss   0.9319
                  grad_loss   0.0107
                    ib_loss  -0.1971
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.5274
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.059  -0.0535 -0.0534 ... -0.2386 -0.0587 -0.0562]]
 discr_rewards:  [[-0.0308 -0.1127 -0.0155 ... -0.1749 -0.0534 -0.0974]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1294 -0.2058 -0.1084 ... -0.453  -0.1517 -0.1931]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 418, num timesteps 858112, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0371
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7328
         discriminator_loss   0.9224
                  gail_loss   0.9118
                  grad_loss   0.0106
                    ib_loss  -0.1965
                  task_loss   0.0383
                       beta   0.0000
             posterior_loss   0.5307
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.065  -0.0936 -0.1442 ... -0.0965 -0.1045 -0.2677]]
 discr_rewards:  [[-0.1078 -0.0923 -0.0342 ... -0.0035 -0.0888 -0.0309]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2123 -0.2254 -0.2179 ... -0.1396 -0.2328 -0.3381]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 419, num timesteps 860160, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0272
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7365
         discriminator_loss   0.9159
                  gail_loss   0.9058
                  grad_loss   0.0101
                    ib_loss  -0.1966
                  task_loss   0.0401
                       beta   0.0000
             posterior_loss   0.4968
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0542 -0.0541 ... -0.0547 -0.179  -0.0593]]
 discr_rewards:  [[-0.0894 -0.0612 -0.06   ... -0.0686 -0.111  -0.1125]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.1814 -0.1549 -0.1536 ... -0.1629 -0.3295 -0.2113]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 420, num timesteps 862208, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0369
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.7432
         discriminator_loss   0.9335
                  gail_loss   0.9213
                  grad_loss   0.0122
                    ib_loss  -0.1968
                  task_loss   0.0366
                       beta   0.0000
             posterior_loss   0.4910
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0572 -0.1835 ... -0.0586 -0.2217 -0.1422]]
 discr_rewards:  [[-0.0158 -0.0156 -0.037  ... -0.072  -0.0528 -0.0889]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1078 -0.1124 -0.2601 ... -0.1701 -0.314  -0.2706]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 421, num timesteps 864256, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0277
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.7289
         discriminator_loss   0.9205
                  gail_loss   0.9094
                  grad_loss   0.0111
                    ib_loss  -0.1970
                  task_loss   0.0432
                       beta   0.0000
             posterior_loss   0.5200
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0998 -0.0586 -0.0606 ... -0.0644 -0.1469 -0.0525]]
 discr_rewards:  [[-0.0451 -0.0193 -0.0481 ...  0.0086 -0.06   -0.0062]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1844 -0.1175 -0.1483 ... -0.0953 -0.2464 -0.0983]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 422, num timesteps 866304, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0334
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7372
         discriminator_loss   0.9333
                  gail_loss   0.9213
                  grad_loss   0.0119
                    ib_loss  -0.1968
                  task_loss   0.0356
                       beta   0.0000
             posterior_loss   0.5045
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0634 -0.0549 -0.1176 ... -0.0799 -0.1034 -0.0531]]
 discr_rewards:  [[-0.0313 -0.0133 -0.0187 ... -0.0591 -0.1786 -0.0748]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1343 -0.1077 -0.1758 ... -0.1785 -0.3215 -0.1673]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 423, num timesteps 868352, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0327
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7429
         discriminator_loss   0.9251
                  gail_loss   0.9147
                  grad_loss   0.0104
                    ib_loss  -0.1967
                  task_loss   0.0380
                       beta   0.0000
             posterior_loss   0.5092
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0633 -0.0611 -0.0589 ... -0.07   -0.2587 -0.0689]]
 discr_rewards:  [[-0.0335 -0.0732 -0.1414 ... -0.2252 -0.0217 -0.0462]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1363 -0.1739 -0.2398 ... -0.3347 -0.3199 -0.1546]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 424, num timesteps 870400, FPS 163 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0279
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.7303
         discriminator_loss   0.9218
                  gail_loss   0.9103
                  grad_loss   0.0115
                    ib_loss  -0.1968
                  task_loss   0.0423
                       beta   0.0000
             posterior_loss   0.4842
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1494 -0.0534 -0.1423 ... -0.1218 -0.0807 -0.0543]]
 discr_rewards:  [[-0.1829 -0.0354 -0.0992 ... -0.1283 -0.0239 -0.0886]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  3.7253e-09 ...  3.7253e-09  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.3719 -0.1284 -0.281  ... -0.2896 -0.1441 -0.1824]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 425, num timesteps 872448, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0313
                action_loss   0.0014
                    bc_loss   0.0000
               dist_entropy   0.7302
         discriminator_loss   0.9254
                  gail_loss   0.9150
                  grad_loss   0.0104
                    ib_loss  -0.1967
                  task_loss   0.0355
                       beta   0.0000
             posterior_loss   0.4939
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.4617 -0.1165 -0.1296 ... -0.0541 -0.1454 -0.0769]]
 discr_rewards:  [[-0.0241 -0.0898 -0.0766 ... -0.0759 -0.0129 -0.1283]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.5253 -0.2459 -0.2457 ... -0.1695 -0.1979 -0.2447]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 426, num timesteps 874496, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0355
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.7355
         discriminator_loss   0.9291
                  gail_loss   0.9190
                  grad_loss   0.0101
                    ib_loss  -0.1969
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.5209
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0681 -0.0543 -0.0931 ... -0.0684 -0.3409 -0.1328]]
 discr_rewards:  [[-0.0983 -0.0081 -0.0041 ... -0.0238 -0.1775 -0.0281]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.206  -0.1019 -0.1367 ... -0.1318 -0.5579 -0.2005]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 427, num timesteps 876544, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0265
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.7424
         discriminator_loss   0.9342
                  gail_loss   0.9246
                  grad_loss   0.0096
                    ib_loss  -0.1969
                  task_loss   0.0408
                       beta   0.0000
             posterior_loss   0.5008
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0526 -0.0546 -0.0857 ... -0.196  -0.0846 -0.0693]]
 discr_rewards:  [[-0.0125 -0.1048 -0.009  ... -0.0698 -0.0284 -0.1671]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1046 -0.199  -0.1343 ... -0.3053 -0.1525 -0.2759]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 428, num timesteps 878592, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss   0.0018
                    bc_loss   0.0000
               dist_entropy   0.7346
         discriminator_loss   0.9108
                  gail_loss   0.9005
                  grad_loss   0.0103
                    ib_loss  -0.1971
                  task_loss   0.0376
                       beta   0.0000
             posterior_loss   0.5117
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0669 -0.053  -0.0608 ... -0.1482 -0.053  -0.1636]]
 discr_rewards:  [[-0.0187 -0.1192 -0.0446 ... -0.0514 -0.0575 -0.0864]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1252 -0.2117 -0.145  ... -0.2391 -0.15   -0.2895]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 429, num timesteps 880640, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0342
                action_loss   0.0033
                    bc_loss   0.0000
               dist_entropy   0.7330
         discriminator_loss   0.9230
                  gail_loss   0.9109
                  grad_loss   0.0121
                    ib_loss  -0.1967
                  task_loss   0.0336
                       beta   0.0000
             posterior_loss   0.4979
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0535 -0.0845 -0.0574 ... -0.0799 -0.1161 -0.1338]]
 discr_rewards:  [[-0.0521 -0.0219 -0.0215 ... -0.017  -0.0134 -0.0973]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.1451 -0.1459 -0.1185 ... -0.1364 -0.169  -0.2706]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 430, num timesteps 882688, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7312
         discriminator_loss   0.9266
                  gail_loss   0.9157
                  grad_loss   0.0108
                    ib_loss  -0.1968
                  task_loss   0.0394
                       beta   0.0000
             posterior_loss   0.4749
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1529 -0.0631 -0.0776 ... -0.0679 -0.0586 -0.0809]]
 discr_rewards:  [[-0.0334 -0.0091 -0.0925 ... -0.0168 -0.0955 -0.0126]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2258 -0.1117 -0.2096 ... -0.1242 -0.1936 -0.133 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 431, num timesteps 884736, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss   0.0022
                    bc_loss   0.0000
               dist_entropy   0.7333
         discriminator_loss   0.9156
                  gail_loss   0.9054
                  grad_loss   0.0101
                    ib_loss  -0.1972
                  task_loss   0.0383
                       beta   0.0000
             posterior_loss   0.5026
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0786 -0.1726 -0.0563 ... -0.1127 -0.0952 -0.0743]]
 discr_rewards:  [[-0.1167 -0.0455 -0.0351 ... -0.0006 -0.0229 -0.0774]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.2349 -0.2576 -0.131  ... -0.1528 -0.1576 -0.1912]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 432, num timesteps 886784, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0301
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7428
         discriminator_loss   0.9239
                  gail_loss   0.9120
                  grad_loss   0.0119
                    ib_loss  -0.1970
                  task_loss   0.0394
                       beta   0.0000
             posterior_loss   0.5139
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0618 -0.2925 -0.0606 ... -0.105  -0.1216 -0.1255]]
 discr_rewards:  [[-0.122  -0.0199 -0.016  ... -0.047  -0.1039 -0.1041]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2234 -0.3519 -0.1161 ... -0.1915 -0.265  -0.2692]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 433, num timesteps 888832, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.7477
         discriminator_loss   0.9225
                  gail_loss   0.9107
                  grad_loss   0.0118
                    ib_loss  -0.1970
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.4888
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1052 -0.1937 -0.0627 ... -0.0528 -0.0706 -0.0986]]
 discr_rewards:  [[-0.049  -0.0571 -0.1218 ... -0.0172 -0.0508 -0.048 ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1937 -0.2903 -0.224  ... -0.1095 -0.1609 -0.1862]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 434, num timesteps 890880, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0256
                action_loss   0.0014
                    bc_loss   0.0000
               dist_entropy   0.7324
         discriminator_loss   0.9319
                  gail_loss   0.9200
                  grad_loss   0.0118
                    ib_loss  -0.1971
                  task_loss   0.0389
                       beta   0.0000
             posterior_loss   0.4824
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2914 -0.053  -0.0593 ... -0.0558 -0.0957 -0.0531]]
 discr_rewards:  [[-0.057  -0.0381 -0.0583 ... -0.1806 -0.0212 -0.1466]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.3879 -0.1306 -0.1571 ... -0.2759 -0.1564 -0.2392]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 435, num timesteps 892928, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0322
                action_loss   0.0030
                    bc_loss   0.0000
               dist_entropy   0.7464
         discriminator_loss   0.9300
                  gail_loss   0.9185
                  grad_loss   0.0114
                    ib_loss  -0.1970
                  task_loss   0.0302
                       beta   0.0000
             posterior_loss   0.5319
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0608 -0.0547 -0.0558 ... -0.0526 -0.0683 -0.0541]]
 discr_rewards:  [[-0.1121 -0.0695 -0.031  ... -0.0645 -0.0061 -0.0311]]
 task_rewards:  [[ 3.7253e-09 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2123 -0.1637 -0.1263 ... -0.1566 -0.1139 -0.1247]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 436, num timesteps 894976, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0307
                action_loss   0.0009
                    bc_loss   0.0000
               dist_entropy   0.7394
         discriminator_loss   0.9190
                  gail_loss   0.9082
                  grad_loss   0.0108
                    ib_loss  -0.1969
                  task_loss   0.0379
                       beta   0.0000
             posterior_loss   0.4887
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0562 -0.0662 ... -0.0901 -0.0613 -0.0532]]
 discr_rewards:  [[-0.0956 -0.1068 -0.0403 ... -0.0248 -0.0988 -0.047 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1875 -0.2025 -0.1461 ... -0.1545 -0.1996 -0.1397]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 437, num timesteps 897024, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0282
                action_loss   0.0009
                    bc_loss   0.0000
               dist_entropy   0.7328
         discriminator_loss   0.9140
                  gail_loss   0.9033
                  grad_loss   0.0108
                    ib_loss  -0.1969
                  task_loss   0.0326
                       beta   0.0000
             posterior_loss   0.4879
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0567 -0.0868 -0.191  ... -0.065  -0.1931 -0.1256]]
 discr_rewards:  [[-0.0265 -0.1362 -0.0943 ... -0.0389 -0.0547 -0.0638]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.1228 -0.2625 -0.3249 ... -0.1435 -0.2873 -0.229 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 438, num timesteps 899072, FPS 162 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0315
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.7449
         discriminator_loss   0.9205
                  gail_loss   0.9099
                  grad_loss   0.0106
                    ib_loss  -0.1970
                  task_loss   0.0309
                       beta   0.0000
             posterior_loss   0.4766
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.059  -0.0892 ... -0.1012 -0.1467 -0.0792]]
 discr_rewards:  [[-0.0778 -0.1191 -0.0196 ... -0.0362 -0.0595 -0.0722]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   3.7253e-09]]
 final_rewards:  [[-0.1697 -0.2176 -0.1483 ... -0.177  -0.2457 -0.1908]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 439, num timesteps 901120, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0234
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7346
         discriminator_loss   0.9157
                  gail_loss   0.9051
                  grad_loss   0.0106
                    ib_loss  -0.1967
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.5013
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0527 -0.0715 -0.356  ... -0.0581 -0.1325 -0.0748]]
 discr_rewards:  [[-0.0383 -0.0718 -0.0131 ... -0.0632 -0.0753 -0.0762]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1305 -0.1829 -0.4086 ... -0.1608 -0.2473 -0.1905]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 440, num timesteps 903168, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0489
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7466
         discriminator_loss   0.9340
                  gail_loss   0.9205
                  grad_loss   0.0135
                    ib_loss  -0.1968
                  task_loss   0.0544
                       beta   0.0000
             posterior_loss   0.5220
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0551 -0.1271 -0.0539 ... -0.1121 -0.431  -0.0683]]
 discr_rewards:  [[-0.1049 -0.0044 -0.0542 ... -0.0263 -0.0553 -0.0796]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1996 -0.171  -0.1477 ... -0.1779 -0.5258 -0.1874]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 441, num timesteps 905216, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0386
                action_loss   0.0009
                    bc_loss   0.0000
               dist_entropy   0.7390
         discriminator_loss   0.9208
                  gail_loss   0.9100
                  grad_loss   0.0108
                    ib_loss  -0.1970
                  task_loss   0.0533
                       beta   0.0000
             posterior_loss   0.5285
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1063 -0.0839 -0.0538 ... -0.0524 -0.0596 -0.0558]]
 discr_rewards:  [[-0.0283 -0.0993 -0.0068 ... -0.0491 -0.0501 -0.1537]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1742 -0.2228 -0.1001 ... -0.141  -0.1492 -0.249 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 442, num timesteps 907264, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0302
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.7326
         discriminator_loss   0.9227
                  gail_loss   0.9124
                  grad_loss   0.0103
                    ib_loss  -0.1967
                  task_loss   0.0442
                       beta   0.0000
             posterior_loss   0.5126
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0731 -0.1138 -0.0528 ... -0.0849 -0.0844 -0.2053]]
 discr_rewards:  [[-0.0184 -0.0373 -0.0836 ... -0.0373 -0.0132 -0.0695]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.131  -0.1906 -0.1759 ... -0.1617 -0.1371 -0.3143]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 443, num timesteps 909312, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0385
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.7290
         discriminator_loss   0.9257
                  gail_loss   0.9153
                  grad_loss   0.0105
                    ib_loss  -0.1969
                  task_loss   0.0318
                       beta   0.0000
             posterior_loss   0.5107
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0535 -0.1323 -0.1052 ... -0.0787 -0.0582 -0.0715]]
 discr_rewards:  [[-0.0286 -0.1327  0.0014 ... -0.136  -0.0749 -0.0681]]
 task_rewards:  [[ 0.0000e+00 -7.4506e-09  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1215 -0.3046 -0.1433 ... -0.2543 -0.1727 -0.1791]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 444, num timesteps 911360, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0361
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7339
         discriminator_loss   0.9170
                  gail_loss   0.9061
                  grad_loss   0.0109
                    ib_loss  -0.1969
                  task_loss   0.0407
                       beta   0.0000
             posterior_loss   0.5023
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0548 -0.1065 -0.2056 ... -0.3221 -0.0661 -0.0678]]
 discr_rewards:  [[-0.0264 -0.1319 -0.0094 ... -0.0299 -0.0439 -0.1357]]
 task_rewards:  [[ 0.0000e+00  7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -7.4506e-09]]
 final_rewards:  [[-0.1207 -0.2779 -0.2545 ... -0.3915 -0.1496 -0.243 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 445, num timesteps 913408, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0295
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7270
         discriminator_loss   0.9299
                  gail_loss   0.9187
                  grad_loss   0.0112
                    ib_loss  -0.1971
                  task_loss   0.0396
                       beta   0.0000
             posterior_loss   0.5014
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0551 -0.0554 -0.1138 ... -0.0637 -0.346  -0.1752]]
 discr_rewards:  [[-0.0818 -0.0163 -0.0105 ... -0.0797 -0.0329 -0.0266]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1765 -0.1112 -0.1638 ... -0.183  -0.4184 -0.2413]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 446, num timesteps 915456, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0282
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.7158
         discriminator_loss   0.9203
                  gail_loss   0.9105
                  grad_loss   0.0098
                    ib_loss  -0.1970
                  task_loss   0.0327
                       beta   0.0000
             posterior_loss   0.5266
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0792 -0.1183 -0.2171 ... -0.0865 -0.0899 -0.0578]]
 discr_rewards:  [[-0.0219 -0.0197 -0.0421 ... -0.0062 -0.1366 -0.0376]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 7.4506e-09 0.0000e+00]]
 final_rewards:  [[-0.1406 -0.1776 -0.2987 ... -0.1322 -0.266  -0.135 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 447, num timesteps 917504, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0260
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.7124
         discriminator_loss   0.9143
                  gail_loss   0.9029
                  grad_loss   0.0114
                    ib_loss  -0.1970
                  task_loss   0.0315
                       beta   0.0000
             posterior_loss   0.4971
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0593 -0.1043 -0.1213 ... -0.0527 -0.1068 -0.1662]]
 discr_rewards:  [[-0.0342 -0.0551 -0.0736 ... -0.115  -0.021  -0.0864]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.133  -0.1989 -0.2344 ... -0.2072 -0.1673 -0.2922]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 448, num timesteps 919552, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0326
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.7155
         discriminator_loss   0.9244
                  gail_loss   0.9142
                  grad_loss   0.0102
                    ib_loss  -0.1966
                  task_loss   0.0288
                       beta   0.0000
             posterior_loss   0.5285
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1525 -0.1067 -0.1105 ... -0.0708 -0.0526 -0.1474]]
 discr_rewards:  [[-0.0771 -0.0299  0.0008 ... -0.0726 -0.0508 -0.0828]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2692 -0.1761 -0.1492 ... -0.1829 -0.1429 -0.2697]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 449, num timesteps 921600, FPS 161 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0377
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7119
         discriminator_loss   0.9219
                  gail_loss   0.9118
                  grad_loss   0.0101
                    ib_loss  -0.1971
                  task_loss   0.0343
                       beta   0.0000
             posterior_loss   0.5173
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0561 -0.0565 -0.0723 ... -0.0899 -0.0539 -0.054 ]]
 discr_rewards:  [[-0.0473 -0.1958 -0.0181 ... -0.0357 -0.0288 -0.0241]]
 task_rewards:  [[0.0000e+00 7.4506e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1429 -0.2918 -0.1299 ... -0.1651 -0.1223 -0.1176]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 450, num timesteps 923648, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0335
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.7144
         discriminator_loss   0.9158
                  gail_loss   0.9055
                  grad_loss   0.0102
                    ib_loss  -0.1969
                  task_loss   0.0410
                       beta   0.0000
             posterior_loss   0.5069
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0788 -0.0983 -0.0909 ... -0.0529 -0.1582 -0.0644]]
 discr_rewards:  [[-0.0621 -0.0424 -0.0872 ... -0.0441 -0.0517 -0.0517]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1804 -0.1803 -0.2176 ... -0.1365 -0.2495 -0.1556]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 451, num timesteps 925696, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0349
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7196
         discriminator_loss   0.9248
                  gail_loss   0.9142
                  grad_loss   0.0106
                    ib_loss  -0.1971
                  task_loss   0.0425
                       beta   0.0000
             posterior_loss   0.4997
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0763 -0.0526 -0.0577 ... -0.0652 -0.2689 -0.1356]]
 discr_rewards:  [[-0.1213 -0.0354 -0.0071 ... -0.0513 -0.1054 -0.1115]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.2371 -0.1275 -0.1043 ... -0.156  -0.4138 -0.2866]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 452, num timesteps 927744, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0333
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7185
         discriminator_loss   0.9189
                  gail_loss   0.9085
                  grad_loss   0.0104
                    ib_loss  -0.1969
                  task_loss   0.0387
                       beta   0.0000
             posterior_loss   0.4935
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2324 -0.0776 -0.0816 ... -0.0614 -0.0558 -0.1334]]
 discr_rewards:  [[-0.03   -0.0843 -0.0885 ... -0.0277 -0.0072 -0.0985]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.3019 -0.2015 -0.2097 ... -0.1286 -0.1025 -0.2714]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 453, num timesteps 929792, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0257
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7182
         discriminator_loss   0.9144
                  gail_loss   0.9046
                  grad_loss   0.0098
                    ib_loss  -0.1969
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.4759
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0763 -0.1123 -0.161  ... -0.0532 -0.1474 -0.0904]]
 discr_rewards:  [[-0.0516 -0.0006 -0.0643 ... -0.0918 -0.0378 -0.0469]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1674 -0.1525 -0.2648 ... -0.1845 -0.2247 -0.1768]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 454, num timesteps 931840, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0336
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.7263
         discriminator_loss   0.9197
                  gail_loss   0.9094
                  grad_loss   0.0103
                    ib_loss  -0.1969
                  task_loss   0.0285
                       beta   0.0000
             posterior_loss   0.4965
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0644 -0.0533 -0.0528 ... -0.1241 -0.1222 -0.0918]]
 discr_rewards:  [[-0.0556 -0.1649 -0.0525 ... -0.0095 -0.0529 -0.0623]]
 task_rewards:  [[ 0.0000e+00 -7.4506e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1595 -0.2577 -0.1448 ... -0.1731 -0.2147 -0.1936]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 455, num timesteps 933888, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0379
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7221
         discriminator_loss   0.9163
                  gail_loss   0.9060
                  grad_loss   0.0102
                    ib_loss  -0.1969
                  task_loss   0.0394
                       beta   0.0000
             posterior_loss   0.5163
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1718 -0.0745 -0.1665 ... -0.0703 -0.1133 -0.1038]]
 discr_rewards:  [[-0.0555 -0.0629 -0.0648 ... -0.0205 -0.0835 -0.0553]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  3.7253e-09 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2669 -0.1769 -0.2708 ... -0.1303 -0.2363 -0.1987]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 456, num timesteps 935936, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0321
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7298
         discriminator_loss   0.9192
                  gail_loss   0.9088
                  grad_loss   0.0105
                    ib_loss  -0.1968
                  task_loss   0.0418
                       beta   0.0000
             posterior_loss   0.4734
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.053  -0.0553 -0.0524 ... -0.0524 -0.2505 -0.0621]]
 discr_rewards:  [[-0.0277 -0.0288 -0.1475 ... -0.0712 -0.0329  0.0074]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -7.4506e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1203 -0.1237 -0.2394 ... -0.1631 -0.323  -0.0942]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 457, num timesteps 937984, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0332
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.7270
         discriminator_loss   0.9209
                  gail_loss   0.9116
                  grad_loss   0.0093
                    ib_loss  -0.1967
                  task_loss   0.0336
                       beta   0.0000
             posterior_loss   0.5079
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0584 -0.0525 -0.2047 ... -0.0559 -0.053  -0.0538]]
 discr_rewards:  [[-0.0799 -0.077  -0.0553 ... -0.1325 -0.1041 -0.0362]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1779 -0.1691 -0.2995 ... -0.228  -0.1966 -0.1295]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 458, num timesteps 940032, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.7313
         discriminator_loss   0.9268
                  gail_loss   0.9165
                  grad_loss   0.0104
                    ib_loss  -0.1969
                  task_loss   0.0387
                       beta   0.0000
             posterior_loss   0.4815
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0696 -0.1486 -0.1292 ... -0.0548 -0.0524 -0.0807]]
 discr_rewards:  [[-0.1617 -0.0352 -0.0266 ... -0.093  -0.0826 -0.0495]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ... -3.7253e-09  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2708 -0.2233 -0.1954 ... -0.1873 -0.1746 -0.1698]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 459, num timesteps 942080, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7205
         discriminator_loss   0.9248
                  gail_loss   0.9152
                  grad_loss   0.0096
                    ib_loss  -0.1971
                  task_loss   0.0351
                       beta   0.0000
             posterior_loss   0.5083
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0601 -0.1497 ... -0.0579 -0.2595 -0.0736]]
 discr_rewards:  [[-0.0341 -0.0153 -0.0303 ... -0.0035 -0.0617 -0.0827]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1261 -0.1149 -0.2196 ... -0.1009 -0.3607 -0.1958]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 460, num timesteps 944128, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0279
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7261
         discriminator_loss   0.9172
                  gail_loss   0.9076
                  grad_loss   0.0097
                    ib_loss  -0.1969
                  task_loss   0.0333
                       beta   0.0000
             posterior_loss   0.4936
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1351 -0.0706 -0.0528 ... -0.0744 -0.0831 -0.0524]]
 discr_rewards:  [[-0.1197 -0.1483 -0.0694 ... -0.0664 -0.017  -0.0454]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2943 -0.2584 -0.1618 ... -0.1803 -0.1397 -0.1373]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 461, num timesteps 946176, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0369
                action_loss   0.0022
                    bc_loss   0.0000
               dist_entropy   0.7289
         discriminator_loss   0.9334
                  gail_loss   0.9230
                  grad_loss   0.0104
                    ib_loss  -0.1967
                  task_loss   0.0311
                       beta   0.0000
             posterior_loss   0.5060
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1464 -0.096  -0.0524 ... -0.1282 -0.0532 -0.0695]]
 discr_rewards:  [[-0.0224 -0.0363 -0.0305 ... -0.0434 -0.1085 -0.0361]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2083 -0.1719 -0.1223 ... -0.2112 -0.2012 -0.1451]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 462, num timesteps 948224, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0293
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.7269
         discriminator_loss   0.9222
                  gail_loss   0.9124
                  grad_loss   0.0098
                    ib_loss  -0.1966
                  task_loss   0.0392
                       beta   0.0000
             posterior_loss   0.4963
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0677 -0.1294 -0.3158 ... -0.0525 -0.0606 -0.0853]]
 discr_rewards:  [[-0.053  -0.0641 -0.0656 ... -0.0723 -0.0907 -0.128 ]]
 task_rewards:  [[ 0.0000e+00  3.7253e-09  3.7253e-09 ...  3.7253e-09 -3.7253e-09
  -7.4506e-09]]
 final_rewards:  [[-0.1602 -0.233  -0.4209 ... -0.1643 -0.1909 -0.2529]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 463, num timesteps 950272, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0315
                action_loss   0.0013
                    bc_loss   0.0000
               dist_entropy   0.7184
         discriminator_loss   0.9034
                  gail_loss   0.8930
                  grad_loss   0.0103
                    ib_loss  -0.1967
                  task_loss   0.0317
                       beta   0.0000
             posterior_loss   0.5262
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0622 -0.2181 -0.1548 ... -0.1317 -0.0542 -0.2683]]
 discr_rewards:  [[-0.0555 -0.065  -0.0821 ... -0.0065 -0.0834 -0.0151]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1571 -0.3227 -0.2764 ... -0.1777 -0.1772 -0.3229]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 464, num timesteps 952320, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0342
                action_loss  -0.0016
                    bc_loss   0.0000
               dist_entropy   0.7227
         discriminator_loss   0.9235
                  gail_loss   0.9117
                  grad_loss   0.0118
                    ib_loss  -0.1965
                  task_loss   0.0329
                       beta   0.0000
             posterior_loss   0.4887
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0524 -0.0994 -0.0526 ... -0.3238 -0.0888 -0.0625]]
 discr_rewards:  [[-0.0193 -0.0509 -0.0371 ... -0.0418 -0.0251 -0.0128]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1112 -0.1899 -0.1292 ... -0.4052 -0.1535 -0.1148]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 465, num timesteps 954368, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0273
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.7185
         discriminator_loss   0.9179
                  gail_loss   0.9070
                  grad_loss   0.0109
                    ib_loss  -0.1972
                  task_loss   0.0383
                       beta   0.0000
             posterior_loss   0.4922
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0539 -0.0605 -0.0612 ... -0.0614 -0.0547 -0.085 ]]
 discr_rewards:  [[-0.0166 -0.067  -0.0845 ... -0.0674 -0.0503 -0.0624]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.11   -0.1671 -0.1852 ... -0.1684 -0.1445 -0.1869]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 466, num timesteps 956416, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0300
                action_loss   0.0008
                    bc_loss   0.0000
               dist_entropy   0.7260
         discriminator_loss   0.9256
                  gail_loss   0.9141
                  grad_loss   0.0115
                    ib_loss  -0.1969
                  task_loss   0.0323
                       beta   0.0000
             posterior_loss   0.5200
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0603 -0.0559 -0.0695 ... -0.065  -0.0596 -0.381 ]]
 discr_rewards:  [[-0.0724 -0.0943 -0.2221 ... -0.1133 -0.1066 -0.0416]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1722 -0.1897 -0.3311 ... -0.2178 -0.2057 -0.4621]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 467, num timesteps 958464, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0319
                action_loss   0.0039
                    bc_loss   0.0000
               dist_entropy   0.7220
         discriminator_loss   0.9155
                  gail_loss   0.9049
                  grad_loss   0.0106
                    ib_loss  -0.1971
                  task_loss   0.0386
                       beta   0.0000
             posterior_loss   0.4972
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2736 -0.0556 -0.0634 ... -0.0885 -0.06   -0.0717]]
 discr_rewards:  [[-0.0575 -0.0973 -0.05   ... -0.0222 -0.0945 -0.0256]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.3706 -0.1924 -0.1529 ... -0.1502 -0.194  -0.1369]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 468, num timesteps 960512, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0304
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7192
         discriminator_loss   0.9158
                  gail_loss   0.9060
                  grad_loss   0.0098
                    ib_loss  -0.1969
                  task_loss   0.0351
                       beta   0.0000
             posterior_loss   0.5106
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0533 -0.0524 -0.0525 ... -0.0588 -0.1069 -0.058 ]]
 discr_rewards:  [[-0.0144 -0.0714 -0.0855 ... -0.1799 -0.0323 -0.1652]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 7.4506e-09 0.0000e+00 7.4506e-09]]
 final_rewards:  [[-0.1072 -0.1633 -0.1776 ... -0.2781 -0.1787 -0.2627]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 469, num timesteps 962560, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0307
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7211
         discriminator_loss   0.9165
                  gail_loss   0.9056
                  grad_loss   0.0109
                    ib_loss  -0.1971
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.4848
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0544 -0.0783 -0.1294 ... -0.1021 -0.0875 -0.0778]]
 discr_rewards:  [[-0.0746 -0.0621 -0.0141 ... -0.0512 -0.1613 -0.0189]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 7.4506e-09 0.0000e+00]]
 final_rewards:  [[-0.1685 -0.18   -0.183  ... -0.1929 -0.2883 -0.1362]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 470, num timesteps 964608, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1031
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7269
         discriminator_loss   0.9264
                  gail_loss   0.9160
                  grad_loss   0.0104
                    ib_loss  -0.1967
                  task_loss   0.1093
                       beta   0.0000
             posterior_loss   0.5427
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2014 -0.0785 -0.0525 ... -0.1692 -0.1865 -0.2486]]
 discr_rewards:  [[-0.0887 -0.0212 -0.0355 ... -0.056  -0.0624 -0.0197]]
 task_rewards:  [[-3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.3297 -0.1393 -0.1275 ... -0.2648 -0.2884 -0.3078]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 471, num timesteps 966656, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0389
                action_loss   0.0021
                    bc_loss   0.0000
               dist_entropy   0.7308
         discriminator_loss   0.9288
                  gail_loss   0.9195
                  grad_loss   0.0092
                    ib_loss  -0.1969
                  task_loss   0.2406
                       beta   0.0000
             posterior_loss   0.4929
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0585 -0.1067 -0.0989 ... -0.0907 -0.2154 -0.1989]]
 discr_rewards:  [[-0.0821 -0.0081 -0.0858 ... -0.0847 -0.0243 -0.0054]]
 task_rewards:  [[0.0000e+00 0.0000e+00 3.7253e-09 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1801 -0.1543 -0.2242 ... -0.215  -0.2792 -0.2438]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 472, num timesteps 968704, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0332
                action_loss   0.0011
                    bc_loss   0.0000
               dist_entropy   0.7200
         discriminator_loss   0.9193
                  gail_loss   0.9083
                  grad_loss   0.0110
                    ib_loss  -0.1970
                  task_loss   0.0481
                       beta   0.0000
             posterior_loss   0.4828
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.0777 -0.1525 ... -0.0932 -0.1139 -0.0632]]
 discr_rewards:  [[-0.0699 -0.1003 -0.0846 ...  0.0026 -0.0254 -0.0206]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1619 -0.2176 -0.2767 ... -0.1302 -0.1788 -0.1234]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 473, num timesteps 970752, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0308
                action_loss   0.0022
                    bc_loss   0.0000
               dist_entropy   0.7249
         discriminator_loss   0.9154
                  gail_loss   0.9051
                  grad_loss   0.0102
                    ib_loss  -0.1968
                  task_loss   0.0365
                       beta   0.0000
             posterior_loss   0.5199
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0852 -0.0653 -0.0581 ... -0.0757 -0.1328 -0.0526]]
 discr_rewards:  [[-0.1038 -0.0434 -0.1307 ... -0.0165 -0.0846 -0.1011]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.2285 -0.1482 -0.2284 ... -0.1317 -0.2569 -0.1932]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 474, num timesteps 972800, FPS 160 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0290
                action_loss   0.0023
                    bc_loss   0.0000
               dist_entropy   0.7248
         discriminator_loss   0.9060
                  gail_loss   0.8963
                  grad_loss   0.0097
                    ib_loss  -0.1964
                  task_loss   0.0369
                       beta   0.0000
             posterior_loss   0.4728
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1294 -0.0647 -0.1095 ... -0.0553 -0.1659 -0.1088]]
 discr_rewards:  [[-0.0562 -0.0373 -0.0354 ... -0.0789 -0.0238 -0.0577]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.2251 -0.1416 -0.1844 ... -0.1737 -0.2292 -0.2061]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 475, num timesteps 974848, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0297
                action_loss   0.0023
                    bc_loss   0.0000
               dist_entropy   0.7205
         discriminator_loss   0.9176
                  gail_loss   0.9080
                  grad_loss   0.0096
                    ib_loss  -0.1967
                  task_loss   0.0320
                       beta   0.0000
             posterior_loss   0.5072
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.053  -0.0761 -0.094  ... -0.0741 -0.0667 -0.0558]]
 discr_rewards:  [[-0.1239 -0.1138 -0.0573 ... -0.0338 -0.0526 -0.0357]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2164 -0.2294 -0.1908 ... -0.1473 -0.1588 -0.131 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 476, num timesteps 976896, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0292
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.7243
         discriminator_loss   0.9287
                  gail_loss   0.9187
                  grad_loss   0.0100
                    ib_loss  -0.1967
                  task_loss   0.0326
                       beta   0.0000
             posterior_loss   0.4785
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1197 -0.2869 -0.0524 ... -0.0563 -0.0847 -0.0709]]
 discr_rewards:  [[-0.0713 -0.0461 -0.0679 ... -0.0776 -0.016  -0.0257]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2305 -0.3726 -0.1598 ... -0.1734 -0.1401 -0.1361]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 477, num timesteps 978944, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0337
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.7336
         discriminator_loss   0.9247
                  gail_loss   0.9142
                  grad_loss   0.0105
                    ib_loss  -0.1967
                  task_loss   0.0422
                       beta   0.0000
             posterior_loss   0.5281
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1351 -0.0535 -0.0535 ... -0.0641 -0.1191 -0.0534]]
 discr_rewards:  [[-6.6249e-02 -1.6160e-01 -8.1569e-02 ... -1.1067e-04 -2.4036e-02
  -1.2639e-01]]
 task_rewards:  [[ 3.7253e-09 -7.4506e-09 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2409 -0.2546 -0.1746 ... -0.1037 -0.1827 -0.2193]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 478, num timesteps 980992, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0254
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7234
         discriminator_loss   0.9206
                  gail_loss   0.9115
                  grad_loss   0.0091
                    ib_loss  -0.1972
                  task_loss   0.0390
                       beta   0.0000
             posterior_loss   0.4922
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1064 -0.0912 -0.0708 ... -0.0754 -0.0617 -0.0688]]
 discr_rewards:  [[-0.2222 -0.0759 -0.036  ... -0.0518 -0.0142 -0.1106]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.3681 -0.2067 -0.1463 ... -0.1667 -0.1154 -0.2189]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 479, num timesteps 983040, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0461
                action_loss   0.0021
                    bc_loss   0.0000
               dist_entropy   0.7449
         discriminator_loss   0.9290
                  gail_loss   0.9198
                  grad_loss   0.0092
                    ib_loss  -0.1971
                  task_loss   0.0420
                       beta   0.0000
             posterior_loss   0.4924
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.177  -0.1592 -0.06   ... -0.1282 -0.0902 -0.1879]]
 discr_rewards:  [[-0.0038 -0.0027 -0.0202 ... -0.0248 -0.146  -0.1454]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
  -3.7253e-09]]
 final_rewards:  [[-0.2204 -0.2014 -0.1197 ... -0.1925 -0.2757 -0.3728]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 480, num timesteps 985088, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0294
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7262
         discriminator_loss   0.9183
                  gail_loss   0.9093
                  grad_loss   0.0090
                    ib_loss  -0.1973
                  task_loss   0.0604
                       beta   0.0000
             posterior_loss   0.4798
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0545 -0.0555 -0.1294 ... -0.1198 -0.1942 -0.2595]]
 discr_rewards:  [[-0.2318 -0.1179 -0.134  ... -0.0424 -0.0049 -0.1273]]
 task_rewards:  [[3.7253e-09 0.0000e+00 7.4506e-09 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.3258 -0.2129 -0.303  ... -0.2017 -0.2387 -0.4263]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 481, num timesteps 987136, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0294
                action_loss   0.0007
                    bc_loss   0.0000
               dist_entropy   0.7244
         discriminator_loss   0.9182
                  gail_loss   0.9084
                  grad_loss   0.0098
                    ib_loss  -0.1968
                  task_loss   0.0328
                       beta   0.0000
             posterior_loss   0.4839
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0657 -0.0526 -0.1331 ... -0.073  -0.2182 -0.0531]]
 discr_rewards:  [[-0.0766 -0.0726  0.0091 ... -0.0371 -0.0256 -0.0707]]
 task_rewards:  [[-3.7253e-09  3.7253e-09  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.1818 -0.1647 -0.1636 ... -0.1497 -0.2833 -0.1633]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 482, num timesteps 989184, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0295
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7259
         discriminator_loss   0.9194
                  gail_loss   0.9103
                  grad_loss   0.0092
                    ib_loss  -0.1969
                  task_loss   0.0322
                       beta   0.0000
             posterior_loss   0.5141
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0525 -0.2214 -0.0947 ... -0.0799 -0.1239 -0.0794]]
 discr_rewards:  [[-0.0951 -0.1534 -0.0447 ... -0.0303 -0.0638 -0.1126]]
 task_rewards:  [[3.7253e-09 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.1871 -0.4143 -0.1789 ... -0.1497 -0.2273 -0.2315]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 483, num timesteps 991232, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0366
                action_loss   0.0016
                    bc_loss   0.0000
               dist_entropy   0.7301
         discriminator_loss   0.9305
                  gail_loss   0.9192
                  grad_loss   0.0113
                    ib_loss  -0.1969
                  task_loss   0.0336
                       beta   0.0000
             posterior_loss   0.4859
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1174 -0.0863 -0.0533 ... -0.1008 -0.0602 -0.1112]]
 discr_rewards:  [[-0.0271 -0.0055 -0.0182 ... -0.0706  0.0005 -0.0299]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ... -3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.184  -0.1313 -0.111  ... -0.211  -0.0992 -0.1805]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 484, num timesteps 993280, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0347
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.7280
         discriminator_loss   0.9313
                  gail_loss   0.9188
                  grad_loss   0.0125
                    ib_loss  -0.1969
                  task_loss   0.0392
                       beta   0.0000
             posterior_loss   0.5104
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.075  -0.0835 -0.126  ... -0.08   -0.0596 -0.1172]]
 discr_rewards:  [[-0.003  -0.0699 -0.0596 ... -0.147  -0.0502 -0.0375]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  0.0000e+00 ...  3.7253e-09  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.1175 -0.1929 -0.2251 ... -0.2664 -0.1493 -0.1941]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 485, num timesteps 995328, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0263
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.7289
         discriminator_loss   0.9252
                  gail_loss   0.9143
                  grad_loss   0.0109
                    ib_loss  -0.1970
                  task_loss   0.0395
                       beta   0.0000
             posterior_loss   0.4781
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.2014 -0.0559 -0.2754 ... -0.1124 -0.098  -0.2372]]
 discr_rewards:  [[-0.0518 -0.0259 -0.0633 ...  0.0032 -0.0895 -0.0179]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.2927 -0.1213 -0.3782 ... -0.1487 -0.227  -0.2946]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 486, num timesteps 997376, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0301
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.7280
         discriminator_loss   0.9200
                  gail_loss   0.9093
                  grad_loss   0.0107
                    ib_loss  -0.1969
                  task_loss   0.0301
                       beta   0.0000
             posterior_loss   0.5096
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0532 -0.1085 -0.08   ... -0.0551 -0.0941 -0.0763]]
 discr_rewards:  [[-0.0174 -0.0325 -0.0187 ... -0.0616 -0.0567 -0.0488]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1101 -0.1805 -0.1383 ... -0.1561 -0.1903 -0.1646]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 487, num timesteps 999424, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0273
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7299
         discriminator_loss   0.9259
                  gail_loss   0.9154
                  grad_loss   0.0105
                    ib_loss  -0.1968
                  task_loss   0.0377
                       beta   0.0000
             posterior_loss   0.4928
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.053  -0.2951 -0.0762 ... -0.1741 -0.1527 -0.0552]]
 discr_rewards:  [[-0.1132 -0.0573 -0.0511 ... -0.1169 -0.0174 -0.0637]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2058 -0.392  -0.1668 ... -0.3305 -0.2096 -0.1584]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 488, num timesteps 1001472, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7226
         discriminator_loss   0.9109
                  gail_loss   0.9015
                  grad_loss   0.0094
                    ib_loss  -0.1967
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.5099
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.053  -0.0635 -0.0671 ... -0.055  -0.0567 -0.0925]]
 discr_rewards:  [[-0.0507 -0.1104 -0.0593 ... -0.0021 -0.0321 -0.0461]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.1432 -0.2134 -0.1659 ... -0.0967 -0.1283 -0.1782]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 489, num timesteps 1003520, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0246
                action_loss   0.0034
                    bc_loss   0.0000
               dist_entropy   0.7276
         discriminator_loss   0.9179
                  gail_loss   0.9081
                  grad_loss   0.0098
                    ib_loss  -0.1965
                  task_loss   0.0327
                       beta   0.0000
             posterior_loss   0.4911
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0542 -0.0822 -0.0524 ... -0.1574 -0.0552 -0.0554]]
 discr_rewards:  [[-0.0481 -0.0368  0.0024 ... -0.1306 -0.1109 -0.0525]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 3.7253e-09 0.0000e+00]]
 final_rewards:  [[-0.1419 -0.1585 -0.0895 ... -0.3275 -0.2055 -0.1474]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 490, num timesteps 1005568, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0256
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.7326
         discriminator_loss   0.9272
                  gail_loss   0.9170
                  grad_loss   0.0102
                    ib_loss  -0.1964
                  task_loss   0.0270
                       beta   0.0000
             posterior_loss   0.4761
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0624 -0.1107 -0.0528 ... -0.1043 -0.059  -0.1165]]
 discr_rewards:  [[-0.0661 -0.039  -0.0248 ... -0.1085 -0.0817 -0.0898]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.168  -0.1892 -0.1172 ... -0.2524 -0.1802 -0.2459]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 491, num timesteps 1007616, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0297
                action_loss   0.0012
                    bc_loss   0.0000
               dist_entropy   0.7312
         discriminator_loss   0.9239
                  gail_loss   0.9131
                  grad_loss   0.0108
                    ib_loss  -0.1972
                  task_loss   0.0296
                       beta   0.0000
             posterior_loss   0.5083
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0576 -0.0524 -0.0771 ... -0.0978 -0.1627 -0.0643]]
 discr_rewards:  [[-0.0107 -0.0425 -0.0277 ... -0.018  -0.1341 -0.1207]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -7.4506e-09
   0.0000e+00]]
 final_rewards:  [[-0.1079 -0.1345 -0.1443 ... -0.1553 -0.3363 -0.2246]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 492, num timesteps 1009664, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0332
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7247
         discriminator_loss   0.9204
                  gail_loss   0.9102
                  grad_loss   0.0102
                    ib_loss  -0.1969
                  task_loss   0.0324
                       beta   0.0000
             posterior_loss   0.5150
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0553 -0.0524 -0.0595 ... -0.0542 -0.0629 -0.0531]]
 discr_rewards:  [[-0.0942 -0.092  -0.1853 ... -0.0424 -0.0919 -0.0555]]
 task_rewards:  [[-3.7253e-09 -3.7253e-09  7.4506e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.189  -0.1838 -0.2844 ... -0.1361 -0.1943 -0.1481]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 493, num timesteps 1011712, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0290
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.7292
         discriminator_loss   0.9222
                  gail_loss   0.9126
                  grad_loss   0.0096
                    ib_loss  -0.1970
                  task_loss   0.0354
                       beta   0.0000
             posterior_loss   0.4995
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0559 -0.1925 -0.0528 ... -0.1195 -0.1163 -0.079 ]]
 discr_rewards:  [[-0.0587 -0.017  -0.0292 ... -0.0318 -0.0637 -0.1649]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.1541 -0.2491 -0.1216 ... -0.1909 -0.2195 -0.2835]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 494, num timesteps 1013760, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0365
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.7339
         discriminator_loss   0.9188
                  gail_loss   0.9091
                  grad_loss   0.0098
                    ib_loss  -0.1970
                  task_loss   0.0451
                       beta   0.0000
             posterior_loss   0.4969
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0536 -0.1734 -0.0529 ... -0.0524 -0.0824 -0.053 ]]
 discr_rewards:  [[-0.1173 -0.0688 -0.0393 ... -0.074  -0.0353 -0.0297]]
 task_rewards:  [[3.7253e-09 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[-0.2105 -0.2817 -0.1317 ... -0.1659 -0.1572 -0.1222]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 495, num timesteps 1015808, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0262
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.7264
         discriminator_loss   0.9147
                  gail_loss   0.9045
                  grad_loss   0.0102
                    ib_loss  -0.1970
                  task_loss   0.0421
                       beta   0.0000
             posterior_loss   0.5012
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1143 -0.0634 -0.0751 ... -0.0616 -0.308  -0.0891]]
 discr_rewards:  [[-0.1023 -0.0894 -0.009  ... -0.0467 -0.0693 -0.158 ]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 3.7253e-09]]
 final_rewards:  [[-0.2561 -0.1923 -0.1237 ... -0.1477 -0.4168 -0.2867]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 496, num timesteps 1017856, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0395
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.7394
         discriminator_loss   0.9255
                  gail_loss   0.9150
                  grad_loss   0.0105
                    ib_loss  -0.1965
                  task_loss   0.0312
                       beta   0.0000
             posterior_loss   0.5194
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1789 -0.1006 -0.0767 ... -0.0724 -0.0535 -0.1037]]
 discr_rewards:  [[-0.1058 -0.026  -0.0984 ... -0.0135 -0.1283 -0.1793]]
 task_rewards:  [[ 3.7253e-09  0.0000e+00  0.0000e+00 ...  0.0000e+00 -3.7253e-09
   3.7253e-09]]
 final_rewards:  [[-0.3243 -0.1661 -0.2146 ... -0.1253 -0.2213 -0.3225]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 497, num timesteps 1019904, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0331
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.7312
         discriminator_loss   0.9252
                  gail_loss   0.9137
                  grad_loss   0.0115
                    ib_loss  -0.1970
                  task_loss   0.0442
                       beta   0.0000
             posterior_loss   0.4978
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.1672 -0.1238 -0.0542 ... -0.088  -0.0937 -0.0575]]
 discr_rewards:  [[-0.0019 -0.0588 -0.0699 ... -0.0425 -0.0142 -0.0337]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00 -3.7253e-09 ...  0.0000e+00  0.0000e+00
   0.0000e+00]]
 final_rewards:  [[-0.2086 -0.2221 -0.1636 ... -0.1699 -0.1474 -0.1307]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 498, num timesteps 1021952, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0289
                action_loss   0.0005
                    bc_loss   0.0000
               dist_entropy   0.7287
         discriminator_loss   0.9258
                  gail_loss   0.9162
                  grad_loss   0.0096
                    ib_loss  -0.1969
                  task_loss   0.0402
                       beta   0.0000
             posterior_loss   0.4892
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.0867 -0.0902 -0.2304 ... -0.2838 -0.1314 -0.053 ]]
 discr_rewards:  [[-0.041  -0.1061 -0.0146 ... -0.0943 -0.1239 -0.1363]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  3.7253e-09
  -7.4506e-09]]
 final_rewards:  [[-0.1673 -0.2358 -0.2846 ... -0.4177 -0.2948 -0.2288]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 499, num timesteps 1024000, FPS 159 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0214
                action_loss   0.0010
                    bc_loss   0.0000
               dist_entropy   0.7254
         discriminator_loss   0.9294
                  gail_loss   0.9172
                  grad_loss   0.0121
                    ib_loss  -0.1966
                  task_loss   0.0313
                       beta   0.0000
             posterior_loss   0.5052
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[-0.264  -0.1154 -0.2628 ... -0.0529 -0.0545 -0.0612]]
 discr_rewards:  [[-0.0507 -0.0983 -0.0726 ... -0.0509 -0.0953 -0.0311]]
 task_rewards:  [[ 0.0000e+00 -3.7253e-09  3.7253e-09 ...  0.0000e+00  3.7253e-09
   0.0000e+00]]
 final_rewards:  [[-0.3542 -0.2532 -0.3748 ... -0.1433 -0.1893 -0.1318]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 500, num timesteps 1026048, FPS 158 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0410
                action_loss   0.0032
                    bc_loss   0.0000
               dist_entropy   0.7270
         discriminator_loss   0.9094
                  gail_loss   0.8982
                  grad_loss   0.0112
                    ib_loss  -0.1968
                  task_loss   0.0241
                       beta   0.0000
             posterior_loss   0.5058
===============================================================================================
(multi-task) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num -steps 2048 --num-processes 1 \
> --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 \
> --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits \
> --gail --extract-obs --gail-algo standard --expert-algo ppo \
> --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior \
> --save-date 200706 --eval-interval 1 \
> --task-curiosity-reward True --latent-space discrete
Logging to /tmp/openai-2020-07-07-20-27-06-590660
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
usage: main.py [-h] [--carla-ego-vehicle-filter CARLA_EGO_VEHICLE_FILTER] [--carla-town CARLA_TOWN] [--carla-task CARLA_TASK] [--carla-discrete-action] [--carla-port CARLA_PORT]
               [--carla-display-size CARLA_DISPLAY_SIZE] [--carla-number-of-vehicles CARLA_NUMBER_OF_VEHICLES] [--carla-number-of-walkers CARLA_NUMBER_OF_WALKERS]
               [--carla-max-past-step CARLA_MAX_PAST_STEP] [--carla-dt CARLA_DT] [--carla-max-time-episode CARLA_MAX_TIME_EPISODE] [--carla-max-waypt CARLA_MAX_WAYPT] [--carla-obs-range CARLA_OBS_RANGE]
               [--carla-lidar-bin CARLA_LIDAR_BIN] [--carla-d-behind CARLA_D_BEHIND] [--carla-out-lane-thres CARLA_OUT_LANE_THRES] [--carla-desired-speed CARLA_DESIRED_SPEED]
               [--carla-max-ego-spawn-times CARLA_MAX_EGO_SPAWN_TIMES] [--algo ALGO] [--gail-algo GAIL_ALGO] [--expert-algo EXPERT_ALGO] [--pretrain-algo PRETRAIN_ALGO] [--load-algo LOAD_ALGO]
               [--test-model TEST_MODEL] [--latent-space LATENT_SPACE] [--save-result] [--render] [--load-model] [--gail] [--task-transition] [--task-curiosity-reward] [--reset-posterior]
               [--reset-transition] [--fix-beta] [--good-end] [--extract-obs] [--use-latent] [--use-random-latent] [--use-constant-latent] [--constant-latent CONSTANT_LATENT] [--posterior]
               [--hierarchical-policy] [--init_beta INIT_BETA] [--i_c I_C] [--latent-dim LATENT_DIM] [--gail-batch-size GAIL_BATCH_SIZE] [--posterior-batch-size POSTERIOR_BATCH_SIZE]
               [--recurrent-hidden-task-state-size RECURRENT_HIDDEN_TASK_STATE_SIZE] [--episode EPISODE] [--pretrain-epoch PRETRAIN_EPOCH] [--bc-epoch BC_EPOCH] [--cvae-epoch CVAE_EPOCH]
               [--ppo-epoch PPO_EPOCH] [--gail-epoch GAIL_EPOCH] [--posterior-epoch POSTERIOR_EPOCH] [--task-epoch TASK_EPOCH] [--lr LR] [--d_lr D_LR] [--p_lr P_LR] [--t_lr T_LR] [--b_lr B_LR] [--eps EPS]
               [--alpha ALPHA] [--gamma GAMMA] [--use-gae] [--gae-lambda GAE_LAMBDA] [--posterior-reward-coef POSTERIOR_REWARD_COEF] [--discr-reward-coef DISCR_REWARD_COEF]
               [--task-reward-coef TASK_REWARD_COEF] [--entropy-coef ENTROPY_COEF] [--value-loss-coef VALUE_LOSS_COEF] [--bc-loss-coef BC_LOSS_COEF] [--max-grad-norm MAX_GRAD_NORM] [--seed SEED]
               [--cuda-deterministic] [--num-processes NUM_PROCESSES] [--num-steps NUM_STEPS] [--num-mini-batch NUM_MINI_BATCH] [--clip-param CLIP_PARAM] [--log-interval LOG_INTERVAL]
               [--vis-interval VIS_INTERVAL] [--save-interval SAVE_INTERVAL] [--eval-interval EVAL_INTERVAL] [--save-episode SAVE_EPISODE] [--eval-episode EVAL_EPISODE] [--num-env-steps NUM_ENV_STEPS]
               [--env-name ENV_NAME] [--log-dir LOG_DIR] [--pre-log-dir PRE_LOG_DIR] [--result-dir RESULT_DIR] [--pre-result-dir PRE_RESULT_DIR] [--save-dir SAVE_DIR] [--experts-dir EXPERTS_DIR]
               [--gail-experts-dir GAIL_EXPERTS_DIR] [--load-dir LOAD_DIR] [--pre-load-dir PRE_LOAD_DIR] [--pretrain-dir PRETRAIN_DIR] [--load-date LOAD_DATE] [--save-date SAVE_DATE] [--no-cuda]
               [--use-proper-time-limits] [--recurrent-policy] [--use-linear-lr-decay]
main.py: error: unrecognized arguments: True
(multi-task) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num -steps 2048 --num-processes 1 \
> --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 \
> --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits \
> --gail --extract-obs --gail-algo standard --expert-algo ppo \
> --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior \
> --save-date 200706 --eval-interval 1 \
> --task-curiosity-reward False --latent-space discrete
Logging to /tmp/openai-2020-07-07-20-28-43-807991
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
usage: main.py [-h] [--carla-ego-vehicle-filter CARLA_EGO_VEHICLE_FILTER] [--carla-town CARLA_TOWN] [--carla-task CARLA_TASK] [--carla-discrete-action] [--carla-port CARLA_PORT]
               [--carla-display-size CARLA_DISPLAY_SIZE] [--carla-number-of-vehicles CARLA_NUMBER_OF_VEHICLES] [--carla-number-of-walkers CARLA_NUMBER_OF_WALKERS]
               [--carla-max-past-step CARLA_MAX_PAST_STEP] [--carla-dt CARLA_DT] [--carla-max-time-episode CARLA_MAX_TIME_EPISODE] [--carla-max-waypt CARLA_MAX_WAYPT] [--carla-obs-range CARLA_OBS_RANGE]
               [--carla-lidar-bin CARLA_LIDAR_BIN] [--carla-d-behind CARLA_D_BEHIND] [--carla-out-lane-thres CARLA_OUT_LANE_THRES] [--carla-desired-speed CARLA_DESIRED_SPEED]
               [--carla-max-ego-spawn-times CARLA_MAX_EGO_SPAWN_TIMES] [--algo ALGO] [--gail-algo GAIL_ALGO] [--expert-algo EXPERT_ALGO] [--pretrain-algo PRETRAIN_ALGO] [--load-algo LOAD_ALGO]
               [--test-model TEST_MODEL] [--latent-space LATENT_SPACE] [--save-result] [--render] [--load-model] [--gail] [--task-transition] [--task-curiosity-reward] [--reset-posterior]
               [--reset-transition] [--fix-beta] [--good-end] [--extract-obs] [--use-latent] [--use-random-latent] [--use-constant-latent] [--constant-latent CONSTANT_LATENT] [--posterior]
               [--hierarchical-policy] [--init_beta INIT_BETA] [--i_c I_C] [--latent-dim LATENT_DIM] [--gail-batch-size GAIL_BATCH_SIZE] [--posterior-batch-size POSTERIOR_BATCH_SIZE]
               [--recurrent-hidden-task-state-size RECURRENT_HIDDEN_TASK_STATE_SIZE] [--episode EPISODE] [--pretrain-epoch PRETRAIN_EPOCH] [--bc-epoch BC_EPOCH] [--cvae-epoch CVAE_EPOCH]
               [--ppo-epoch PPO_EPOCH] [--gail-epoch GAIL_EPOCH] [--posterior-epoch POSTERIOR_EPOCH] [--task-epoch TASK_EPOCH] [--lr LR] [--d_lr D_LR] [--p_lr P_LR] [--t_lr T_LR] [--b_lr B_LR] [--eps EPS]
               [--alpha ALPHA] [--gamma GAMMA] [--use-gae] [--gae-lambda GAE_LAMBDA] [--posterior-reward-coef POSTERIOR_REWARD_COEF] [--discr-reward-coef DISCR_REWARD_COEF]
               [--task-reward-coef TASK_REWARD_COEF] [--entropy-coef ENTROPY_COEF] [--value-loss-coef VALUE_LOSS_COEF] [--bc-loss-coef BC_LOSS_COEF] [--max-grad-norm MAX_GRAD_NORM] [--seed SEED]
               [--cuda-deterministic] [--num-processes NUM_PROCESSES] [--num-steps NUM_STEPS] [--num-mini-batch NUM_MINI_BATCH] [--clip-param CLIP_PARAM] [--log-interval LOG_INTERVAL]
               [--vis-interval VIS_INTERVAL] [--save-interval SAVE_INTERVAL] [--eval-interval EVAL_INTERVAL] [--save-episode SAVE_EPISODE] [--eval-episode EVAL_EPISODE] [--num-env-steps NUM_ENV_STEPS]
               [--env-name ENV_NAME] [--log-dir LOG_DIR] [--pre-log-dir PRE_LOG_DIR] [--result-dir RESULT_DIR] [--pre-result-dir PRE_RESULT_DIR] [--save-dir SAVE_DIR] [--experts-dir EXPERTS_DIR]
               [--gail-experts-dir GAIL_EXPERTS_DIR] [--load-dir LOAD_DIR] [--pre-load-dir PRE_LOAD_DIR] [--pretrain-dir PRETRAIN_DIR] [--load-date LOAD_DATE] [--save-date SAVE_DATE] [--no-cuda]
               [--use-proper-time-limits] [--recurrent-policy] [--use-linear-lr-decay]
main.py: error: unrecognized arguments: False
(multi-task) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num -steps 2048 --num-processes 1 \
> --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 \
> --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits \
> --gail --extract-obs --gail-algo standard --expert-algo ppo \
> --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior \
> --save-date 200706 --eval-interval 1 \
> --task-curiosity-reward --latent-space discrete
Logging to /tmp/openai-2020-07-07-20-29-13-390074
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/jinwoo/miniconda3/envs/multi-task/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
************************************************************************************************
0. task:  MountainToyCar-v1  device:  cuda:0
   observation shape:  (1,)
1. policy_lr & task_lr 0.0003, b_lr: 0.0001, discr_lr: 0.0001, postr_lr: 0.0001
2. policy: hierarchical_policy
   action distribution is Categorical
3. algorithm: ppo
4. posterior: true
   latent distribution is Categorical
5. discriminator: standard_discriminator
6. task_transition_model: true
   latent distribution is Categorical
************************************************************************************************
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-10.      -2.0196  -1.2432 ...  -0.0379  -0.0379  -0.0379]]
 postr_rewards:  [[10.      2.0196  1.2432 ...  0.0379  0.0379  0.0379]]
 discr_rewards:  [[-0.0088  0.0345  0.0346 ... -0.0044  0.0088 -0.0045]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0088  0.0345  0.0346 ... -0.0044  0.0088 -0.0045]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 0, num timesteps 2048, FPS 35 
 Last 5 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0174
                action_loss  -0.0301
                    bc_loss   0.0000
               dist_entropy   1.0870
         discriminator_loss   2.0412
                  gail_loss   1.6990
                  grad_loss   0.3422
                    ib_loss   0.0000
                  task_loss   0.0837
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0379 -0.0379 -0.0379 ... -0.0387 -0.0387 -0.0387]]
 postr_rewards:  [[0.0379 0.0379 0.0379 ... 0.0387 0.0387 0.0387]]
 discr_rewards:  [[-0.005   0.0505 -0.0051 ...  0.0047  0.0569  0.0061]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.005   0.0505 -0.0051 ...  0.0047  0.0569  0.0061]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 1, num timesteps 4096, FPS 57 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss  -0.0305
                    bc_loss   0.0000
               dist_entropy   1.0503
         discriminator_loss   1.8818
                  gail_loss   1.7831
                  grad_loss   0.0987
                    ib_loss   0.0000
                  task_loss   0.0134
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0387 -0.0387 -0.0387 ... -0.0391 -0.0391 -0.0391]]
 postr_rewards:  [[0.0387 0.0387 0.0387 ... 0.0391 0.0391 0.0391]]
 discr_rewards:  [[ 0.0604 -0.001  -0.0005 ...  0.0496 -0.0214  0.0487]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[ 0.0604 -0.001  -0.0005 ...  0.0496 -0.0214  0.0487]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 2, num timesteps 6144, FPS 72 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0395
                action_loss  -0.0419
                    bc_loss   0.0000
               dist_entropy   0.9756
         discriminator_loss   1.8352
                  gail_loss   1.7590
                  grad_loss   0.0762
                    ib_loss   0.0000
                  task_loss   0.0297
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0391 -0.0391 -0.0391 ... -0.0393 -0.0393 -0.0393]]
 postr_rewards:  [[0.0391 0.0391 0.0391 ... 0.0393 0.0393 0.0393]]
 discr_rewards:  [[-0.0188  0.047   0.0469 ... -0.0007 -0.0012 -0.0018]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0188  0.047   0.0469 ... -0.0007 -0.0012 -0.0018]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 3, num timesteps 8192, FPS 83 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0268
                action_loss  -0.0428
                    bc_loss   0.0000
               dist_entropy   0.8553
         discriminator_loss   1.8145
                  gail_loss   1.7495
                  grad_loss   0.0650
                    ib_loss   0.0000
                  task_loss   0.0379
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0393 -0.0393 -0.0393 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[0.0393 0.0393 0.0393 ... 0.0394 0.0394 0.0394]]
 discr_rewards:  [[ 0.0522  0.0516  0.0511 ... -0.0552  0.0461  0.0463]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[ 0.0522  0.0516  0.0511 ... -0.0552  0.0461  0.0463]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 4, num timesteps 10240, FPS 91 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0149
                action_loss  -0.0342
                    bc_loss   0.0000
               dist_entropy   0.6724
         discriminator_loss   1.7527
                  gail_loss   1.7157
                  grad_loss   0.0370
                    ib_loss   0.0000
                  task_loss   0.0248
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0394 0.0394 0.0394 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[ 0.032   0.0322  0.0325 ... -0.0637 -0.0079  0.0414]]
 task_rewards:  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 3.7253e-09 0.0000e+00 0.0000e+00]]
 final_rewards:  [[ 0.032   0.0322  0.0325 ... -0.0637 -0.0079  0.0414]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 5, num timesteps 12288, FPS 97 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0087
                action_loss  -0.0275
                    bc_loss   0.0000
               dist_entropy   0.5021
         discriminator_loss   1.6937
                  gail_loss   1.6704
                  grad_loss   0.0234
                    ib_loss   0.0000
                  task_loss   0.0133
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[ 0.0198 -0.0842  0.0188 ...  0.0175  0.0179  0.0182]]
 task_rewards:  [[0.0000e+00 3.7253e-09 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
 final_rewards:  [[ 0.0198 -0.0842  0.0188 ...  0.0175  0.0179  0.0182]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 6, num timesteps 14336, FPS 102 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0196
                    bc_loss   0.0000
               dist_entropy   0.3527
         discriminator_loss   1.5813
                  gail_loss   1.5703
                  grad_loss   0.0110
                    ib_loss   0.0000
                  task_loss   0.0091
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.001  -0.0009 -0.0007 ... -0.0013 -0.0012 -0.0012]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.001  -0.0009 -0.0007 ... -0.0013 -0.0012 -0.0012]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 7, num timesteps 16384, FPS 105 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0022
                action_loss  -0.0102
                    bc_loss   0.0000
               dist_entropy   0.2755
         discriminator_loss   1.4486
                  gail_loss   1.4421
                  grad_loss   0.0065
                    ib_loss   0.0000
                  task_loss   0.0061
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0394 0.0394 0.0394]]
 discr_rewards:  [[-0.0187 -0.0187 -0.0188 ... -0.0187 -0.0187 -0.0187]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0187 -0.0187 -0.0188 ... -0.0187 -0.0187 -0.0187]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 8, num timesteps 18432, FPS 108 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0216
                action_loss  -0.0045
                    bc_loss   0.0000
               dist_entropy   0.2943
         discriminator_loss   1.3305
                  gail_loss   1.3242
                  grad_loss   0.0063
                    ib_loss   0.0000
                  task_loss   0.0020
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0393 -0.0393 -0.0393]]
 postr_rewards:  [[0.0394 0.0394 0.0394 ... 0.0393 0.0393 0.0393]]
 discr_rewards:  [[-0.0328 -0.0328 -0.0328 ...  0.0205 -0.0278 -0.0278]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0328 -0.0328 -0.0328 ...  0.0205 -0.0278 -0.0278]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 9, num timesteps 20480, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0622
                action_loss  -0.0081
                    bc_loss   0.0000
               dist_entropy   0.3285
         discriminator_loss   1.2404
                  gail_loss   1.2327
                  grad_loss   0.0077
                    ib_loss   0.0000
                  task_loss   0.0155
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0393 -0.0393 -0.0393 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[0.0393 0.0393 0.0393 ... 0.0394 0.0394 0.0394]]
 discr_rewards:  [[-0.0353 -0.0354 -0.0355 ... -0.0455 -0.0452  0.0225]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0353 -0.0354 -0.0355 ... -0.0455 -0.0452  0.0225]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 10, num timesteps 22528, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0952
                action_loss  -0.0087
                    bc_loss   0.0000
               dist_entropy   0.3782
         discriminator_loss   1.1919
                  gail_loss   1.1784
                  grad_loss   0.0135
                    ib_loss   0.0000
                  task_loss   0.0505
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[0.0394 0.0394 0.0394 ... 0.0394 0.0394 0.0394]]
 discr_rewards:  [[-0.0518 -0.0513 -0.0508 ... -0.0484 -0.0492 -0.0501]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0518 -0.0513 -0.0508 ... -0.0484 -0.0492 -0.0501]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 11, num timesteps 24576, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.1107
                action_loss  -0.0101
                    bc_loss   0.0000
               dist_entropy   0.4382
         discriminator_loss   1.1610
                  gail_loss   1.1427
                  grad_loss   0.0183
                    ib_loss   0.0000
                  task_loss   0.0881
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0394 0.0394 0.0394 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0547 -0.0556 -0.0564 ... -0.0426 -0.0419 -0.0412]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0547 -0.0556 -0.0564 ... -0.0426 -0.0419 -0.0412]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 12, num timesteps 26624, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0923
                action_loss  -0.0137
                    bc_loss   0.0000
               dist_entropy   0.4721
         discriminator_loss   1.1583
                  gail_loss   1.1380
                  grad_loss   0.0204
                    ib_loss   0.0000
                  task_loss   0.0846
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0421 -0.0416 -0.0411 ... -0.057  -0.0564  0.0326]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0421 -0.0416 -0.0411 ... -0.057  -0.0564  0.0326]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 13, num timesteps 28672, FPS 119 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0626
                action_loss  -0.0192
                    bc_loss   0.0000
               dist_entropy   0.5329
         discriminator_loss   1.1673
                  gail_loss   1.1448
                  grad_loss   0.0225
                    ib_loss   0.0000
                  task_loss   0.0913
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0575 -0.0567 -0.0558 ... -0.0537  0.0347 -0.0558]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0575 -0.0567 -0.0558 ... -0.0537  0.0347 -0.0558]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 14, num timesteps 30720, FPS 121 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0577
                action_loss  -0.0224
                    bc_loss   0.0000
               dist_entropy   0.6125
         discriminator_loss   1.1659
                  gail_loss   1.1411
                  grad_loss   0.0247
                    ib_loss   0.0000
                  task_loss   0.0619
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0585 -0.0596 -0.0607 ... -0.0451  0.0404  0.0414]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0585 -0.0596 -0.0607 ... -0.0451  0.0404  0.0414]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 15, num timesteps 32768, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0320
                action_loss  -0.0287
                    bc_loss   0.0000
               dist_entropy   0.6742
         discriminator_loss   1.1885
                  gail_loss   1.1657
                  grad_loss   0.0227
                    ib_loss   0.0000
                  task_loss   0.0512
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0408 -0.0397  0.0372 ... -0.0553 -0.0553 -0.0554]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0408 -0.0397  0.0372 ... -0.0553 -0.0553 -0.0554]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 16, num timesteps 34816, FPS 123 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0203
                action_loss  -0.0340
                    bc_loss   0.0000
               dist_entropy   0.7063
         discriminator_loss   1.2207
                  gail_loss   1.1981
                  grad_loss   0.0226
                    ib_loss   0.0000
                  task_loss   0.0310
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0394 0.0394 0.0394]]
 discr_rewards:  [[ 0.0064 -0.0563 -0.0563 ... -0.046  -0.0454 -0.0448]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[ 0.0064 -0.0563 -0.0563 ... -0.046  -0.0454 -0.0448]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 17, num timesteps 36864, FPS 123 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0092
                action_loss  -0.0429
                    bc_loss   0.0000
               dist_entropy   0.7278
         discriminator_loss   1.2245
                  gail_loss   1.2057
                  grad_loss   0.0188
                    ib_loss   0.0000
                  task_loss   0.0191
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0394 -0.0394 -0.0394]]
 postr_rewards:  [[0.0394 0.0394 0.0394 ... 0.0394 0.0394 0.0394]]
 discr_rewards:  [[ 0.0008  0.0013 -0.0357 ... -0.0329  0.0049  0.0055]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[ 0.0008  0.0013 -0.0357 ... -0.0329  0.0049  0.0055]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 18, num timesteps 38912, FPS 124 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0368
                    bc_loss   0.0000
               dist_entropy   0.6969
         discriminator_loss   1.2583
                  gail_loss   1.2397
                  grad_loss   0.0186
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0394 -0.0394 -0.0394 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0394 0.0394 0.0394 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0077 -0.0072 -0.0184 ... -0.0069 -0.0067 -0.0066]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0077 -0.0072 -0.0184 ... -0.0069 -0.0067 -0.0066]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 19, num timesteps 40960, FPS 125 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0224
                    bc_loss   0.0000
               dist_entropy   0.6413
         discriminator_loss   1.2696
                  gail_loss   1.2521
                  grad_loss   0.0176
                    ib_loss   0.0000
                  task_loss   0.0095
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0193 -0.0192 -0.0047 ... -0.0245 -0.0242 -0.0239]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0193 -0.0192 -0.0047 ... -0.0245 -0.0242 -0.0239]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 20, num timesteps 43008, FPS 126 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0080
                action_loss  -0.0054
                    bc_loss   0.0000
               dist_entropy   0.5783
         discriminator_loss   1.2592
                  gail_loss   1.2432
                  grad_loss   0.0160
                    ib_loss   0.0000
                  task_loss   0.0083
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0319 -0.0316 -0.0313 ... -0.0359 -0.0358 -0.0356]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0319 -0.0316 -0.0313 ... -0.0359 -0.0358 -0.0356]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 21, num timesteps 45056, FPS 126 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0041
                action_loss  -0.0093
                    bc_loss   0.0000
               dist_entropy   0.6166
         discriminator_loss   1.2328
                  gail_loss   1.2166
                  grad_loss   0.0162
                    ib_loss   0.0000
                  task_loss   0.0139
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0416 -0.0414 -0.0412 ... -0.0456 -0.0457 -0.0458]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0416 -0.0414 -0.0412 ... -0.0456 -0.0457 -0.0458]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 22, num timesteps 47104, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0053
                action_loss  -0.0142
                    bc_loss   0.0000
               dist_entropy   0.6590
         discriminator_loss   1.2181
                  gail_loss   1.2034
                  grad_loss   0.0146
                    ib_loss   0.0000
                  task_loss   0.0052
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0019 -0.0448 -0.0449 ...  0.019   0.0177 -0.0278]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0019 -0.0448 -0.0449 ...  0.019   0.0177 -0.0278]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 23, num timesteps 49152, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0045
                    bc_loss   0.0000
               dist_entropy   0.6962
         discriminator_loss   1.2317
                  gail_loss   1.2210
                  grad_loss   0.0107
                    ib_loss   0.0000
                  task_loss   0.0163
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0245  0.0082  0.0063 ...  0.005  -0.0305  0.0021]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0245  0.0082  0.0063 ...  0.005  -0.0305  0.0021]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 24, num timesteps 51200, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0099
                    bc_loss   0.0000
               dist_entropy   0.7094
         discriminator_loss   1.2395
                  gail_loss   1.2309
                  grad_loss   0.0086
                    ib_loss   0.0000
                  task_loss   0.0194
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0205 -0.0206 -0.0207 ... -0.0224 -0.023  -0.0404]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0205 -0.0206 -0.0207 ... -0.0224 -0.023  -0.0404]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 25, num timesteps 53248, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.7066
         discriminator_loss   1.2413
                  gail_loss   1.2332
                  grad_loss   0.0081
                    ib_loss   0.0000
                  task_loss   0.0096
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.038  -0.0385 -0.0284 ... -0.0257 -0.0241 -0.0331]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.038  -0.0385 -0.0284 ... -0.0257 -0.0241 -0.0331]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 26, num timesteps 55296, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0053
                action_loss  -0.0062
                    bc_loss   0.0000
               dist_entropy   0.6848
         discriminator_loss   1.2215
                  gail_loss   1.2145
                  grad_loss   0.0070
                    ib_loss   0.0000
                  task_loss   0.0067
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0363 -0.0154 -0.0137 ... -0.0392 -0.0205 -0.0397]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0363 -0.0154 -0.0137 ... -0.0392 -0.0205 -0.0397]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 27, num timesteps 57344, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0087
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.6990
         discriminator_loss   1.2251
                  gail_loss   1.2187
                  grad_loss   0.0064
                    ib_loss   0.0000
                  task_loss   0.0118
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0377 -0.0378 -0.024  ... -0.0242 -0.026  -0.0414]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0377 -0.0378 -0.024  ... -0.0242 -0.026  -0.0414]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 28, num timesteps 59392, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0072
                action_loss  -0.0077
                    bc_loss   0.0000
               dist_entropy   0.6799
         discriminator_loss   1.2214
                  gail_loss   1.2155
                  grad_loss   0.0060
                    ib_loss   0.0000
                  task_loss   0.0192
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0472 -0.0241 -0.05   ... -0.0131 -0.0384 -0.0385]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0472 -0.0241 -0.05   ... -0.0131 -0.0384 -0.0385]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 29, num timesteps 61440, FPS 119 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0048
                action_loss  -0.0080
                    bc_loss   0.0000
               dist_entropy   0.6949
         discriminator_loss   1.2208
                  gail_loss   1.2143
                  grad_loss   0.0065
                    ib_loss   0.0000
                  task_loss   0.0174
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0175 -0.0349 -0.0349 ... -0.0092 -0.0266 -0.0084]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0175 -0.0349 -0.0349 ... -0.0092 -0.0266 -0.0084]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 30, num timesteps 63488, FPS 119 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6857
         discriminator_loss   1.2207
                  gail_loss   1.2144
                  grad_loss   0.0063
                    ib_loss   0.0000
                  task_loss   0.0133
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0059 -0.0299 -0.0298 ... -0.0103 -0.0348 -0.0354]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0059 -0.0299 -0.0298 ... -0.0103 -0.0348 -0.0354]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 31, num timesteps 65536, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.6909
         discriminator_loss   1.2225
                  gail_loss   1.2168
                  grad_loss   0.0057
                    ib_loss   0.0000
                  task_loss   0.0090
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0332 -0.0177 -0.0343 ... -0.0379 -0.0367 -0.0354]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0332 -0.0177 -0.0343 ... -0.0379 -0.0367 -0.0354]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 32, num timesteps 67584, FPS 121 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0061
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6955
         discriminator_loss   1.2092
                  gail_loss   1.2035
                  grad_loss   0.0057
                    ib_loss   0.0000
                  task_loss   0.0110
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0335 -0.0321 -0.0306 ... -0.0353 -0.0352 -0.0351]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0335 -0.0321 -0.0306 ... -0.0353 -0.0352 -0.0351]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 33, num timesteps 69632, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0110
                action_loss  -0.0126
                    bc_loss   0.0000
               dist_entropy   0.6620
         discriminator_loss   1.2160
                  gail_loss   1.2102
                  grad_loss   0.0058
                    ib_loss   0.0000
                  task_loss   0.0145
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0422 -0.0091 -0.0418 ... -0.0095 -0.0094 -0.0422]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0422 -0.0091 -0.0418 ... -0.0095 -0.0094 -0.0422]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 34, num timesteps 71680, FPS 122 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0074
                action_loss  -0.0053
                    bc_loss   0.0000
               dist_entropy   0.6811
         discriminator_loss   1.2035
                  gail_loss   1.1977
                  grad_loss   0.0058
                    ib_loss   0.0000
                  task_loss   0.0256
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0389 -0.0128 -0.0387 ... -0.0128 -0.0132 -0.0396]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0389 -0.0128 -0.0387 ... -0.0128 -0.0132 -0.0396]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 35, num timesteps 73728, FPS 123 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0016
                action_loss  -0.0157
                    bc_loss   0.0000
               dist_entropy   0.6950
         discriminator_loss   1.2155
                  gail_loss   1.2098
                  grad_loss   0.0057
                    ib_loss   0.0000
                  task_loss   0.0113
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0254 -0.0307 -0.0263 ... -0.0333 -0.034  -0.0348]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0254 -0.0307 -0.0263 ... -0.0333 -0.034  -0.0348]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 36, num timesteps 75776, FPS 123 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0050
                action_loss  -0.0111
                    bc_loss   0.0000
               dist_entropy   0.6849
         discriminator_loss   1.2099
                  gail_loss   1.2033
                  grad_loss   0.0065
                    ib_loss   0.0000
                  task_loss   0.0064
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0473 -0.0301 -0.0485 ... -0.0267 -0.0277 -0.0466]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0473 -0.0301 -0.0485 ... -0.0267 -0.0277 -0.0466]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 37, num timesteps 77824, FPS 124 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6918
         discriminator_loss   1.1991
                  gail_loss   1.1925
                  grad_loss   0.0066
                    ib_loss   0.0000
                  task_loss   0.0079
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.033  -0.0439 -0.0345 ... -0.0264 -0.0353 -0.0226]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.033  -0.0439 -0.0345 ... -0.0264 -0.0353 -0.0226]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 38, num timesteps 79872, FPS 124 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0072
                action_loss  -0.0103
                    bc_loss   0.0000
               dist_entropy   0.6802
         discriminator_loss   1.2056
                  gail_loss   1.1998
                  grad_loss   0.0059
                    ib_loss   0.0000
                  task_loss   0.0073
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0365 -0.0347 -0.0329 ... -0.0049 -0.0263 -0.0255]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0365 -0.0347 -0.0329 ... -0.0049 -0.0263 -0.0255]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 39, num timesteps 81920, FPS 125 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0054
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.6850
         discriminator_loss   1.2187
                  gail_loss   1.2128
                  grad_loss   0.0059
                    ib_loss   0.0000
                  task_loss   0.0133
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0238 -0.0232 -0.0041 ... -0.0231 -0.0394 -0.0385]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0238 -0.0232 -0.0041 ... -0.0231 -0.0394 -0.0385]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 40, num timesteps 83968, FPS 125 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0042
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6929
         discriminator_loss   1.2066
                  gail_loss   1.2007
                  grad_loss   0.0059
                    ib_loss   0.0000
                  task_loss   0.0118
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.026  -0.025  -0.032  ... -0.034  -0.0256 -0.0331]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.026  -0.025  -0.032  ... -0.034  -0.0256 -0.0331]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 41, num timesteps 86016, FPS 125 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0019
                action_loss  -0.0037
                    bc_loss   0.0000
               dist_entropy   0.6807
         discriminator_loss   1.1854
                  gail_loss   1.1784
                  grad_loss   0.0070
                    ib_loss   0.0000
                  task_loss   0.0107
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0376 -0.0158 -0.0366 ... -0.0358 -0.0359 -0.0146]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0376 -0.0158 -0.0366 ... -0.0358 -0.0359 -0.0146]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 42, num timesteps 88064, FPS 126 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6825
         discriminator_loss   1.2016
                  gail_loss   1.1957
                  grad_loss   0.0059
                    ib_loss   0.0000
                  task_loss   0.0066
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.037  -0.015  -0.0373 ... -0.0257 -0.025  -0.0012]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.037  -0.015  -0.0373 ... -0.0257 -0.025  -0.0012]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 43, num timesteps 90112, FPS 126 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0032
                action_loss  -0.0035
                    bc_loss   0.0000
               dist_entropy   0.6951
         discriminator_loss   1.1949
                  gail_loss   1.1878
                  grad_loss   0.0071
                    ib_loss   0.0000
                  task_loss   0.0087
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0111 -0.0108 -0.0106 ... -0.0235 -0.0256 -0.0267]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0111 -0.0108 -0.0106 ... -0.0235 -0.0256 -0.0267]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 44, num timesteps 92160, FPS 126 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0114
                    bc_loss   0.0000
               dist_entropy   0.6913
         discriminator_loss   1.2071
                  gail_loss   1.2003
                  grad_loss   0.0069
                    ib_loss   0.0000
                  task_loss   0.0086
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0226 -0.0238 -0.0396 ... -0.0429 -0.0431 -0.0432]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0226 -0.0238 -0.0396 ... -0.0429 -0.0431 -0.0432]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 45, num timesteps 94208, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0053
                action_loss  -0.0035
                    bc_loss   0.0000
               dist_entropy   0.6677
         discriminator_loss   1.1885
                  gail_loss   1.1819
                  grad_loss   0.0066
                    ib_loss   0.0000
                  task_loss   0.0077
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0492 -0.0491 -0.0212 ... -0.0433 -0.042  -0.0406]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0492 -0.0491 -0.0212 ... -0.0433 -0.042  -0.0406]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 46, num timesteps 96256, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss  -0.0103
                    bc_loss   0.0000
               dist_entropy   0.6854
         discriminator_loss   1.1835
                  gail_loss   1.1769
                  grad_loss   0.0066
                    ib_loss   0.0000
                  task_loss   0.0129
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.013  -0.0114 -0.0345 ... -0.0167 -0.0411 -0.0181]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.013  -0.0114 -0.0345 ... -0.0167 -0.0411 -0.0181]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 47, num timesteps 98304, FPS 128 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.6954
         discriminator_loss   1.1766
                  gail_loss   1.1708
                  grad_loss   0.0058
                    ib_loss   0.0000
                  task_loss   0.0104
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0334 -0.034  -0.0292 ... -0.0278 -0.0333 -0.028 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0334 -0.034  -0.0292 ... -0.0278 -0.0333 -0.028 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 48, num timesteps 100352, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0090
                    bc_loss   0.0000
               dist_entropy   0.6918
         discriminator_loss   1.1799
                  gail_loss   1.1740
                  grad_loss   0.0060
                    ib_loss   0.0000
                  task_loss   0.0071
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0386 -0.0386 -0.0386 ... -0.034  -0.0337 -0.0333]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0386 -0.0386 -0.0386 ... -0.034  -0.0337 -0.0333]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 49, num timesteps 102400, FPS 127 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6942
         discriminator_loss   1.1804
                  gail_loss   1.1752
                  grad_loss   0.0053
                    ib_loss   0.0000
                  task_loss   0.0063
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0349 -0.0349 -0.0348 ... -0.0204 -0.0302 -0.0212]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0349 -0.0349 -0.0348 ... -0.0204 -0.0302 -0.0212]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 50, num timesteps 104448, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0016
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.6840
         discriminator_loss   1.1704
                  gail_loss   1.1645
                  grad_loss   0.0060
                    ib_loss   0.0000
                  task_loss   0.0065
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0389 -0.0393 -0.0178 ... -0.0268 -0.0467 -0.0274]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0389 -0.0393 -0.0178 ... -0.0268 -0.0467 -0.0274]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 51, num timesteps 106496, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6902
         discriminator_loss   1.1642
                  gail_loss   1.1577
                  grad_loss   0.0065
                    ib_loss   0.0000
                  task_loss   0.0057
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0284 -0.0286 -0.0289 ... -0.045  -0.0286 -0.0444]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0284 -0.0286 -0.0289 ... -0.045  -0.0286 -0.0444]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 52, num timesteps 108544, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss  -0.0052
                    bc_loss   0.0000
               dist_entropy   0.6936
         discriminator_loss   1.1636
                  gail_loss   1.1576
                  grad_loss   0.0060
                    ib_loss   0.0000
                  task_loss   0.0144
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0377 -0.0372 -0.0366 ... -0.0365 -0.0339 -0.0337]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0377 -0.0372 -0.0366 ... -0.0365 -0.0339 -0.0337]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 53, num timesteps 110592, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0185
                    bc_loss   0.0000
               dist_entropy   0.6901
         discriminator_loss   1.1576
                  gail_loss   1.1508
                  grad_loss   0.0068
                    ib_loss   0.0000
                  task_loss   0.0075
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0336 -0.0409 -0.0407 ... -0.0439 -0.0395 -0.0398]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0336 -0.0409 -0.0407 ... -0.0439 -0.0395 -0.0398]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 54, num timesteps 112640, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.6700
         discriminator_loss   1.1455
                  gail_loss   1.1386
                  grad_loss   0.0069
                    ib_loss   0.0000
                  task_loss   0.0080
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0213 -0.0215 -0.0534 ... -0.0431 -0.0431 -0.043 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0213 -0.0215 -0.0534 ... -0.0431 -0.0431 -0.043 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 55, num timesteps 114688, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0028
                action_loss  -0.0205
                    bc_loss   0.0000
               dist_entropy   0.6821
         discriminator_loss   1.1290
                  gail_loss   1.1212
                  grad_loss   0.0078
                    ib_loss   0.0000
                  task_loss   0.0053
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0125 -0.0127 -0.0131 ... -0.0473 -0.0472 -0.0472]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0125 -0.0127 -0.0131 ... -0.0473 -0.0472 -0.0472]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 56, num timesteps 116736, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0071
                    bc_loss   0.0000
               dist_entropy   0.6935
         discriminator_loss   1.1279
                  gail_loss   1.1211
                  grad_loss   0.0067
                    ib_loss   0.0000
                  task_loss   0.0064
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0391 -0.039  -0.0388 ... -0.0373 -0.0395 -0.0359]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0391 -0.039  -0.0388 ... -0.0373 -0.0395 -0.0359]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 57, num timesteps 118784, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0014
                action_loss  -0.0079
                    bc_loss   0.0000
               dist_entropy   0.6850
         discriminator_loss   1.1213
                  gail_loss   1.1132
                  grad_loss   0.0081
                    ib_loss   0.0000
                  task_loss   0.0076
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0276 -0.0451 -0.0446 ... -0.0242 -0.043  -0.0427]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0276 -0.0451 -0.0446 ... -0.0242 -0.043  -0.0427]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 58, num timesteps 120832, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0085
                    bc_loss   0.0000
               dist_entropy   0.6925
         discriminator_loss   1.1263
                  gail_loss   1.1185
                  grad_loss   0.0079
                    ib_loss   0.0000
                  task_loss   0.0030
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0331 -0.0303 -0.0323 ... -0.0379 -0.0376 -0.0378]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0331 -0.0303 -0.0323 ... -0.0379 -0.0376 -0.0378]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 59, num timesteps 122880, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0022
                action_loss  -0.0139
                    bc_loss   0.0000
               dist_entropy   0.6859
         discriminator_loss   1.1318
                  gail_loss   1.1229
                  grad_loss   0.0089
                    ib_loss   0.0000
                  task_loss   0.0037
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0312 -0.0483 -0.0483 ... -0.0274 -0.0272 -0.027 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0312 -0.0483 -0.0483 ... -0.0274 -0.0272 -0.027 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 60, num timesteps 124928, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0156
                    bc_loss   0.0000
               dist_entropy   0.6938
         discriminator_loss   1.1062
                  gail_loss   1.0973
                  grad_loss   0.0089
                    ib_loss   0.0000
                  task_loss   0.0043
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0408 -0.0359 -0.0407 ... -0.0237 -0.0303 -0.022 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0408 -0.0359 -0.0407 ... -0.0237 -0.0303 -0.022 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 61, num timesteps 126976, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0053
                    bc_loss   0.0000
               dist_entropy   0.6878
         discriminator_loss   1.1038
                  gail_loss   1.0941
                  grad_loss   0.0097
                    ib_loss   0.0000
                  task_loss   0.0022
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0372 -0.0368 -0.0113 ... -0.0298 -0.0503 -0.0299]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0372 -0.0368 -0.0113 ... -0.0298 -0.0503 -0.0299]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 62, num timesteps 129024, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0105
                    bc_loss   0.0000
               dist_entropy   0.6943
         discriminator_loss   1.0954
                  gail_loss   1.0861
                  grad_loss   0.0092
                    ib_loss   0.0000
                  task_loss   0.0039
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0439 -0.0439 -0.0381 ... -0.0356 -0.0358 -0.036 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0439 -0.0439 -0.0381 ... -0.0356 -0.0358 -0.036 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 63, num timesteps 131072, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.6902
         discriminator_loss   1.1103
                  gail_loss   1.1012
                  grad_loss   0.0091
                    ib_loss   0.0000
                  task_loss   0.0056
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0211 -0.0215 -0.0442 ... -0.0041 -0.026  -0.0002]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0211 -0.0215 -0.0442 ... -0.0041 -0.026  -0.0002]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 64, num timesteps 133120, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0099
                    bc_loss   0.0000
               dist_entropy   0.6926
         discriminator_loss   1.1061
                  gail_loss   1.0975
                  grad_loss   0.0086
                    ib_loss   0.0000
                  task_loss   0.0050
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-1.6171e-02 -1.2488e-03 -7.0691e-05 ... -4.1967e-02 -3.7255e-02
  -4.2019e-02]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-1.6171e-02 -1.2488e-03 -7.0691e-05 ... -4.1967e-02 -3.7255e-02
  -4.2019e-02]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 65, num timesteps 135168, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.6862
         discriminator_loss   1.1109
                  gail_loss   1.1030
                  grad_loss   0.0079
                    ib_loss   0.0000
                  task_loss   0.0060
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0461 -0.0464 -0.0341 ... -0.05   -0.05   -0.0322]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0461 -0.0464 -0.0341 ... -0.05   -0.05   -0.0322]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 66, num timesteps 137216, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0014
                action_loss  -0.0160
                    bc_loss   0.0000
               dist_entropy   0.6946
         discriminator_loss   1.0791
                  gail_loss   1.0696
                  grad_loss   0.0095
                    ib_loss   0.0000
                  task_loss   0.0044
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.041  -0.0389 -0.039  ... -0.0402 -0.037  -0.0395]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.041  -0.0389 -0.039  ... -0.0402 -0.037  -0.0395]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 67, num timesteps 139264, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0017
                action_loss  -0.0128
                    bc_loss   0.0000
               dist_entropy   0.6865
         discriminator_loss   1.1003
                  gail_loss   1.0924
                  grad_loss   0.0079
                    ib_loss   0.0000
                  task_loss   0.0026
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.047  -0.0466 -0.0461 ... -0.0336 -0.0494 -0.0493]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.047  -0.0466 -0.0461 ... -0.0336 -0.0494 -0.0493]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 68, num timesteps 141312, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0099
                    bc_loss   0.0000
               dist_entropy   0.6925
         discriminator_loss   1.0844
                  gail_loss   1.0757
                  grad_loss   0.0087
                    ib_loss   0.0000
                  task_loss   0.0047
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0402 -0.04   -0.0436 ... -0.044  -0.044  -0.0423]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0402 -0.04   -0.0436 ... -0.044  -0.044  -0.0423]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 69, num timesteps 143360, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0011
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.6872
         discriminator_loss   1.0805
                  gail_loss   1.0721
                  grad_loss   0.0084
                    ib_loss   0.0000
                  task_loss   0.0040
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0491 -0.0491 -0.0374 ... -0.0372 -0.0483 -0.0373]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0491 -0.0491 -0.0374 ... -0.0372 -0.0483 -0.0373]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 70, num timesteps 145408, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0073
                    bc_loss   0.0000
               dist_entropy   0.6916
         discriminator_loss   1.0697
                  gail_loss   1.0618
                  grad_loss   0.0080
                    ib_loss   0.0000
                  task_loss   0.0028
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0439 -0.0441 -0.0418 ... -0.0445 -0.0389 -0.0437]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0439 -0.0441 -0.0418 ... -0.0445 -0.0389 -0.0437]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 71, num timesteps 147456, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6868
         discriminator_loss   1.0820
                  gail_loss   1.0744
                  grad_loss   0.0077
                    ib_loss   0.0000
                  task_loss   0.0033
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0349 -0.0449 -0.0445 ... -0.0378 -0.0469 -0.0378]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0349 -0.0449 -0.0445 ... -0.0378 -0.0469 -0.0378]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 72, num timesteps 149504, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0017
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6910
         discriminator_loss   1.0770
                  gail_loss   1.0693
                  grad_loss   0.0077
                    ib_loss   0.0000
                  task_loss   0.0076
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0422 -0.0452 -0.0452 ... -0.0465 -0.0421 -0.0463]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0422 -0.0452 -0.0452 ... -0.0465 -0.0421 -0.0463]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 73, num timesteps 151552, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0012
                action_loss  -0.0130
                    bc_loss   0.0000
               dist_entropy   0.6893
         discriminator_loss   1.0626
                  gail_loss   1.0557
                  grad_loss   0.0069
                    ib_loss   0.0000
                  task_loss   0.0044
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0386 -0.0384 -0.0382 ... -0.0384 -0.0496 -0.0497]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0386 -0.0384 -0.0382 ... -0.0384 -0.0496 -0.0497]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 74, num timesteps 153600, FPS 120 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0143
                    bc_loss   0.0000
               dist_entropy   0.6893
         discriminator_loss   1.0614
                  gail_loss   1.0543
                  grad_loss   0.0071
                    ib_loss   0.0000
                  task_loss   0.0035
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0417 -0.0418 -0.0419 ... -0.042  -0.0461 -0.0462]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0417 -0.0418 -0.0419 ... -0.042  -0.0461 -0.0462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 75, num timesteps 155648, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0067
                    bc_loss   0.0000
               dist_entropy   0.6858
         discriminator_loss   1.0754
                  gail_loss   1.0680
                  grad_loss   0.0075
                    ib_loss   0.0000
                  task_loss   0.0046
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.05   -0.0411 -0.0411 ... -0.0389 -0.0393 -0.0502]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.05   -0.0411 -0.0411 ... -0.0389 -0.0393 -0.0502]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 76, num timesteps 157696, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0055
                    bc_loss   0.0000
               dist_entropy   0.6600
         discriminator_loss   1.0651
                  gail_loss   1.0592
                  grad_loss   0.0059
                    ib_loss   0.0000
                  task_loss   0.0109
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0537 -0.0538 -0.0537 ... -0.0315 -0.0538 -0.0538]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0537 -0.0538 -0.0537 ... -0.0315 -0.0538 -0.0538]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 77, num timesteps 159744, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0016
                action_loss  -0.0175
                    bc_loss   0.0000
               dist_entropy   0.6914
         discriminator_loss   1.0492
                  gail_loss   1.0430
                  grad_loss   0.0062
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0442 -0.0442 -0.0495 ... -0.0395 -0.046  -0.0459]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0442 -0.0442 -0.0495 ... -0.0395 -0.046  -0.0459]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 78, num timesteps 161792, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0051
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.6864
         discriminator_loss   1.0555
                  gail_loss   1.0502
                  grad_loss   0.0053
                    ib_loss   0.0000
                  task_loss   0.0071
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0338 -0.0465 -0.0465 ... -0.0441 -0.0449 -0.0329]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0338 -0.0465 -0.0465 ... -0.0441 -0.0449 -0.0329]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 79, num timesteps 163840, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0073
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6864
         discriminator_loss   1.0770
                  gail_loss   1.0725
                  grad_loss   0.0044
                    ib_loss   0.0000
                  task_loss   0.0269
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0474 -0.0392 -0.0401 ... -0.044  -0.044  -0.0439]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0474 -0.0392 -0.0401 ... -0.044  -0.044  -0.0439]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 80, num timesteps 165888, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0080
                    bc_loss   0.0000
               dist_entropy   0.6802
         discriminator_loss   1.0443
                  gail_loss   1.0393
                  grad_loss   0.0050
                    ib_loss   0.0000
                  task_loss   0.0177
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0503 -0.0501 -0.0499 ... -0.0507 -0.0416 -0.0507]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0503 -0.0501 -0.0499 ... -0.0507 -0.0416 -0.0507]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 81, num timesteps 167936, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss  -0.0130
                    bc_loss   0.0000
               dist_entropy   0.6932
         discriminator_loss   1.0475
                  gail_loss   1.0430
                  grad_loss   0.0045
                    ib_loss   0.0000
                  task_loss   0.0113
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0435 -0.0479 -0.0479 ... -0.0357 -0.0421 -0.0415]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0435 -0.0479 -0.0479 ... -0.0357 -0.0421 -0.0415]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 82, num timesteps 169984, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0016
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6952
         discriminator_loss   1.0514
                  gail_loss   1.0474
                  grad_loss   0.0040
                    ib_loss   0.0000
                  task_loss   0.0097
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0391 -0.0365 -0.0359 ... -0.0435 -0.0422 -0.0434]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0391 -0.0365 -0.0359 ... -0.0435 -0.0422 -0.0434]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 83, num timesteps 172032, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0054
                    bc_loss   0.0000
               dist_entropy   0.6916
         discriminator_loss   1.0462
                  gail_loss   1.0414
                  grad_loss   0.0048
                    ib_loss   0.0000
                  task_loss   0.0077
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.051  -0.0376 -0.0376 ... -0.0374 -0.0376 -0.0512]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.051  -0.0376 -0.0376 ... -0.0374 -0.0376 -0.0512]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 84, num timesteps 174080, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0017
                action_loss  -0.0123
                    bc_loss   0.0000
               dist_entropy   0.6937
         discriminator_loss   1.0350
                  gail_loss   1.0304
                  grad_loss   0.0047
                    ib_loss   0.0000
                  task_loss   0.0080
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0423 -0.0426 -0.0418 ... -0.0448 -0.0448 -0.0448]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0423 -0.0426 -0.0418 ... -0.0448 -0.0448 -0.0448]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 85, num timesteps 176128, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6934
         discriminator_loss   1.0575
                  gail_loss   1.0540
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0039
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0488 -0.0456 -0.0456 ... -0.037  -0.0311 -0.0309]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0488 -0.0456 -0.0456 ... -0.037  -0.0311 -0.0309]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 86, num timesteps 178176, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0032
                    bc_loss   0.0000
               dist_entropy   0.6859
         discriminator_loss   1.0541
                  gail_loss   1.0505
                  grad_loss   0.0036
                    ib_loss   0.0000
                  task_loss   0.0073
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.025  -0.0428 -0.0431 ... -0.0521 -0.0357 -0.0519]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.025  -0.0428 -0.0431 ... -0.0521 -0.0357 -0.0519]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 87, num timesteps 180224, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0092
                    bc_loss   0.0000
               dist_entropy   0.6908
         discriminator_loss   1.0422
                  gail_loss   1.0381
                  grad_loss   0.0041
                    ib_loss   0.0000
                  task_loss   0.0079
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0485 -0.0484 -0.0399 ... -0.0264 -0.0365 -0.0374]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0485 -0.0484 -0.0399 ... -0.0264 -0.0365 -0.0374]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 88, num timesteps 182272, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.6810
         discriminator_loss   1.0462
                  gail_loss   1.0432
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0097
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0288 -0.0298 -0.0339 ... -0.0426 -0.0461 -0.0453]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0288 -0.0298 -0.0339 ... -0.0426 -0.0461 -0.0453]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 89, num timesteps 184320, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0045
                    bc_loss   0.0000
               dist_entropy   0.6854
         discriminator_loss   1.0522
                  gail_loss   1.0482
                  grad_loss   0.0040
                    ib_loss   0.0000
                  task_loss   0.0084
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0432 -0.044  -0.0433 ... -0.039  -0.0396 -0.0403]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0432 -0.044  -0.0433 ... -0.039  -0.0396 -0.0403]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 90, num timesteps 186368, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6866
         discriminator_loss   1.0454
                  gail_loss   1.0417
                  grad_loss   0.0037
                    ib_loss   0.0000
                  task_loss   0.0121
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.042  -0.0452 -0.0437 ... -0.047  -0.0471 -0.0471]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.042  -0.0452 -0.0437 ... -0.047  -0.0471 -0.0471]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 91, num timesteps 188416, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6833
         discriminator_loss   1.0389
                  gail_loss   1.0357
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0110
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0536 -0.0421 -0.0536 ... -0.0536 -0.0421 -0.0421]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0536 -0.0421 -0.0536 ... -0.0536 -0.0421 -0.0421]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 92, num timesteps 190464, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0017
                action_loss  -0.0069
                    bc_loss   0.0000
               dist_entropy   0.6891
         discriminator_loss   1.0293
                  gail_loss   1.0258
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0077
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0477 -0.0482 -0.0482 ... -0.0449 -0.0446 -0.045 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0477 -0.0482 -0.0482 ... -0.0449 -0.0446 -0.045 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 93, num timesteps 192512, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0008
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6874
         discriminator_loss   1.0313
                  gail_loss   1.0278
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0044
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0427 -0.0425 -0.0424 ... -0.0324 -0.0328 -0.0334]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0427 -0.0425 -0.0424 ... -0.0324 -0.0328 -0.0334]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 94, num timesteps 194560, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0009
                action_loss  -0.0140
                    bc_loss   0.0000
               dist_entropy   0.6903
         discriminator_loss   1.0402
                  gail_loss   1.0368
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0026
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0409 -0.0417 -0.0426 ... -0.0516 -0.0453 -0.0452]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0409 -0.0417 -0.0426 ... -0.0516 -0.0453 -0.0452]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 95, num timesteps 196608, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0048
                    bc_loss   0.0000
               dist_entropy   0.6682
         discriminator_loss   1.0442
                  gail_loss   1.0412
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0092
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0423 -0.0479 -0.0478 ... -0.0498 -0.0497 -0.0434]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0423 -0.0479 -0.0478 ... -0.0498 -0.0497 -0.0434]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 96, num timesteps 198656, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6798
         discriminator_loss   1.0499
                  gail_loss   1.0471
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0137
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0454 -0.045  -0.0443 ... -0.0444 -0.0486 -0.0449]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0454 -0.045  -0.0443 ... -0.0444 -0.0486 -0.0449]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 97, num timesteps 200704, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0023
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.6856
         discriminator_loss   1.0466
                  gail_loss   1.0439
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0099
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0456 -0.0486 -0.0489 ... -0.0224 -0.0253 -0.0278]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0456 -0.0486 -0.0489 ... -0.0224 -0.0253 -0.0278]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 98, num timesteps 202752, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0042
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.6764
         discriminator_loss   1.0347
                  gail_loss   1.0314
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0115
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0372 -0.026  -0.0432 ... -0.0527 -0.0518 -0.0366]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0372 -0.026  -0.0432 ... -0.0527 -0.0518 -0.0366]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 99, num timesteps 204800, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0118
                action_loss  -0.0071
                    bc_loss   0.0000
               dist_entropy   0.6765
         discriminator_loss   1.0580
                  gail_loss   1.0551
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0305
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.05   -0.04   -0.05   ... -0.0526 -0.0432 -0.0522]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.05   -0.04   -0.05   ... -0.0526 -0.0432 -0.0522]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 100, num timesteps 206848, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.6828
         discriminator_loss   1.0459
                  gail_loss   1.0429
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0250
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0441 -0.0438 -0.0482 ... -0.0484 -0.0483 -0.0483]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0441 -0.0438 -0.0482 ... -0.0484 -0.0483 -0.0483]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 101, num timesteps 208896, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0083
                    bc_loss   0.0000
               dist_entropy   0.6849
         discriminator_loss   1.0446
                  gail_loss   1.0417
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0113
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0518 -0.0409 -0.0409 ... -0.0332 -0.0463 -0.0339]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0518 -0.0409 -0.0409 ... -0.0332 -0.0463 -0.0339]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 102, num timesteps 210944, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0045
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6877
         discriminator_loss   1.0500
                  gail_loss   1.0470
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0446 -0.0442 -0.0457 ... -0.0412 -0.0412 -0.0444]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0446 -0.0442 -0.0457 ... -0.0412 -0.0412 -0.0444]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 103, num timesteps 212992, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0049
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.6832
         discriminator_loss   1.0331
                  gail_loss   1.0304
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0137
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0311 -0.0456 -0.0461 ... -0.0309 -0.0424 -0.0396]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0311 -0.0456 -0.0461 ... -0.0309 -0.0424 -0.0396]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 104, num timesteps 215040, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0106
                    bc_loss   0.0000
               dist_entropy   0.6916
         discriminator_loss   1.0478
                  gail_loss   1.0451
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0135
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0311 -0.0302 -0.0257 ... -0.044  -0.0435 -0.0429]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0311 -0.0302 -0.0257 ... -0.044  -0.0435 -0.0429]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 105, num timesteps 217088, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0057
                    bc_loss   0.0000
               dist_entropy   0.6849
         discriminator_loss   1.0379
                  gail_loss   1.0349
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0085
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0498 -0.0492 -0.0322 ... -0.0399 -0.0378 -0.0254]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0498 -0.0492 -0.0322 ... -0.0399 -0.0378 -0.0254]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 106, num timesteps 219136, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0028
                action_loss  -0.0067
                    bc_loss   0.0000
               dist_entropy   0.6926
         discriminator_loss   1.0272
                  gail_loss   1.0237
                  grad_loss   0.0036
                    ib_loss   0.0000
                  task_loss   0.0137
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0322 -0.0307 -0.0296 ... -0.0461 -0.0395 -0.0477]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0322 -0.0307 -0.0296 ... -0.0461 -0.0395 -0.0477]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 107, num timesteps 221184, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0150
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6913
         discriminator_loss   1.0533
                  gail_loss   1.0501
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0344
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0412 -0.0423 -0.0434 ... -0.0408 -0.0404 -0.041 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0412 -0.0423 -0.0434 ... -0.0408 -0.0404 -0.041 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 108, num timesteps 223232, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0035
                    bc_loss   0.0000
               dist_entropy   0.6817
         discriminator_loss   1.0385
                  gail_loss   1.0352
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0512
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0442 -0.0442 -0.0304 ... -0.0545 -0.0365 -0.0544]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0442 -0.0442 -0.0304 ... -0.0545 -0.0365 -0.0544]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 109, num timesteps 225280, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0155
                    bc_loss   0.0000
               dist_entropy   0.6931
         discriminator_loss   1.0354
                  gail_loss   1.0321
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0083
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0498 -0.0453 -0.0498 ... -0.0431 -0.0448 -0.0443]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0498 -0.0453 -0.0498 ... -0.0431 -0.0448 -0.0443]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 110, num timesteps 227328, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0037
                    bc_loss   0.0000
               dist_entropy   0.6991
         discriminator_loss   1.0380
                  gail_loss   1.0351
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0083
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0502 -0.044  -0.0445 ... -0.038  -0.0462 -0.0385]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0502 -0.044  -0.0445 ... -0.038  -0.0462 -0.0385]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 111, num timesteps 229376, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0032
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6999
         discriminator_loss   1.0343
                  gail_loss   1.0309
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0138
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0416 -0.0481 -0.0426 ... -0.0459 -0.045  -0.0498]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0416 -0.0481 -0.0426 ... -0.0459 -0.045  -0.0498]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 112, num timesteps 231424, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0163
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6946
         discriminator_loss   1.0539
                  gail_loss   1.0509
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0238
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0473 -0.0464 -0.0382 ... -0.0317 -0.0321 -0.0394]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0473 -0.0464 -0.0382 ... -0.0317 -0.0321 -0.0394]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 113, num timesteps 233472, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0021
                action_loss  -0.0119
                    bc_loss   0.0000
               dist_entropy   0.7014
         discriminator_loss   1.0266
                  gail_loss   1.0233
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0545
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0342 -0.0434 -0.0442 ... -0.0171 -0.0184 -0.02  ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0342 -0.0434 -0.0442 ... -0.0171 -0.0184 -0.02  ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 114, num timesteps 235520, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.6979
         discriminator_loss   1.0424
                  gail_loss   1.0390
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0103
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0153 -0.0175 -0.02   ... -0.047  -0.0305 -0.0316]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0153 -0.0175 -0.02   ... -0.047  -0.0305 -0.0316]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 115, num timesteps 237568, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss  -0.0110
                    bc_loss   0.0000
               dist_entropy   0.7002
         discriminator_loss   1.0287
                  gail_loss   1.0255
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0206
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0423 -0.0432 -0.044  ... -0.0422 -0.0512 -0.0514]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0423 -0.0432 -0.044  ... -0.0422 -0.0512 -0.0514]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 116, num timesteps 239616, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6994
         discriminator_loss   1.0314
                  gail_loss   1.0283
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0132
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0411 -0.0415 -0.0505 ... -0.0446 -0.0508 -0.0445]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0411 -0.0415 -0.0505 ... -0.0446 -0.0508 -0.0445]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 117, num timesteps 241664, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0018
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6971
         discriminator_loss   1.0277
                  gail_loss   1.0246
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0097
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0466 -0.0465 -0.0432 ... -0.0388 -0.0396 -0.0404]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0466 -0.0465 -0.0432 ... -0.0388 -0.0396 -0.0404]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 118, num timesteps 243712, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0008
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.7007
         discriminator_loss   1.0373
                  gail_loss   1.0343
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0046
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0509 -0.0516 -0.0522 ... -0.0529 -0.0532 -0.0535]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0509 -0.0516 -0.0522 ... -0.0529 -0.0532 -0.0535]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 119, num timesteps 245760, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0022
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6980
         discriminator_loss   1.0271
                  gail_loss   1.0237
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0042
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0472 -0.0523 -0.0466 ... -0.0515 -0.0455 -0.0518]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0472 -0.0523 -0.0466 ... -0.0515 -0.0455 -0.0518]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 120, num timesteps 247808, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0125
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6911
         discriminator_loss   1.0505
                  gail_loss   1.0476
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0230
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0568 -0.0569 -0.0569 ... -0.031  -0.0309 -0.0468]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0568 -0.0569 -0.0569 ... -0.031  -0.0309 -0.0468]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 121, num timesteps 249856, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0057
                    bc_loss   0.0000
               dist_entropy   0.6963
         discriminator_loss   1.0271
                  gail_loss   1.0238
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0374
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0415 -0.0418 -0.0423 ... -0.0138 -0.0218 -0.0226]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0415 -0.0418 -0.0423 ... -0.0138 -0.0218 -0.0226]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 122, num timesteps 251904, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0060
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7023
         discriminator_loss   1.0414
                  gail_loss   1.0385
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0165
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0248 -0.0185 -0.0273 ... -0.0436 -0.041  -0.0411]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0248 -0.0185 -0.0273 ... -0.0436 -0.041  -0.0411]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 123, num timesteps 253952, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7026
         discriminator_loss   1.0332
                  gail_loss   1.0304
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0172
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0404 -0.0394 -0.0407 ... -0.0485 -0.0489 -0.0539]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0404 -0.0394 -0.0407 ... -0.0485 -0.0489 -0.0539]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 124, num timesteps 256000, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0088
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.7028
         discriminator_loss   1.0398
                  gail_loss   1.0369
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0210
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0416 -0.0417 -0.0418 ... -0.0487 -0.0486 -0.0485]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0416 -0.0417 -0.0418 ... -0.0487 -0.0486 -0.0485]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 125, num timesteps 258048, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0017
                action_loss  -0.0061
                    bc_loss   0.0000
               dist_entropy   0.6940
         discriminator_loss   1.0261
                  gail_loss   1.0229
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0227
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0447 -0.0448 -0.0399 ... -0.0516 -0.0448 -0.0518]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0447 -0.0448 -0.0399 ... -0.0516 -0.0448 -0.0518]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 126, num timesteps 260096, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss  -0.0103
                    bc_loss   0.0000
               dist_entropy   0.6989
         discriminator_loss   1.0370
                  gail_loss   1.0339
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0084
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0494 -0.0436 -0.0495 ... -0.0485 -0.0488 -0.0492]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0494 -0.0436 -0.0495 ... -0.0485 -0.0488 -0.0492]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 127, num timesteps 262144, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0061
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6961
         discriminator_loss   1.0454
                  gail_loss   1.0427
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0121
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0528 -0.0524 -0.052  ... -0.0498 -0.0337 -0.0506]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0528 -0.0524 -0.052  ... -0.0498 -0.0337 -0.0506]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 128, num timesteps 264192, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.6929
         discriminator_loss   1.0455
                  gail_loss   1.0425
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0144
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0488 -0.0436 -0.0444 ... -0.0356 -0.0365 -0.0392]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0488 -0.0436 -0.0444 ... -0.0356 -0.0365 -0.0392]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 129, num timesteps 266240, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6976
         discriminator_loss   1.0312
                  gail_loss   1.0282
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0117
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0411 -0.0379 -0.0392 ... -0.0437 -0.0444 -0.0452]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0411 -0.0379 -0.0392 ... -0.0437 -0.0444 -0.0452]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 130, num timesteps 268288, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.6848
         discriminator_loss   1.0348
                  gail_loss   1.0319
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0123
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0528 -0.0359 -0.0368 ... -0.0521 -0.051  -0.0498]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0528 -0.0359 -0.0368 ... -0.0521 -0.051  -0.0498]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 131, num timesteps 270336, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0069
                action_loss  -0.0086
                    bc_loss   0.0000
               dist_entropy   0.6942
         discriminator_loss   1.0420
                  gail_loss   1.0393
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0192
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0449 -0.044  -0.0451 ... -0.029  -0.0419 -0.0421]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0449 -0.044  -0.0451 ... -0.029  -0.0419 -0.0421]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 132, num timesteps 272384, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.6928
         discriminator_loss   1.0304
                  gail_loss   1.0274
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0153
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0279 -0.0405 -0.0411 ... -0.0402 -0.0402 -0.0592]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0279 -0.0405 -0.0411 ... -0.0402 -0.0402 -0.0592]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 133, num timesteps 274432, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0094
                action_loss  -0.0059
                    bc_loss   0.0000
               dist_entropy   0.6945
         discriminator_loss   1.0437
                  gail_loss   1.0405
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0220
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0504 -0.0504 -0.0476 ... -0.0442 -0.0445 -0.0474]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0504 -0.0504 -0.0476 ... -0.0442 -0.0445 -0.0474]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 134, num timesteps 276480, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6889
         discriminator_loss   1.0359
                  gail_loss   1.0328
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0216
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.051  -0.0376 -0.0379 ... -0.0562 -0.0392 -0.0558]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.051  -0.0376 -0.0379 ... -0.0562 -0.0392 -0.0558]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 135, num timesteps 278528, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0048
                action_loss  -0.0119
                    bc_loss   0.0000
               dist_entropy   0.6925
         discriminator_loss   1.0359
                  gail_loss   1.0328
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0197
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0498 -0.0498 -0.0535 ... -0.0544 -0.0494 -0.0491]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0498 -0.0498 -0.0535 ... -0.0544 -0.0494 -0.0491]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 136, num timesteps 280576, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0062
                    bc_loss   0.0000
               dist_entropy   0.6842
         discriminator_loss   1.0264
                  gail_loss   1.0236
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0158
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0434 -0.0432 -0.043  ... -0.0431 -0.0513 -0.0511]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0434 -0.0432 -0.043  ... -0.0431 -0.0513 -0.0511]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 137, num timesteps 282624, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.6893
         discriminator_loss   1.0293
                  gail_loss   1.0260
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0195
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0474 -0.0474 -0.0487 ... -0.0478 -0.0456 -0.0479]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0474 -0.0474 -0.0487 ... -0.0478 -0.0456 -0.0479]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 138, num timesteps 284672, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.6852
         discriminator_loss   1.0330
                  gail_loss   1.0303
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0178
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0547 -0.0397 -0.0548 ... -0.0549 -0.0406 -0.0411]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0547 -0.0397 -0.0548 ... -0.0549 -0.0406 -0.0411]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 139, num timesteps 286720, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0056
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6756
         discriminator_loss   1.0383
                  gail_loss   1.0351
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0112
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0412 -0.0544 -0.042  ... -0.0256 -0.0247 -0.0241]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0412 -0.0544 -0.042  ... -0.0256 -0.0247 -0.0241]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 140, num timesteps 288768, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.6849
         discriminator_loss   1.0307
                  gail_loss   1.0272
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0200
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0201 -0.0081 -0.0196 ... -0.0522 -0.0453 -0.0459]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0201 -0.0081 -0.0196 ... -0.0522 -0.0453 -0.0459]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 141, num timesteps 290816, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0060
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6814
         discriminator_loss   1.0408
                  gail_loss   1.0379
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0167
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0519 -0.0525 -0.0531 ... -0.0513 -0.0526 -0.0527]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0519 -0.0525 -0.0531 ... -0.0513 -0.0526 -0.0527]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 142, num timesteps 292864, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0083
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.6774
         discriminator_loss   1.0380
                  gail_loss   1.0352
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0171
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0557 -0.0557 -0.0407 ... -0.0502 -0.0506 -0.051 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0557 -0.0557 -0.0407 ... -0.0502 -0.0506 -0.051 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 143, num timesteps 294912, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0067
                    bc_loss   0.0000
               dist_entropy   0.6829
         discriminator_loss   1.0422
                  gail_loss   1.0393
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0206
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0449 -0.0392 -0.0459 ... -0.041  -0.0417 -0.0494]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0449 -0.0392 -0.0459 ... -0.041  -0.0417 -0.0494]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 144, num timesteps 296960, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0052
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.6768
         discriminator_loss   1.0306
                  gail_loss   1.0276
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0236
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.05   -0.046  -0.0464 ... -0.045  -0.0255 -0.0413]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.05   -0.046  -0.0464 ... -0.045  -0.0255 -0.0413]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 145, num timesteps 299008, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0037
                    bc_loss   0.0000
               dist_entropy   0.6729
         discriminator_loss   1.0242
                  gail_loss   1.0211
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0203
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0292 -0.0249 -0.0235 ... -0.0526 -0.0448 -0.0528]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0292 -0.0249 -0.0235 ... -0.0526 -0.0448 -0.0528]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 146, num timesteps 301056, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0112
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6431
         discriminator_loss   1.0448
                  gail_loss   1.0416
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0452
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0476 -0.0476 -0.0464 ... -0.0379 -0.0376 -0.0434]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0476 -0.0476 -0.0464 ... -0.0379 -0.0376 -0.0434]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 147, num timesteps 303104, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.6664
         discriminator_loss   1.0324
                  gail_loss   1.0297
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0398
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0537 -0.0295 -0.0536 ... -0.056  -0.0394 -0.0551]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0537 -0.0295 -0.0536 ... -0.056  -0.0394 -0.0551]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 148, num timesteps 305152, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0108
                    bc_loss   0.0000
               dist_entropy   0.6536
         discriminator_loss   1.0306
                  gail_loss   1.0272
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0133
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0533 -0.044  -0.0435 ... -0.046  -0.0513 -0.0517]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0533 -0.044  -0.0435 ... -0.046  -0.0513 -0.0517]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 149, num timesteps 307200, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0046
                action_loss   0.0006
                    bc_loss   0.0000
               dist_entropy   0.6491
         discriminator_loss   1.0269
                  gail_loss   1.0240
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0136
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0494 -0.0467 -0.0494 ... -0.0483 -0.0429 -0.0484]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0494 -0.0467 -0.0494 ... -0.0483 -0.0429 -0.0484]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 150, num timesteps 309248, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0042
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.6454
         discriminator_loss   1.0317
                  gail_loss   1.0288
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0169
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0542 -0.0542 -0.0542 ... -0.0419 -0.0412 -0.0391]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0542 -0.0542 -0.0542 ... -0.0419 -0.0412 -0.0391]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 151, num timesteps 311296, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0111
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.5960
         discriminator_loss   1.0530
                  gail_loss   1.0498
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0328
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.033  -0.0524 -0.0522 ... -0.0423 -0.043  -0.0437]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.033  -0.0524 -0.0522 ... -0.0423 -0.043  -0.0437]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 152, num timesteps 313344, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0140
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.5612
         discriminator_loss   1.0395
                  gail_loss   1.0364
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0546
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0491 -0.0497 -0.0502 ... -0.0465 -0.0482 -0.0498]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0491 -0.0497 -0.0502 ... -0.0465 -0.0482 -0.0498]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 153, num timesteps 315392, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.5661
         discriminator_loss   1.0493
                  gail_loss   1.0464
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0371
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.041  -0.0482 -0.0455 ... -0.0491 -0.0467 -0.0488]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.041  -0.0482 -0.0455 ... -0.0491 -0.0467 -0.0488]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 154, num timesteps 317440, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0041
                action_loss  -0.0048
                    bc_loss   0.0000
               dist_entropy   0.6106
         discriminator_loss   1.0224
                  gail_loss   1.0193
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0140
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0338 -0.055  -0.0545 ... -0.0431 -0.0519 -0.0518]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0338 -0.055  -0.0545 ... -0.0431 -0.0519 -0.0518]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 155, num timesteps 319488, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0098
                    bc_loss   0.0000
               dist_entropy   0.6143
         discriminator_loss   1.0301
                  gail_loss   1.0267
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0147
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0525 -0.0431 -0.0431 ... -0.044  -0.0442 -0.0515]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0525 -0.0431 -0.0431 ... -0.044  -0.0442 -0.0515]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 156, num timesteps 321536, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6170
         discriminator_loss   1.0264
                  gail_loss   1.0233
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0076
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0442 -0.0541 -0.0438 ... -0.0444 -0.0453 -0.046 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0442 -0.0541 -0.0438 ... -0.0444 -0.0453 -0.046 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 157, num timesteps 323584, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0093
                action_loss  -0.0043
                    bc_loss   0.0000
               dist_entropy   0.5591
         discriminator_loss   1.0406
                  gail_loss   1.0374
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0192
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0572 -0.0575 -0.0576 ... -0.0567 -0.0308 -0.0566]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0572 -0.0575 -0.0576 ... -0.0567 -0.0308 -0.0566]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 158, num timesteps 325632, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0071
                    bc_loss   0.0000
               dist_entropy   0.5721
         discriminator_loss   1.0277
                  gail_loss   1.0246
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0329
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0438 -0.0438 -0.0438 ... -0.0452 -0.0442 -0.0453]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0438 -0.0438 -0.0438 ... -0.0452 -0.0442 -0.0453]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 159, num timesteps 327680, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.5803
         discriminator_loss   1.0342
                  gail_loss   1.0311
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0188
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0547 -0.0376 -0.0545 ... -0.0452 -0.0451 -0.0466]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0547 -0.0376 -0.0545 ... -0.0452 -0.0451 -0.0466]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 160, num timesteps 329728, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0112
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.5164
         discriminator_loss   1.0517
                  gail_loss   1.0490
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0247
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.055  -0.055  -0.0551 ... -0.0551 -0.0551 -0.0551]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.055  -0.055  -0.0551 ... -0.0551 -0.0551 -0.0551]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 161, num timesteps 331776, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0086
                    bc_loss   0.0000
               dist_entropy   0.6196
         discriminator_loss   1.0296
                  gail_loss   1.0263
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0356
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0356 -0.0432 -0.0355 ... -0.0184 -0.0546 -0.0541]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0356 -0.0432 -0.0355 ... -0.0184 -0.0546 -0.0541]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 162, num timesteps 333824, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0129
                    bc_loss   0.0000
               dist_entropy   0.6063
         discriminator_loss   1.0364
                  gail_loss   1.0335
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0073
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0363 -0.0349 -0.0354 ... -0.0556 -0.0364 -0.0552]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0363 -0.0349 -0.0354 ... -0.0556 -0.0364 -0.0552]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 163, num timesteps 335872, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.6050
         discriminator_loss   1.0319
                  gail_loss   1.0284
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0201
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0462 -0.0484 -0.0488 ... -0.0471 -0.0475 -0.0479]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0462 -0.0484 -0.0488 ... -0.0471 -0.0475 -0.0479]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 164, num timesteps 337920, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0064
                    bc_loss   0.0000
               dist_entropy   0.5726
         discriminator_loss   1.0438
                  gail_loss   1.0410
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0179
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0393 -0.0398 -0.0355 ... -0.0555 -0.0557 -0.0558]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0393 -0.0398 -0.0355 ... -0.0555 -0.0557 -0.0558]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 165, num timesteps 339968, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0060
                action_loss  -0.0040
                    bc_loss   0.0000
               dist_entropy   0.5658
         discriminator_loss   1.0326
                  gail_loss   1.0296
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0253
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0437 -0.0451 -0.0464 ... -0.0471 -0.0414 -0.0471]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0437 -0.0451 -0.0464 ... -0.0471 -0.0414 -0.0471]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 166, num timesteps 342016, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0048
                action_loss  -0.0065
                    bc_loss   0.0000
               dist_entropy   0.5762
         discriminator_loss   1.0401
                  gail_loss   1.0371
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0198
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.058  -0.058  -0.058  ... -0.0453 -0.0454 -0.0455]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.058  -0.058  -0.058  ... -0.0453 -0.0454 -0.0455]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 167, num timesteps 344064, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0108
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.5653
         discriminator_loss   1.0368
                  gail_loss   1.0334
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0240
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0492 -0.0493 -0.0396 ... -0.0483 -0.0431 -0.0426]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0492 -0.0493 -0.0396 ... -0.0483 -0.0431 -0.0426]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 168, num timesteps 346112, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0011
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.6220
         discriminator_loss   1.0297
                  gail_loss   1.0265
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0346
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0484 -0.0452 -0.0478 ... -0.0542 -0.0545 -0.0549]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0484 -0.0452 -0.0478 ... -0.0542 -0.0545 -0.0549]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 169, num timesteps 348160, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0097
                    bc_loss   0.0000
               dist_entropy   0.6178
         discriminator_loss   1.0230
                  gail_loss   1.0200
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0067
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0475 -0.0477 -0.0479 ... -0.0499 -0.0456 -0.0482]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0475 -0.0477 -0.0479 ... -0.0499 -0.0456 -0.0482]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 170, num timesteps 350208, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6087
         discriminator_loss   1.0336
                  gail_loss   1.0304
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0166
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0439 -0.0443 -0.0481 ... -0.0313 -0.0301 -0.054 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0439 -0.0443 -0.0481 ... -0.0313 -0.0301 -0.054 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 171, num timesteps 352256, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0032
                action_loss  -0.0071
                    bc_loss   0.0000
               dist_entropy   0.6154
         discriminator_loss   1.0322
                  gail_loss   1.0287
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0094
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0447 -0.0342 -0.0434 ... -0.014  -0.0138 -0.0361]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0447 -0.0342 -0.0434 ... -0.014  -0.0138 -0.0361]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 172, num timesteps 354304, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0025
                    bc_loss   0.0000
               dist_entropy   0.6287
         discriminator_loss   1.0393
                  gail_loss   1.0364
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0122
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0242 -0.0245 -0.0249 ... -0.0549 -0.0297 -0.0536]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0242 -0.0245 -0.0249 ... -0.0549 -0.0297 -0.0536]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 173, num timesteps 356352, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0094
                    bc_loss   0.0000
               dist_entropy   0.6304
         discriminator_loss   1.0310
                  gail_loss   1.0277
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0117
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0422 -0.0407 -0.0366 ... -0.0328 -0.0311 -0.0267]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0422 -0.0407 -0.0366 ... -0.0328 -0.0311 -0.0267]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 174, num timesteps 358400, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0087
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.5252
         discriminator_loss   1.0405
                  gail_loss   1.0373
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0230
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0399 -0.04   -0.0505 ... -0.0508 -0.0508 -0.0508]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0399 -0.04   -0.0505 ... -0.0508 -0.0508 -0.0508]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 175, num timesteps 360448, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0011
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6513
         discriminator_loss   1.0309
                  gail_loss   1.0280
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0234
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0473 -0.0433 -0.0418 ... -0.0386 -0.038  -0.0375]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0473 -0.0433 -0.0418 ... -0.0386 -0.038  -0.0375]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 176, num timesteps 362496, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0077
                    bc_loss   0.0000
               dist_entropy   0.5739
         discriminator_loss   1.0450
                  gail_loss   1.0417
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0137
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0467 -0.0461 -0.0274 ... -0.0449 -0.0442 -0.0436]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0467 -0.0461 -0.0274 ... -0.0449 -0.0442 -0.0436]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 177, num timesteps 364544, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0043
                    bc_loss   0.0000
               dist_entropy   0.6240
         discriminator_loss   1.0237
                  gail_loss   1.0206
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0201
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0506 -0.05   -0.0495 ... -0.0559 -0.037  -0.0375]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0506 -0.05   -0.0495 ... -0.0559 -0.037  -0.0375]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 178, num timesteps 366592, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0023
                action_loss  -0.0124
                    bc_loss   0.0000
               dist_entropy   0.6566
         discriminator_loss   1.0169
                  gail_loss   1.0130
                  grad_loss   0.0039
                    ib_loss   0.0000
                  task_loss   0.0105
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0345 -0.0339 -0.0576 ... -0.0375 -0.0373 -0.0434]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0345 -0.0339 -0.0576 ... -0.0375 -0.0373 -0.0434]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 179, num timesteps 368640, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0021
                action_loss  -0.0257
                    bc_loss   0.0000
               dist_entropy   0.7045
         discriminator_loss   1.0152
                  gail_loss   1.0120
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0046
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0598 -0.0248 -0.0246 ... -0.0355 -0.0377 -0.0392]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0598 -0.0248 -0.0246 ... -0.0355 -0.0377 -0.0392]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 180, num timesteps 370688, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0061
                action_loss  -0.0061
                    bc_loss   0.0000
               dist_entropy   0.6465
         discriminator_loss   1.0244
                  gail_loss   1.0206
                  grad_loss   0.0038
                    ib_loss   0.0000
                  task_loss   0.0182
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0552 -0.0551 -0.054  ... -0.0344 -0.0338 -0.0333]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0552 -0.0551 -0.054  ... -0.0344 -0.0338 -0.0333]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 181, num timesteps 372736, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss   0.0002
                    bc_loss   0.0000
               dist_entropy   0.6488
         discriminator_loss   1.0359
                  gail_loss   1.0326
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0244
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.037  -0.0145 -0.0366 ... -0.0345 -0.0444 -0.0439]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.037  -0.0145 -0.0366 ... -0.0345 -0.0444 -0.0439]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 182, num timesteps 374784, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0042
                action_loss  -0.0038
                    bc_loss   0.0000
               dist_entropy   0.6541
         discriminator_loss   1.0331
                  gail_loss   1.0301
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0204
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0509 -0.0505 -0.0501 ... -0.0544 -0.0298 -0.0546]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0509 -0.0505 -0.0501 ... -0.0544 -0.0298 -0.0546]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 183, num timesteps 376832, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0028
                action_loss  -0.0072
                    bc_loss   0.0000
               dist_entropy   0.6994
         discriminator_loss   1.0369
                  gail_loss   1.0340
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0108
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0419 -0.0413 -0.0408 ... -0.0371 -0.0369 -0.0366]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0419 -0.0413 -0.0408 ... -0.0371 -0.0369 -0.0366]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 184, num timesteps 378880, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0084
                    bc_loss   0.0000
               dist_entropy   0.7386
         discriminator_loss   1.0434
                  gail_loss   1.0400
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0100
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0303 -0.0534 -0.0293 ... -0.0421 -0.0426 -0.0508]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0303 -0.0534 -0.0293 ... -0.0421 -0.0426 -0.0508]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 185, num timesteps 380928, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss  -0.0069
                    bc_loss   0.0000
               dist_entropy   0.7439
         discriminator_loss   1.0266
                  gail_loss   1.0234
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0082
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0523 -0.0527 -0.0531 ... -0.0524 -0.0509 -0.0457]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0523 -0.0527 -0.0531 ... -0.0524 -0.0509 -0.0457]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 186, num timesteps 382976, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6959
         discriminator_loss   1.0308
                  gail_loss   1.0276
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0117
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0404 -0.0567 -0.0566 ... -0.0542 -0.0533 -0.0278]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0404 -0.0567 -0.0566 ... -0.0542 -0.0533 -0.0278]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 187, num timesteps 385024, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0087
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.6560
         discriminator_loss   1.0369
                  gail_loss   1.0339
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0185
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0362 -0.0354 -0.0346 ... -0.0558 -0.056  -0.0561]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0362 -0.0354 -0.0346 ... -0.0558 -0.056  -0.0561]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 188, num timesteps 387072, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0049
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.6517
         discriminator_loss   1.0369
                  gail_loss   1.0335
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0240
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0464 -0.0461 -0.0453 ... -0.0506 -0.05   -0.0495]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0464 -0.0461 -0.0453 ... -0.0506 -0.05   -0.0495]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 189, num timesteps 389120, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0090
                action_loss  -0.0045
                    bc_loss   0.0000
               dist_entropy   0.5812
         discriminator_loss   1.0336
                  gail_loss   1.0306
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0298
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.032  -0.0316 -0.0313 ... -0.0504 -0.0248 -0.0513]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.032  -0.0316 -0.0313 ... -0.0504 -0.0248 -0.0513]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 190, num timesteps 391168, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0121
                    bc_loss   0.0000
               dist_entropy   0.6703
         discriminator_loss   1.0273
                  gail_loss   1.0240
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0231
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0408 -0.0436 -0.0428 ... -0.0446 -0.0538 -0.0437]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0408 -0.0436 -0.0428 ... -0.0446 -0.0538 -0.0437]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 191, num timesteps 393216, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0079
                action_loss  -0.0111
                    bc_loss   0.0000
               dist_entropy   0.5433
         discriminator_loss   1.0341
                  gail_loss   1.0311
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0148
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0487 -0.0482 -0.0477 ... -0.0513 -0.0511 -0.0508]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0487 -0.0482 -0.0477 ... -0.0513 -0.0511 -0.0508]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 192, num timesteps 395264, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.5585
         discriminator_loss   1.0409
                  gail_loss   1.0381
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0232
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0337 -0.0515 -0.0523 ... -0.0592 -0.059  -0.0588]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0337 -0.0515 -0.0523 ... -0.0592 -0.059  -0.0588]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 193, num timesteps 397312, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0073
                    bc_loss   0.0000
               dist_entropy   0.5693
         discriminator_loss   1.0275
                  gail_loss   1.0239
                  grad_loss   0.0036
                    ib_loss   0.0000
                  task_loss   0.0202
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0427 -0.0422 -0.0416 ... -0.0421 -0.0499 -0.049 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0427 -0.0422 -0.0416 ... -0.0421 -0.0499 -0.049 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 194, num timesteps 399360, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0061
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.5920
         discriminator_loss   1.0365
                  gail_loss   1.0335
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0188
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0498 -0.0502 -0.0443 ... -0.0339 -0.0354 -0.0233]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0498 -0.0502 -0.0443 ... -0.0339 -0.0354 -0.0233]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 195, num timesteps 401408, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0032
                    bc_loss   0.0000
               dist_entropy   0.6113
         discriminator_loss   1.0164
                  gail_loss   1.0131
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0211
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0504 -0.0522 -0.0539 ... -0.0411 -0.0411 -0.0519]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0504 -0.0522 -0.0539 ... -0.0411 -0.0411 -0.0519]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 196, num timesteps 403456, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0060
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.5763
         discriminator_loss   1.0233
                  gail_loss   1.0200
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0165
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.042  -0.0423 -0.0509 ... -0.0499 -0.0499 -0.0499]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.042  -0.0423 -0.0509 ... -0.0499 -0.0499 -0.0499]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 197, num timesteps 405504, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.5938
         discriminator_loss   1.0263
                  gail_loss   1.0234
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0196
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0396 -0.0395 -0.0395 ... -0.0353 -0.0351 -0.035 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0396 -0.0395 -0.0395 ... -0.0353 -0.0351 -0.035 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 198, num timesteps 407552, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss  -0.0088
                    bc_loss   0.0000
               dist_entropy   0.6062
         discriminator_loss   1.0237
                  gail_loss   1.0207
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0087
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0224 -0.0226 -0.023  ... -0.0515 -0.0514 -0.0513]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0224 -0.0226 -0.023  ... -0.0515 -0.0514 -0.0513]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 199, num timesteps 409600, FPS 118 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0078
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.5217
         discriminator_loss   1.0376
                  gail_loss   1.0345
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0239
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0454 -0.0454 -0.0454 ... -0.0515 -0.0522 -0.0529]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0454 -0.0454 -0.0454 ... -0.0515 -0.0522 -0.0529]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 200, num timesteps 411648, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.5286
         discriminator_loss   1.0410
                  gail_loss   1.0381
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0257
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0501 -0.0475 -0.0466 ... -0.0518 -0.0519 -0.0519]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0501 -0.0475 -0.0466 ... -0.0518 -0.0519 -0.0519]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 201, num timesteps 413696, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0065
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.5319
         discriminator_loss   1.0275
                  gail_loss   1.0246
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0209
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0471 -0.0472 -0.0429 ... -0.0518 -0.0525 -0.0398]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0471 -0.0472 -0.0429 ... -0.0518 -0.0525 -0.0398]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 202, num timesteps 415744, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0010
                action_loss  -0.0159
                    bc_loss   0.0000
               dist_entropy   0.6399
         discriminator_loss   1.0209
                  gail_loss   1.0177
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0177
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.04   -0.0511 -0.0407 ... -0.0322 -0.0339 -0.0345]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.04   -0.0511 -0.0407 ... -0.0322 -0.0339 -0.0345]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 203, num timesteps 417792, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0045
                action_loss  -0.0072
                    bc_loss   0.0000
               dist_entropy   0.5943
         discriminator_loss   1.0229
                  gail_loss   1.0198
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0064
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0417 -0.0425 -0.0433 ... -0.0503 -0.03   -0.0503]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0417 -0.0425 -0.0433 ... -0.0503 -0.03   -0.0503]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 204, num timesteps 419840, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0045
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.6221
         discriminator_loss   1.0410
                  gail_loss   1.0381
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0159
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0444 -0.0445 -0.0446 ... -0.0455 -0.046  -0.0463]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0444 -0.0445 -0.0446 ... -0.0455 -0.046  -0.0463]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 205, num timesteps 421888, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0071
                    bc_loss   0.0000
               dist_entropy   0.6197
         discriminator_loss   1.0337
                  gail_loss   1.0307
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0164
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0579 -0.0577 -0.0575 ... -0.0188 -0.0177 -0.0495]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0579 -0.0577 -0.0575 ... -0.0188 -0.0177 -0.0495]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 206, num timesteps 423936, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0050
                    bc_loss   0.0000
               dist_entropy   0.5800
         discriminator_loss   1.0472
                  gail_loss   1.0444
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0165
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0416 -0.041  -0.0224 ... -0.0329 -0.0315 -0.0302]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0416 -0.041  -0.0224 ... -0.0329 -0.0315 -0.0302]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 207, num timesteps 425984, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6218
         discriminator_loss   1.0345
                  gail_loss   1.0314
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0164
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.035  -0.034  -0.0331 ... -0.045  -0.0512 -0.0509]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.035  -0.034  -0.0331 ... -0.045  -0.0512 -0.0509]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 208, num timesteps 428032, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0045
                action_loss  -0.0065
                    bc_loss   0.0000
               dist_entropy   0.6234
         discriminator_loss   1.0293
                  gail_loss   1.0260
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0167
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0562 -0.0435 -0.0433 ... -0.0495 -0.0495 -0.0495]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0562 -0.0435 -0.0433 ... -0.0495 -0.0495 -0.0495]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 209, num timesteps 430080, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0133
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.5675
         discriminator_loss   1.0415
                  gail_loss   1.0385
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0412
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0545 -0.0545 -0.0351 ... -0.0491 -0.0445 -0.0442]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0545 -0.0545 -0.0351 ... -0.0491 -0.0445 -0.0442]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 210, num timesteps 432128, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6016
         discriminator_loss   1.0353
                  gail_loss   1.0325
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0457
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0462 -0.0462 -0.0425 ... -0.0569 -0.0569 -0.0568]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0462 -0.0462 -0.0425 ... -0.0569 -0.0569 -0.0568]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 211, num timesteps 434176, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0136
                    bc_loss   0.0000
               dist_entropy   0.6226
         discriminator_loss   1.0316
                  gail_loss   1.0286
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0089
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0458 -0.0454 -0.045  ... -0.0449 -0.046  -0.0464]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0458 -0.0454 -0.045  ... -0.0449 -0.046  -0.0464]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 212, num timesteps 436224, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0021
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6492
         discriminator_loss   1.0388
                  gail_loss   1.0358
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0150
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0568 -0.0565 -0.0353 ... -0.0268 -0.0369 -0.0365]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0568 -0.0565 -0.0353 ... -0.0268 -0.0369 -0.0365]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 213, num timesteps 438272, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0080
                action_loss  -0.0074
                    bc_loss   0.0000
               dist_entropy   0.6098
         discriminator_loss   1.0370
                  gail_loss   1.0343
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0108
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0177 -0.0418 -0.0418 ... -0.0209 -0.0447 -0.0444]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0177 -0.0418 -0.0418 ... -0.0209 -0.0447 -0.0444]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 214, num timesteps 440320, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.6204
         discriminator_loss   1.0381
                  gail_loss   1.0350
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0283
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0273 -0.0397 -0.0398 ... -0.0518 -0.0514 -0.051 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0273 -0.0397 -0.0398 ... -0.0518 -0.0514 -0.051 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 215, num timesteps 442368, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0056
                    bc_loss   0.0000
               dist_entropy   0.6179
         discriminator_loss   1.0212
                  gail_loss   1.0182
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0165
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.038  -0.0376 -0.0372 ... -0.0569 -0.0569 -0.057 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.038  -0.0376 -0.0372 ... -0.0569 -0.0569 -0.057 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 216, num timesteps 444416, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0072
                    bc_loss   0.0000
               dist_entropy   0.5899
         discriminator_loss   1.0344
                  gail_loss   1.0314
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0158
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0461 -0.0431 -0.0456 ... -0.056  -0.056  -0.056 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0461 -0.0431 -0.0456 ... -0.056  -0.056  -0.056 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 217, num timesteps 446464, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0133
                    bc_loss   0.0000
               dist_entropy   0.6163
         discriminator_loss   1.0204
                  gail_loss   1.0173
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0145
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0477 -0.048  -0.0434 ... -0.0573 -0.0573 -0.0365]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0477 -0.048  -0.0434 ... -0.0573 -0.0573 -0.0365]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 218, num timesteps 448512, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0019
                action_loss  -0.0112
                    bc_loss   0.0000
               dist_entropy   0.6406
         discriminator_loss   1.0192
                  gail_loss   1.0159
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0075
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0443 -0.0442 -0.0492 ... -0.0078 -0.025  -0.0258]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0443 -0.0442 -0.0492 ... -0.0078 -0.025  -0.0258]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 219, num timesteps 450560, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.6107
         discriminator_loss   1.0328
                  gail_loss   1.0298
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0084
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0357 -0.0367 -0.0377 ... -0.049  -0.0174 -0.0488]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0357 -0.0367 -0.0377 ... -0.049  -0.0174 -0.0488]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 220, num timesteps 452608, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0063
                action_loss  -0.0055
                    bc_loss   0.0000
               dist_entropy   0.5528
         discriminator_loss   1.0422
                  gail_loss   1.0392
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0223
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0433 -0.0434 -0.0437 ... -0.0374 -0.0376 -0.0379]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0433 -0.0434 -0.0437 ... -0.0374 -0.0376 -0.0379]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 221, num timesteps 454656, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss  -0.0045
                    bc_loss   0.0000
               dist_entropy   0.5946
         discriminator_loss   1.0362
                  gail_loss   1.0333
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0165
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0325 -0.033  -0.0336 ... -0.0109 -0.0188 -0.0198]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0325 -0.033  -0.0336 ... -0.0109 -0.0188 -0.0198]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 222, num timesteps 456704, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.5681
         discriminator_loss   1.0322
                  gail_loss   1.0293
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0139
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0292 -0.0304 -0.0319 ... -0.0354 -0.0537 -0.0537]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0292 -0.0304 -0.0319 ... -0.0354 -0.0537 -0.0537]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 223, num timesteps 458752, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0052
                    bc_loss   0.0000
               dist_entropy   0.5877
         discriminator_loss   1.0195
                  gail_loss   1.0163
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0236
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0376 -0.0376 -0.0422 ... -0.0394 -0.0395 -0.0395]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0376 -0.0376 -0.0422 ... -0.0394 -0.0395 -0.0395]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 224, num timesteps 460800, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0094
                action_loss  -0.0097
                    bc_loss   0.0000
               dist_entropy   0.5484
         discriminator_loss   1.0372
                  gail_loss   1.0341
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0271
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0481 -0.0481 -0.0409 ... -0.0549 -0.055  -0.0551]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0481 -0.0481 -0.0409 ... -0.0549 -0.055  -0.0551]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 225, num timesteps 462848, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0084
                action_loss   0.0004
                    bc_loss   0.0000
               dist_entropy   0.4881
         discriminator_loss   1.0456
                  gail_loss   1.0426
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0381
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0532 -0.0533 -0.0534 ... -0.042  -0.0482 -0.0409]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0532 -0.0533 -0.0534 ... -0.042  -0.0482 -0.0409]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 226, num timesteps 464896, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.6135
         discriminator_loss   1.0248
                  gail_loss   1.0216
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0212
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0303 -0.0562 -0.0561 ... -0.0525 -0.0398 -0.0403]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0303 -0.0562 -0.0561 ... -0.0525 -0.0398 -0.0403]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 227, num timesteps 466944, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0021
                action_loss  -0.0094
                    bc_loss   0.0000
               dist_entropy   0.6165
         discriminator_loss   1.0281
                  gail_loss   1.0250
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0055
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0574 -0.0579 -0.0389 ... -0.0358 -0.0381 -0.0348]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0574 -0.0579 -0.0389 ... -0.0358 -0.0381 -0.0348]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 228, num timesteps 468992, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0098
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.5585
         discriminator_loss   1.0367
                  gail_loss   1.0335
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0219
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0489 -0.0211 -0.0212 ... -0.0442 -0.0515 -0.0431]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0489 -0.0211 -0.0212 ... -0.0442 -0.0515 -0.0431]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 229, num timesteps 471040, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.6096
         discriminator_loss   1.0269
                  gail_loss   1.0237
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0316
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.05   -0.0487 -0.0487 ... -0.0477 -0.0485 -0.0487]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.05   -0.0487 -0.0487 ... -0.0477 -0.0485 -0.0487]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 230, num timesteps 473088, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6105
         discriminator_loss   1.0376
                  gail_loss   1.0347
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0158
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0425 -0.0431 -0.0436 ... -0.0511 -0.0523 -0.0269]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0425 -0.0431 -0.0436 ... -0.0511 -0.0523 -0.0269]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 231, num timesteps 475136, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss  -0.0116
                    bc_loss   0.0000
               dist_entropy   0.6322
         discriminator_loss   1.0367
                  gail_loss   1.0337
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0217
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0382 -0.0456 -0.0462 ... -0.0445 -0.0448 -0.0393]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0382 -0.0456 -0.0462 ... -0.0445 -0.0448 -0.0393]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 232, num timesteps 477184, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.6462
         discriminator_loss   1.0294
                  gail_loss   1.0258
                  grad_loss   0.0036
                    ib_loss   0.0000
                  task_loss   0.0150
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0518 -0.052  -0.0522 ... -0.0468 -0.0469 -0.0472]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0518 -0.052  -0.0522 ... -0.0468 -0.0469 -0.0472]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 233, num timesteps 479232, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0023
                action_loss  -0.0053
                    bc_loss   0.0000
               dist_entropy   0.6522
         discriminator_loss   1.0294
                  gail_loss   1.0265
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0076
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.045  -0.0542 -0.0539 ... -0.0479 -0.0482 -0.0492]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.045  -0.0542 -0.0539 ... -0.0479 -0.0482 -0.0492]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 234, num timesteps 481280, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0042
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.6254
         discriminator_loss   1.0272
                  gail_loss   1.0242
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0106
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0515 -0.045  -0.0512 ... -0.0517 -0.0513 -0.0463]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0515 -0.045  -0.0512 ... -0.0517 -0.0513 -0.0463]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 235, num timesteps 483328, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.6183
         discriminator_loss   1.0262
                  gail_loss   1.0232
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0124
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0574 -0.0585 -0.0383 ... -0.064  -0.0304 -0.0641]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0574 -0.0585 -0.0383 ... -0.064  -0.0304 -0.0641]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 236, num timesteps 485376, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0128
                action_loss  -0.0079
                    bc_loss   0.0000
               dist_entropy   0.5580
         discriminator_loss   1.0303
                  gail_loss   1.0272
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0251
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0521 -0.052  -0.0386 ... -0.0401 -0.0229 -0.0421]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0521 -0.052  -0.0386 ... -0.0401 -0.0229 -0.0421]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 237, num timesteps 487424, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.5816
         discriminator_loss   1.0422
                  gail_loss   1.0392
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0376
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0485 -0.0496 -0.0224 ... -0.0379 -0.0379 -0.0559]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0485 -0.0496 -0.0224 ... -0.0379 -0.0379 -0.0559]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 238, num timesteps 489472, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0099
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.6036
         discriminator_loss   1.0428
                  gail_loss   1.0399
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0332
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0472 -0.0474 -0.0477 ... -0.0529 -0.0514 -0.0495]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0472 -0.0474 -0.0477 ... -0.0529 -0.0514 -0.0495]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 239, num timesteps 491520, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0056
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6054
         discriminator_loss   1.0325
                  gail_loss   1.0294
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0363
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0421 -0.0401 -0.0381 ... -0.0505 -0.0401 -0.0385]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0421 -0.0401 -0.0381 ... -0.0505 -0.0401 -0.0385]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 240, num timesteps 493568, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0035
                    bc_loss   0.0000
               dist_entropy   0.6132
         discriminator_loss   1.0329
                  gail_loss   1.0300
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0231
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0349 -0.0341 -0.0332 ... -0.0587 -0.0246 -0.0248]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0349 -0.0341 -0.0332 ... -0.0587 -0.0246 -0.0248]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 241, num timesteps 495616, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss  -0.0053
                    bc_loss   0.0000
               dist_entropy   0.5843
         discriminator_loss   1.0464
                  gail_loss   1.0433
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0269
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0587 -0.0588 -0.0587 ... -0.041  -0.0405 -0.0544]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0587 -0.0588 -0.0587 ... -0.041  -0.0405 -0.0544]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 242, num timesteps 497664, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0073
                action_loss  -0.0102
                    bc_loss   0.0000
               dist_entropy   0.5993
         discriminator_loss   1.0329
                  gail_loss   1.0299
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0171
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0515 -0.0509 -0.0503 ... -0.0299 -0.0322 -0.0355]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0515 -0.0509 -0.0503 ... -0.0299 -0.0322 -0.0355]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 243, num timesteps 499712, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0098
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.5917
         discriminator_loss   1.0430
                  gail_loss   1.0400
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0269
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0298 -0.0313 -0.0331 ... -0.0498 -0.0495 -0.0492]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0298 -0.0313 -0.0331 ... -0.0498 -0.0495 -0.0492]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 244, num timesteps 501760, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0084
                action_loss  -0.0036
                    bc_loss   0.0000
               dist_entropy   0.5895
         discriminator_loss   1.0275
                  gail_loss   1.0246
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0449
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0461 -0.0468 -0.0491 ... -0.0412 -0.052  -0.0517]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0461 -0.0468 -0.0491 ... -0.0412 -0.052  -0.0517]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 245, num timesteps 503808, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.6014
         discriminator_loss   1.0349
                  gail_loss   1.0321
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0255
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0411 -0.0421 -0.0523 ... -0.0463 -0.0485 -0.0487]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0411 -0.0421 -0.0523 ... -0.0463 -0.0485 -0.0487]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 246, num timesteps 505856, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0069
                    bc_loss   0.0000
               dist_entropy   0.6275
         discriminator_loss   1.0239
                  gail_loss   1.0209
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0125
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0555 -0.0553 -0.0552 ... -0.0395 -0.0389 -0.0353]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0555 -0.0553 -0.0552 ... -0.0395 -0.0389 -0.0353]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 247, num timesteps 507904, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0088
                    bc_loss   0.0000
               dist_entropy   0.6162
         discriminator_loss   1.0270
                  gail_loss   1.0239
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0063
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.053  -0.0527 -0.0523 ... -0.0418 -0.0515 -0.052 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.053  -0.0527 -0.0523 ... -0.0418 -0.0515 -0.052 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 248, num timesteps 509952, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0026
                action_loss  -0.0113
                    bc_loss   0.0000
               dist_entropy   0.6383
         discriminator_loss   1.0150
                  gail_loss   1.0115
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0115
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0564 -0.0561 -0.0558 ... -0.0352 -0.0357 -0.0363]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0564 -0.0561 -0.0558 ... -0.0352 -0.0357 -0.0363]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 249, num timesteps 512000, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0045
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.6129
         discriminator_loss   1.0331
                  gail_loss   1.0302
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0110
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0531 -0.0397 -0.0532 ... -0.0477 -0.0481 -0.0513]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0531 -0.0397 -0.0532 ... -0.0477 -0.0481 -0.0513]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 250, num timesteps 514048, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0041
                action_loss   0.0003
                    bc_loss   0.0000
               dist_entropy   0.6089
         discriminator_loss   1.0204
                  gail_loss   1.0173
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0198
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0434 -0.0438 -0.0511 ... -0.0396 -0.0397 -0.0542]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0434 -0.0438 -0.0511 ... -0.0396 -0.0397 -0.0542]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 251, num timesteps 516096, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0060
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.5933
         discriminator_loss   1.0475
                  gail_loss   1.0445
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0184
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0542 -0.0398 -0.0546 ... -0.0611 -0.0608 -0.0606]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0542 -0.0398 -0.0546 ... -0.0611 -0.0608 -0.0606]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 252, num timesteps 518144, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0049
                action_loss  -0.0117
                    bc_loss   0.0000
               dist_entropy   0.6160
         discriminator_loss   1.0275
                  gail_loss   1.0244
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0206
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0508 -0.0515 -0.0468 ... -0.0362 -0.0127 -0.0362]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0508 -0.0515 -0.0468 ... -0.0362 -0.0127 -0.0362]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 253, num timesteps 520192, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0062
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.5998
         discriminator_loss   1.0362
                  gail_loss   1.0330
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0198
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0302 -0.0305 -0.0309 ... -0.0502 -0.0509 -0.0444]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0302 -0.0305 -0.0309 ... -0.0502 -0.0509 -0.0444]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 254, num timesteps 522240, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6182
         discriminator_loss   1.0389
                  gail_loss   1.0359
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0176
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.056  -0.0563 -0.0565 ... -0.0562 -0.056  -0.0558]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.056  -0.0563 -0.0565 ... -0.0562 -0.056  -0.0558]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 255, num timesteps 524288, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6296
         discriminator_loss   1.0212
                  gail_loss   1.0182
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0126
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0479 -0.0477 -0.0475 ... -0.051  -0.0519 -0.0527]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0479 -0.0477 -0.0475 ... -0.051  -0.0519 -0.0527]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 256, num timesteps 526336, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.6308
         discriminator_loss   1.0351
                  gail_loss   1.0323
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0139
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0416 -0.0468 -0.0427 ... -0.0355 -0.0355 -0.0355]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0416 -0.0468 -0.0427 ... -0.0355 -0.0355 -0.0355]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 257, num timesteps 528384, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0056
                action_loss  -0.0160
                    bc_loss   0.0000
               dist_entropy   0.6208
         discriminator_loss   1.0274
                  gail_loss   1.0244
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0127
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0274 -0.0521 -0.0523 ... -0.0439 -0.0443 -0.0562]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0274 -0.0521 -0.0523 ... -0.0439 -0.0443 -0.0562]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 258, num timesteps 530432, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0109
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.5970
         discriminator_loss   1.0316
                  gail_loss   1.0284
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0350
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0585 -0.0581 -0.0579 ... -0.0536 -0.0535 -0.0535]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0585 -0.0581 -0.0579 ... -0.0536 -0.0535 -0.0535]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 259, num timesteps 532480, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0189
                    bc_loss   0.0000
               dist_entropy   0.6289
         discriminator_loss   1.0238
                  gail_loss   1.0203
                  grad_loss   0.0036
                    ib_loss   0.0000
                  task_loss   0.0292
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0495 -0.0495 -0.0496 ... -0.0455 -0.0513 -0.0519]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0495 -0.0495 -0.0496 ... -0.0455 -0.0513 -0.0519]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 260, num timesteps 534528, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0084
                action_loss  -0.0083
                    bc_loss   0.0000
               dist_entropy   0.6044
         discriminator_loss   1.0348
                  gail_loss   1.0321
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0193
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0576 -0.0572 -0.0567 ... -0.0592 -0.059  -0.0589]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0576 -0.0572 -0.0567 ... -0.0592 -0.059  -0.0589]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 261, num timesteps 536576, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0154
                action_loss  -0.0056
                    bc_loss   0.0000
               dist_entropy   0.5439
         discriminator_loss   1.0427
                  gail_loss   1.0397
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0530
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0616 -0.0614 -0.0612 ... -0.0549 -0.0266 -0.052 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0616 -0.0614 -0.0612 ... -0.0549 -0.0266 -0.052 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 262, num timesteps 538624, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0230
                    bc_loss   0.0000
               dist_entropy   0.6348
         discriminator_loss   1.0163
                  gail_loss   1.0130
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0480
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0439 -0.0427 -0.0522 ... -0.0502 -0.0525 -0.053 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0439 -0.0427 -0.0522 ... -0.0502 -0.0525 -0.053 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 263, num timesteps 540672, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0080
                action_loss  -0.0058
                    bc_loss   0.0000
               dist_entropy   0.6405
         discriminator_loss   1.0279
                  gail_loss   1.0250
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0227
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0423 -0.0534 -0.0529 ... -0.0437 -0.0436 -0.0435]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0423 -0.0534 -0.0529 ... -0.0437 -0.0436 -0.0435]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 264, num timesteps 542720, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0023
                action_loss  -0.0070
                    bc_loss   0.0000
               dist_entropy   0.6672
         discriminator_loss   1.0189
                  gail_loss   1.0156
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0257
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0362 -0.0537 -0.0536 ... -0.0491 -0.047  -0.0465]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0362 -0.0537 -0.0536 ... -0.0491 -0.047  -0.0465]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 265, num timesteps 544768, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0012
                action_loss  -0.0130
                    bc_loss   0.0000
               dist_entropy   0.6727
         discriminator_loss   1.0238
                  gail_loss   1.0208
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0052
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.059  -0.0589 -0.0316 ... -0.0303 -0.0582 -0.0311]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.059  -0.0589 -0.0316 ... -0.0303 -0.0582 -0.0311]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 266, num timesteps 546816, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0203
                    bc_loss   0.0000
               dist_entropy   0.6872
         discriminator_loss   1.0177
                  gail_loss   1.0144
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0040
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0451 -0.0458 -0.0505 ... -0.0555 -0.0556 -0.0427]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0451 -0.0458 -0.0505 ... -0.0555 -0.0556 -0.0427]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 267, num timesteps 548864, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.6637
         discriminator_loss   1.0437
                  gail_loss   1.0408
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0502 -0.0445 -0.0512 ... -0.0428 -0.0438 -0.0421]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0502 -0.0445 -0.0512 ... -0.0428 -0.0438 -0.0421]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 268, num timesteps 550912, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0044
                    bc_loss   0.0000
               dist_entropy   0.6713
         discriminator_loss   1.0345
                  gail_loss   1.0315
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0126
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0393 -0.0407 -0.0423 ... -0.0454 -0.0543 -0.0543]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0393 -0.0407 -0.0423 ... -0.0454 -0.0543 -0.0543]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 269, num timesteps 552960, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0071
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6620
         discriminator_loss   1.0377
                  gail_loss   1.0346
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0141
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0491 -0.049  -0.0486 ... -0.0548 -0.0542 -0.0534]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0491 -0.049  -0.0486 ... -0.0548 -0.0542 -0.0534]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 270, num timesteps 555008, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0062
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6618
         discriminator_loss   1.0353
                  gail_loss   1.0324
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0176
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0418 -0.0466 -0.0395 ... -0.0482 -0.0484 -0.0486]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0418 -0.0466 -0.0395 ... -0.0482 -0.0484 -0.0486]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 271, num timesteps 557056, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0057
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.6430
         discriminator_loss   1.0299
                  gail_loss   1.0268
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0208
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0521 -0.0519 -0.0431 ... -0.0542 -0.0376 -0.0357]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0521 -0.0519 -0.0431 ... -0.0542 -0.0376 -0.0357]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 272, num timesteps 559104, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0066
                action_loss  -0.0075
                    bc_loss   0.0000
               dist_entropy   0.6562
         discriminator_loss   1.0452
                  gail_loss   1.0424
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0167
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0412 -0.0506 -0.0381 ... -0.0272 -0.0374 -0.0371]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0412 -0.0506 -0.0381 ... -0.0272 -0.0374 -0.0371]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 273, num timesteps 561152, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0076
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6415
         discriminator_loss   1.0321
                  gail_loss   1.0291
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0331
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0324 -0.0323 -0.0321 ... -0.0397 -0.0388 -0.0381]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0324 -0.0323 -0.0321 ... -0.0397 -0.0388 -0.0381]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 274, num timesteps 563200, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss  -0.0067
                    bc_loss   0.0000
               dist_entropy   0.6357
         discriminator_loss   1.0310
                  gail_loss   1.0282
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0286
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0567 -0.0567 -0.0567 ... -0.0441 -0.045  -0.0473]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0567 -0.0567 -0.0567 ... -0.0441 -0.045  -0.0473]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 275, num timesteps 565248, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6453
         discriminator_loss   1.0169
                  gail_loss   1.0137
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0223
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0515 -0.0412 -0.0424 ... -0.047  -0.0476 -0.0482]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0515 -0.0412 -0.0424 ... -0.047  -0.0476 -0.0482]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 276, num timesteps 567296, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6222
         discriminator_loss   1.0294
                  gail_loss   1.0265
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0095
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0443 -0.0448 -0.0503 ... -0.0439 -0.0441 -0.0444]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0443 -0.0448 -0.0503 ... -0.0439 -0.0441 -0.0444]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 277, num timesteps 569344, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0094
                action_loss  -0.0082
                    bc_loss   0.0000
               dist_entropy   0.5628
         discriminator_loss   1.0462
                  gail_loss   1.0433
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0230
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0399 -0.0404 -0.026  ... -0.0421 -0.0411 -0.0266]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0399 -0.0404 -0.026  ... -0.0421 -0.0411 -0.0266]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 278, num timesteps 571392, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0080
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6033
         discriminator_loss   1.0412
                  gail_loss   1.0382
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0260
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0412 -0.0406 -0.0399 ... -0.0425 -0.0435 -0.0445]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0412 -0.0406 -0.0399 ... -0.0425 -0.0435 -0.0445]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 279, num timesteps 573440, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0077
                action_loss  -0.0048
                    bc_loss   0.0000
               dist_entropy   0.5911
         discriminator_loss   1.0421
                  gail_loss   1.0395
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0325
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.034  -0.035  -0.0462 ... -0.064  -0.0643 -0.064 ]]
 task_rewards:  [[ 0.0000e+00  0.0000e+00  0.0000e+00 ...  0.0000e+00  0.0000e+00
  -3.7253e-09]]
 final_rewards:  [[-0.034  -0.035  -0.0462 ... -0.064  -0.0643 -0.064 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 280, num timesteps 575488, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0121
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.5797
         discriminator_loss   1.0345
                  gail_loss   1.0313
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0357
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0536 -0.0522 -0.0503 ... -0.0425 -0.0526 -0.0429]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0536 -0.0522 -0.0503 ... -0.0425 -0.0526 -0.0429]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 281, num timesteps 577536, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0051
                action_loss  -0.0061
                    bc_loss   0.0000
               dist_entropy   0.6099
         discriminator_loss   1.0344
                  gail_loss   1.0318
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0351
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0465 -0.0463 -0.0463 ... -0.0336 -0.0364 -0.0439]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0465 -0.0463 -0.0463 ... -0.0336 -0.0364 -0.0439]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 282, num timesteps 579584, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0073
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.5993
         discriminator_loss   1.0344
                  gail_loss   1.0312
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0226
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0521 -0.0527 -0.0532 ... -0.0544 -0.0456 -0.0545]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0521 -0.0527 -0.0532 ... -0.0544 -0.0456 -0.0545]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 283, num timesteps 581632, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0105
                action_loss  -0.0055
                    bc_loss   0.0000
               dist_entropy   0.5930
         discriminator_loss   1.0316
                  gail_loss   1.0287
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0418
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0518 -0.0518 -0.0423 ... -0.0368 -0.0373 -0.0564]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0518 -0.0518 -0.0423 ... -0.0368 -0.0373 -0.0564]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 284, num timesteps 583680, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0076
                action_loss  -0.0073
                    bc_loss   0.0000
               dist_entropy   0.5831
         discriminator_loss   1.0327
                  gail_loss   1.0296
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0525
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0496 -0.0503 -0.0431 ... -0.0511 -0.0509 -0.0508]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0496 -0.0503 -0.0431 ... -0.0511 -0.0509 -0.0508]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 285, num timesteps 585728, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0118
                    bc_loss   0.0000
               dist_entropy   0.6177
         discriminator_loss   1.0325
                  gail_loss   1.0296
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0130
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0525 -0.0389 -0.0525 ... -0.0553 -0.0343 -0.0548]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0525 -0.0389 -0.0525 ... -0.0553 -0.0343 -0.0548]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 286, num timesteps 587776, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0019
                action_loss  -0.0110
                    bc_loss   0.0000
               dist_entropy   0.6109
         discriminator_loss   1.0244
                  gail_loss   1.0214
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0132
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0476 -0.0487 -0.0499 ... -0.0454 -0.0456 -0.0458]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0476 -0.0487 -0.0499 ... -0.0454 -0.0456 -0.0458]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 287, num timesteps 589824, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0073
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.5724
         discriminator_loss   1.0348
                  gail_loss   1.0317
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0190
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0556 -0.0557 -0.0559 ... -0.049  -0.0512 -0.0533]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0556 -0.0557 -0.0559 ... -0.049  -0.0512 -0.0533]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 288, num timesteps 591872, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0057
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.5643
         discriminator_loss   1.0375
                  gail_loss   1.0346
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0342
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0493 -0.0506 -0.0511 ... -0.039  -0.0387 -0.0385]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0493 -0.0506 -0.0511 ... -0.039  -0.0387 -0.0385]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 289, num timesteps 593920, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0071
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.5346
         discriminator_loss   1.0379
                  gail_loss   1.0351
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0301
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0447 -0.0446 -0.0446 ... -0.0492 -0.0454 -0.0447]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0447 -0.0446 -0.0446 ... -0.0492 -0.0454 -0.0447]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 290, num timesteps 595968, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0135
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.5375
         discriminator_loss   1.0384
                  gail_loss   1.0351
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0498
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0558 -0.053  -0.0498 ... -0.0552 -0.0553 -0.0374]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0558 -0.053  -0.0498 ... -0.0552 -0.0553 -0.0374]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 291, num timesteps 598016, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0090
                action_loss  -0.0029
                    bc_loss   0.0000
               dist_entropy   0.5355
         discriminator_loss   1.0321
                  gail_loss   1.0290
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0456
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0494 -0.0448 -0.0497 ... -0.0541 -0.0539 -0.0537]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0494 -0.0448 -0.0497 ... -0.0541 -0.0539 -0.0537]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 292, num timesteps 600064, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0055
                    bc_loss   0.0000
               dist_entropy   0.5714
         discriminator_loss   1.0254
                  gail_loss   1.0222
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0310
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0357 -0.0355 -0.0486 ... -0.0402 -0.0541 -0.0543]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0357 -0.0355 -0.0486 ... -0.0402 -0.0541 -0.0543]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 293, num timesteps 602112, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0071
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.4919
         discriminator_loss   1.0343
                  gail_loss   1.0311
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0202
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0437 -0.0493 -0.0492 ... -0.039  -0.0396 -0.0402]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0437 -0.0493 -0.0492 ... -0.039  -0.0396 -0.0402]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 294, num timesteps 604160, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0019
                action_loss  -0.0059
                    bc_loss   0.0000
               dist_entropy   0.5747
         discriminator_loss   1.0216
                  gail_loss   1.0186
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0157
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0523 -0.0529 -0.0536 ... -0.0591 -0.0293 -0.0591]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0523 -0.0529 -0.0536 ... -0.0591 -0.0293 -0.0591]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 295, num timesteps 606208, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0059
                    bc_loss   0.0000
               dist_entropy   0.5240
         discriminator_loss   1.0252
                  gail_loss   1.0221
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0115
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0457 -0.0457 -0.0445 ... -0.035  -0.0413 -0.0418]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0457 -0.0457 -0.0445 ... -0.035  -0.0413 -0.0418]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 296, num timesteps 608256, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss  -0.0059
                    bc_loss   0.0000
               dist_entropy   0.5317
         discriminator_loss   1.0260
                  gail_loss   1.0227
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0147
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0341 -0.053  -0.0536 ... -0.0528 -0.0546 -0.0553]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0341 -0.053  -0.0536 ... -0.0528 -0.0546 -0.0553]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 297, num timesteps 610304, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0102
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.4886
         discriminator_loss   1.0333
                  gail_loss   1.0301
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0228
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0486 -0.0467 -0.0441 ... -0.0482 -0.0483 -0.0448]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0486 -0.0467 -0.0441 ... -0.0482 -0.0483 -0.0448]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 298, num timesteps 612352, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0073
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.5158
         discriminator_loss   1.0284
                  gail_loss   1.0255
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0327
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0393 -0.0578 -0.0578 ... -0.0591 -0.0589 -0.0586]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0393 -0.0578 -0.0578 ... -0.0591 -0.0589 -0.0586]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 299, num timesteps 614400, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0065
                action_loss  -0.0059
                    bc_loss   0.0000
               dist_entropy   0.4678
         discriminator_loss   1.0224
                  gail_loss   1.0192
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0310
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0536 -0.04   -0.04   ... -0.0572 -0.0572 -0.0572]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0536 -0.04   -0.04   ... -0.0572 -0.0572 -0.0572]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 300, num timesteps 616448, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0041
                    bc_loss   0.0000
               dist_entropy   0.5082
         discriminator_loss   1.0257
                  gail_loss   1.0224
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0223
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0469 -0.0469 -0.0469 ... -0.0546 -0.0546 -0.0545]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0469 -0.0469 -0.0469 ... -0.0546 -0.0546 -0.0545]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 301, num timesteps 618496, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.4473
         discriminator_loss   1.0346
                  gail_loss   1.0315
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0123
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0403 -0.0411 -0.0421 ... -0.0369 -0.0502 -0.0377]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0403 -0.0411 -0.0421 ... -0.0369 -0.0502 -0.0377]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 302, num timesteps 620544, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0046
                action_loss  -0.0076
                    bc_loss   0.0000
               dist_entropy   0.5382
         discriminator_loss   1.0346
                  gail_loss   1.0315
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0167
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0534 -0.0539 -0.0545 ... -0.0386 -0.0389 -0.039 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0534 -0.0539 -0.0545 ... -0.0386 -0.0389 -0.039 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 303, num timesteps 622592, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0075
                    bc_loss   0.0000
               dist_entropy   0.5265
         discriminator_loss   1.0189
                  gail_loss   1.0154
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0178
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0534 -0.0384 -0.053  ... -0.0485 -0.0488 -0.049 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0534 -0.0384 -0.053  ... -0.0485 -0.0488 -0.049 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 304, num timesteps 624640, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.5541
         discriminator_loss   1.0192
                  gail_loss   1.0162
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0154
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0521 -0.0525 -0.0429 ... -0.0548 -0.0545 -0.0539]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0521 -0.0525 -0.0429 ... -0.0548 -0.0545 -0.0539]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 305, num timesteps 626688, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0049
                action_loss  -0.0081
                    bc_loss   0.0000
               dist_entropy   0.5421
         discriminator_loss   1.0295
                  gail_loss   1.0263
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0092
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0403 -0.0391 -0.0379 ... -0.0323 -0.0322 -0.0322]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0403 -0.0391 -0.0379 ... -0.0323 -0.0322 -0.0322]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 306, num timesteps 628736, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0067
                    bc_loss   0.0000
               dist_entropy   0.5191
         discriminator_loss   1.0329
                  gail_loss   1.0297
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0186
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0189 -0.0191 -0.0195 ... -0.0438 -0.0374 -0.0467]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0189 -0.0191 -0.0195 ... -0.0438 -0.0374 -0.0467]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 307, num timesteps 630784, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.5332
         discriminator_loss   1.0280
                  gail_loss   1.0247
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0124
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0308 -0.0335 -0.0547 ... -0.0567 -0.0567 -0.0303]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0308 -0.0335 -0.0547 ... -0.0567 -0.0567 -0.0303]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 308, num timesteps 632832, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0077
                    bc_loss   0.0000
               dist_entropy   0.5668
         discriminator_loss   1.0344
                  gail_loss   1.0313
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0146
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0426 -0.0427 -0.0424 ... -0.056  -0.0564 -0.0332]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0426 -0.0427 -0.0424 ... -0.056  -0.0564 -0.0332]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 309, num timesteps 634880, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0059
                    bc_loss   0.0000
               dist_entropy   0.5417
         discriminator_loss   1.0342
                  gail_loss   1.0311
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0162
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.051  -0.0513 -0.0424 ... -0.0502 -0.0507 -0.0512]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.051  -0.0513 -0.0424 ... -0.0502 -0.0507 -0.0512]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 310, num timesteps 636928, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.5772
         discriminator_loss   1.0196
                  gail_loss   1.0166
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0089
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0481 -0.0456 -0.0467 ... -0.0588 -0.0588 -0.033 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0481 -0.0456 -0.0467 ... -0.0588 -0.0588 -0.033 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 311, num timesteps 638976, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0052
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.5570
         discriminator_loss   1.0299
                  gail_loss   1.0269
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0115
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0437 -0.0499 -0.0441 ... -0.0447 -0.0441 -0.0435]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0437 -0.0499 -0.0441 ... -0.0447 -0.0441 -0.0435]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 312, num timesteps 641024, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0106
                    bc_loss   0.0000
               dist_entropy   0.5956
         discriminator_loss   1.0122
                  gail_loss   1.0089
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0157
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0552 -0.0546 -0.0541 ... -0.0528 -0.0522 -0.0516]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0552 -0.0546 -0.0541 ... -0.0528 -0.0522 -0.0516]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 313, num timesteps 643072, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0057
                action_loss  -0.0111
                    bc_loss   0.0000
               dist_entropy   0.5545
         discriminator_loss   1.0245
                  gail_loss   1.0213
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0124
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0266 -0.0393 -0.039  ... -0.0528 -0.0525 -0.0521]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0266 -0.0393 -0.039  ... -0.0528 -0.0525 -0.0521]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 314, num timesteps 645120, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.5293
         discriminator_loss   1.0376
                  gail_loss   1.0343
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0289
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0563 -0.0355 -0.0557 ... -0.0372 -0.039  -0.0402]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0563 -0.0355 -0.0557 ... -0.0372 -0.039  -0.0402]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 315, num timesteps 647168, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0057
                action_loss  -0.0137
                    bc_loss   0.0000
               dist_entropy   0.5710
         discriminator_loss   1.0250
                  gail_loss   1.0219
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0208
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0578 -0.0573 -0.042  ... -0.0451 -0.0541 -0.0544]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0578 -0.0573 -0.042  ... -0.0451 -0.0541 -0.0544]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 316, num timesteps 649216, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0120
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.5112
         discriminator_loss   1.0408
                  gail_loss   1.0380
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0305 -0.0587 -0.059  ... -0.0622 -0.0622 -0.0622]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0305 -0.0587 -0.059  ... -0.0622 -0.0622 -0.0622]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 317, num timesteps 651264, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0176
                    bc_loss   0.0000
               dist_entropy   0.5913
         discriminator_loss   1.0191
                  gail_loss   1.0159
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0313
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0532 -0.0531 -0.053  ... -0.0375 -0.0386 -0.0227]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0532 -0.0531 -0.053  ... -0.0375 -0.0386 -0.0227]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 318, num timesteps 653312, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0032
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.5831
         discriminator_loss   1.0244
                  gail_loss   1.0211
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0094
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0355 -0.0269 -0.0389 ... -0.044  -0.048  -0.0454]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0355 -0.0269 -0.0389 ... -0.044  -0.048  -0.0454]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 319, num timesteps 655360, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0042
                    bc_loss   0.0000
               dist_entropy   0.6076
         discriminator_loss   1.0250
                  gail_loss   1.0216
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0098
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.041  -0.0568 -0.0428 ... -0.05   -0.0491 -0.0504]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.041  -0.0568 -0.0428 ... -0.05   -0.0491 -0.0504]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 320, num timesteps 657408, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.5575
         discriminator_loss   1.0317
                  gail_loss   1.0283
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0180
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0547 -0.035  -0.0542 ... -0.0418 -0.0414 -0.0477]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0547 -0.035  -0.0542 ... -0.0418 -0.0414 -0.0477]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 321, num timesteps 659456, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0149
                    bc_loss   0.0000
               dist_entropy   0.5826
         discriminator_loss   1.0426
                  gail_loss   1.0395
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0551 -0.0414 -0.0535 ... -0.0495 -0.0503 -0.0512]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0551 -0.0414 -0.0535 ... -0.0495 -0.0503 -0.0512]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 322, num timesteps 661504, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6376
         discriminator_loss   1.0219
                  gail_loss   1.0188
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0130
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.045  -0.0461 -0.0471 ... -0.0286 -0.0289 -0.0293]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.045  -0.0461 -0.0471 ... -0.0286 -0.0289 -0.0293]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 323, num timesteps 663552, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6087
         discriminator_loss   1.0461
                  gail_loss   1.0433
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0107
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0335 -0.0341 -0.0349 ... -0.0487 -0.048  -0.0474]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0335 -0.0341 -0.0349 ... -0.0487 -0.048  -0.0474]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 324, num timesteps 665600, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0255
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.5891
         discriminator_loss   1.0505
                  gail_loss   1.0477
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0623
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0364 -0.0365 -0.0367 ... -0.0358 -0.0363 -0.0598]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0364 -0.0365 -0.0367 ... -0.0358 -0.0363 -0.0598]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 325, num timesteps 667648, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0092
                    bc_loss   0.0000
               dist_entropy   0.6046
         discriminator_loss   1.0249
                  gail_loss   1.0220
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.1139
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0471 -0.0468 -0.047  ... -0.0544 -0.0542 -0.0328]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0471 -0.0468 -0.047  ... -0.0544 -0.0542 -0.0328]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 326, num timesteps 669696, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.6260
         discriminator_loss   1.0394
                  gail_loss   1.0363
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0164
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0491 -0.0391 -0.0482 ... -0.0461 -0.0458 -0.047 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0491 -0.0391 -0.0482 ... -0.0461 -0.0458 -0.047 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 327, num timesteps 671744, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0048
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.6242
         discriminator_loss   1.0243
                  gail_loss   1.0214
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0185
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0552 -0.0405 -0.0547 ... -0.0601 -0.0594 -0.0586]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0552 -0.0405 -0.0547 ... -0.0601 -0.0594 -0.0586]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 328, num timesteps 673792, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss  -0.0180
                    bc_loss   0.0000
               dist_entropy   0.6399
         discriminator_loss   1.0167
                  gail_loss   1.0131
                  grad_loss   0.0036
                    ib_loss   0.0000
                  task_loss   0.0136
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0525 -0.0538 -0.055  ... -0.0442 -0.0439 -0.0435]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0525 -0.0538 -0.055  ... -0.0442 -0.0439 -0.0435]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 329, num timesteps 675840, FPS 110 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0082
                    bc_loss   0.0000
               dist_entropy   0.6324
         discriminator_loss   1.0338
                  gail_loss   1.0303
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0153
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0562 -0.0556 -0.0549 ... -0.0475 -0.0532 -0.0505]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0562 -0.0556 -0.0549 ... -0.0475 -0.0532 -0.0505]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 330, num timesteps 677888, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0143
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.5930
         discriminator_loss   1.0326
                  gail_loss   1.0295
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0266
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0426 -0.0407 -0.0539 ... -0.0301 -0.0288 -0.0276]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0426 -0.0407 -0.0539 ... -0.0301 -0.0288 -0.0276]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 331, num timesteps 679936, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0050
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.5951
         discriminator_loss   1.0355
                  gail_loss   1.0327
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0400
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.025  -0.0102 -0.0234 ... -0.0444 -0.0457 -0.0472]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.025  -0.0102 -0.0234 ... -0.0444 -0.0457 -0.0472]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 332, num timesteps 681984, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.5927
         discriminator_loss   1.0314
                  gail_loss   1.0283
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0180
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0403 -0.0418 -0.0546 ... -0.0431 -0.0546 -0.0544]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0403 -0.0418 -0.0546 ... -0.0431 -0.0546 -0.0544]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 333, num timesteps 684032, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0048
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.5880
         discriminator_loss   1.0336
                  gail_loss   1.0306
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0292
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0553 -0.0554 -0.0556 ... -0.0348 -0.0575 -0.0569]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0553 -0.0554 -0.0556 ... -0.0348 -0.0575 -0.0569]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 334, num timesteps 686080, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0091
                    bc_loss   0.0000
               dist_entropy   0.6312
         discriminator_loss   1.0208
                  gail_loss   1.0173
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0140
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.051  -0.0438 -0.0428 ... -0.0415 -0.0419 -0.0423]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.051  -0.0438 -0.0428 ... -0.0415 -0.0419 -0.0423]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 335, num timesteps 688128, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.6230
         discriminator_loss   1.0269
                  gail_loss   1.0237
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0114
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0341 -0.0247 -0.0356 ... -0.0392 -0.0548 -0.037 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0341 -0.0247 -0.0356 ... -0.0392 -0.0548 -0.037 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 336, num timesteps 690176, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0116
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6048
         discriminator_loss   1.0379
                  gail_loss   1.0349
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0412 -0.052  -0.0516 ... -0.0514 -0.0505 -0.0495]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0412 -0.052  -0.0516 ... -0.0514 -0.0505 -0.0495]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 337, num timesteps 692224, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0052
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.5929
         discriminator_loss   1.0258
                  gail_loss   1.0228
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0461
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0512 -0.0275 -0.0491 ... -0.0445 -0.0441 -0.0436]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0512 -0.0275 -0.0491 ... -0.0445 -0.0441 -0.0436]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 338, num timesteps 694272, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.5924
         discriminator_loss   1.0383
                  gail_loss   1.0352
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0152
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0451 -0.0446 -0.0441 ... -0.0482 -0.0496 -0.051 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0451 -0.0446 -0.0441 ... -0.0482 -0.0496 -0.051 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 339, num timesteps 696320, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0046
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6151
         discriminator_loss   1.0309
                  gail_loss   1.0281
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0215
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0521 -0.0531 -0.0537 ... -0.045  -0.0441 -0.0433]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0521 -0.0531 -0.0537 ... -0.045  -0.0441 -0.0433]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 340, num timesteps 698368, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6030
         discriminator_loss   1.0263
                  gail_loss   1.0232
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0134
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0342 -0.0334 -0.0327 ... -0.0459 -0.044  -0.0462]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0342 -0.0334 -0.0327 ... -0.0459 -0.044  -0.0462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 341, num timesteps 700416, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0163
                    bc_loss   0.0000
               dist_entropy   0.6422
         discriminator_loss   1.0254
                  gail_loss   1.0223
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0086
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0611 -0.0344 -0.061  ... -0.0313 -0.0608 -0.0609]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0611 -0.0344 -0.061  ... -0.0313 -0.0608 -0.0609]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 342, num timesteps 702464, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0070
                action_loss  -0.0043
                    bc_loss   0.0000
               dist_entropy   0.6079
         discriminator_loss   1.0152
                  gail_loss   1.0118
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0178
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0425 -0.0561 -0.0561 ... -0.0538 -0.0538 -0.0538]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0425 -0.0561 -0.0561 ... -0.0538 -0.0538 -0.0538]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 343, num timesteps 704512, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0083
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.5736
         discriminator_loss   1.0298
                  gail_loss   1.0269
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0313
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0461 -0.0501 -0.0501 ... -0.042  -0.0425 -0.043 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0461 -0.0501 -0.0501 ... -0.042  -0.0425 -0.043 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 344, num timesteps 706560, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0052
                action_loss  -0.0086
                    bc_loss   0.0000
               dist_entropy   0.5884
         discriminator_loss   1.0210
                  gail_loss   1.0180
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0241
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.048  -0.0485 -0.0491 ... -0.0357 -0.037  -0.0382]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.048  -0.0485 -0.0491 ... -0.0357 -0.037  -0.0382]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 345, num timesteps 708608, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.5927
         discriminator_loss   1.0436
                  gail_loss   1.0406
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0133
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0556 -0.0566 -0.0575 ... -0.0556 -0.055  -0.0544]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0556 -0.0566 -0.0575 ... -0.0556 -0.055  -0.0544]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 346, num timesteps 710656, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0147
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.5466
         discriminator_loss   1.0450
                  gail_loss   1.0420
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0382
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0581 -0.0575 -0.0569 ... -0.0277 -0.0619 -0.0241]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0581 -0.0575 -0.0569 ... -0.0277 -0.0619 -0.0241]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 347, num timesteps 712704, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0153
                    bc_loss   0.0000
               dist_entropy   0.5851
         discriminator_loss   1.0199
                  gail_loss   1.0169
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0449
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0436 -0.0422 -0.0406 ... -0.0457 -0.046  -0.0463]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0436 -0.0422 -0.0406 ... -0.0457 -0.046  -0.0463]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 348, num timesteps 714752, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.5891
         discriminator_loss   1.0345
                  gail_loss   1.0311
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0126
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.059  -0.0301 -0.0593 ... -0.0397 -0.0405 -0.0547]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.059  -0.0301 -0.0593 ... -0.0397 -0.0405 -0.0547]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 349, num timesteps 716800, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss  -0.0141
                    bc_loss   0.0000
               dist_entropy   0.6127
         discriminator_loss   1.0180
                  gail_loss   1.0149
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0077
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0558 -0.0559 -0.056  ... -0.0555 -0.0551 -0.0549]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0558 -0.0559 -0.056  ... -0.0555 -0.0551 -0.0549]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 350, num timesteps 718848, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0066
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.5716
         discriminator_loss   1.0304
                  gail_loss   1.0269
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0143
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.048  -0.0483 -0.0482 ... -0.0515 -0.0509 -0.0356]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.048  -0.0483 -0.0482 ... -0.0515 -0.0509 -0.0356]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 351, num timesteps 720896, FPS 111 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.6187
         discriminator_loss   1.0300
                  gail_loss   1.0270
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0186
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0566 -0.0292 -0.028  ... -0.0441 -0.0446 -0.0452]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0566 -0.0292 -0.028  ... -0.0441 -0.0446 -0.0452]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 352, num timesteps 722944, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0076
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.5725
         discriminator_loss   1.0357
                  gail_loss   1.0327
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0198
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0535 -0.0542 -0.0549 ... -0.0345 -0.034  -0.0335]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0535 -0.0542 -0.0549 ... -0.0345 -0.034  -0.0335]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 353, num timesteps 724992, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0028
                action_loss  -0.0101
                    bc_loss   0.0000
               dist_entropy   0.6162
         discriminator_loss   1.0231
                  gail_loss   1.0198
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0212
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0449 -0.0446 -0.0443 ... -0.0512 -0.0515 -0.0444]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0449 -0.0446 -0.0443 ... -0.0512 -0.0515 -0.0444]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 354, num timesteps 727040, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0112
                    bc_loss   0.0000
               dist_entropy   0.6252
         discriminator_loss   1.0216
                  gail_loss   1.0186
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0090
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0547 -0.0544 -0.0541 ... -0.0432 -0.0415 -0.0564]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0547 -0.0544 -0.0541 ... -0.0432 -0.0415 -0.0564]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 355, num timesteps 729088, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0041
                    bc_loss   0.0000
               dist_entropy   0.5960
         discriminator_loss   1.0294
                  gail_loss   1.0263
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0121
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0484 -0.0483 -0.0478 ... -0.0595 -0.0606 -0.0614]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0484 -0.0483 -0.0478 ... -0.0595 -0.0606 -0.0614]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 356, num timesteps 731136, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0069
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.5840
         discriminator_loss   1.0194
                  gail_loss   1.0160
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0325
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0447 -0.045  -0.045  ... -0.0332 -0.0323 -0.0314]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0447 -0.045  -0.045  ... -0.0332 -0.0323 -0.0314]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 357, num timesteps 733184, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0041
                action_loss  -0.0024
                    bc_loss   0.0000
               dist_entropy   0.6004
         discriminator_loss   1.0340
                  gail_loss   1.0308
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0195
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0416 -0.0409 -0.0172 ... -0.0484 -0.0457 -0.0487]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0416 -0.0409 -0.0172 ... -0.0484 -0.0457 -0.0487]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 358, num timesteps 735232, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0032
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.5968
         discriminator_loss   1.0335
                  gail_loss   1.0306
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0122
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0562 -0.0392 -0.0563 ... -0.0492 -0.0484 -0.0489]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0562 -0.0392 -0.0563 ... -0.0492 -0.0484 -0.0489]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 359, num timesteps 737280, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6107
         discriminator_loss   1.0215
                  gail_loss   1.0182
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0071
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0538 -0.0541 -0.0542 ... -0.0538 -0.0427 -0.0442]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0538 -0.0541 -0.0542 ... -0.0538 -0.0427 -0.0442]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 360, num timesteps 739328, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0023
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6160
         discriminator_loss   1.0301
                  gail_loss   1.0268
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0111
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0504 -0.0433 -0.0418 ... -0.0517 -0.0429 -0.0528]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0504 -0.0433 -0.0418 ... -0.0517 -0.0429 -0.0528]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 361, num timesteps 741376, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0016
                action_loss  -0.0064
                    bc_loss   0.0000
               dist_entropy   0.6295
         discriminator_loss   1.0243
                  gail_loss   1.0212
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0057
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0515 -0.0453 -0.0504 ... -0.0416 -0.0417 -0.0389]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0515 -0.0453 -0.0504 ... -0.0416 -0.0417 -0.0389]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 362, num timesteps 743424, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0021
                action_loss  -0.0093
                    bc_loss   0.0000
               dist_entropy   0.6301
         discriminator_loss   1.0263
                  gail_loss   1.0229
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0071
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0525 -0.0528 -0.033  ... -0.0481 -0.0489 -0.0497]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0525 -0.0528 -0.033  ... -0.0481 -0.0489 -0.0497]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 363, num timesteps 745472, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0039
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.5880
         discriminator_loss   1.0320
                  gail_loss   1.0289
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0087
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0563 -0.0571 -0.0579 ... -0.051  -0.0508 -0.0508]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0563 -0.0571 -0.0579 ... -0.051  -0.0508 -0.0508]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 364, num timesteps 747520, FPS 112 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0109
                    bc_loss   0.0000
               dist_entropy   0.5735
         discriminator_loss   1.0349
                  gail_loss   1.0319
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0128
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0439 -0.044  -0.0442 ... -0.0409 -0.0486 -0.0352]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0439 -0.044  -0.0442 ... -0.0409 -0.0486 -0.0352]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 365, num timesteps 749568, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0089
                action_loss  -0.0038
                    bc_loss   0.0000
               dist_entropy   0.5836
         discriminator_loss   1.0440
                  gail_loss   1.0415
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0234
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0269 -0.0511 -0.0221 ... -0.0347 -0.0583 -0.0583]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0269 -0.0511 -0.0221 ... -0.0347 -0.0583 -0.0583]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 366, num timesteps 751616, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss  -0.0062
                    bc_loss   0.0000
               dist_entropy   0.6151
         discriminator_loss   1.0144
                  gail_loss   1.0110
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0245
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0458 -0.0438 -0.0439 ... -0.0449 -0.0489 -0.045 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0458 -0.0438 -0.0439 ... -0.0449 -0.0489 -0.045 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 367, num timesteps 753664, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0027
                    bc_loss   0.0000
               dist_entropy   0.5788
         discriminator_loss   1.0359
                  gail_loss   1.0326
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0202
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0552 -0.0553 -0.0554 ... -0.0487 -0.0501 -0.0468]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0552 -0.0553 -0.0554 ... -0.0487 -0.0501 -0.0468]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 368, num timesteps 755712, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0034
                action_loss   0.0001
                    bc_loss   0.0000
               dist_entropy   0.5970
         discriminator_loss   1.0288
                  gail_loss   1.0260
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0192
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0464 -0.0451 -0.0518 ... -0.0507 -0.0507 -0.0422]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0464 -0.0451 -0.0518 ... -0.0507 -0.0507 -0.0422]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 369, num timesteps 757760, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0036
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.5907
         discriminator_loss   1.0296
                  gail_loss   1.0267
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0558 -0.04   -0.0404 ... -0.0551 -0.0546 -0.054 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0558 -0.04   -0.0404 ... -0.0551 -0.0546 -0.054 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 370, num timesteps 759808, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0016
                action_loss  -0.0098
                    bc_loss   0.0000
               dist_entropy   0.6315
         discriminator_loss   1.0138
                  gail_loss   1.0107
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0116
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0428 -0.042  -0.0382 ... -0.0538 -0.0524 -0.0195]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0428 -0.042  -0.0382 ... -0.0538 -0.0524 -0.0195]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 371, num timesteps 761856, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0040
                action_loss  -0.0118
                    bc_loss   0.0000
               dist_entropy   0.5894
         discriminator_loss   1.0278
                  gail_loss   1.0248
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0099
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0425 -0.0319 -0.0404 ... -0.039  -0.0378 -0.0366]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0425 -0.0319 -0.0404 ... -0.039  -0.0378 -0.0366]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 372, num timesteps 763904, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.5972
         discriminator_loss   1.0073
                  gail_loss   1.0039
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0110
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0402 -0.0393 -0.0132 ... -0.0313 -0.0328 -0.0373]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0402 -0.0393 -0.0132 ... -0.0313 -0.0328 -0.0373]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 373, num timesteps 765952, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0143
                    bc_loss   0.0000
               dist_entropy   0.6080
         discriminator_loss   1.0228
                  gail_loss   1.0195
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0088
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0551 -0.0567 -0.0582 ... -0.0222 -0.0227 -0.0234]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0551 -0.0567 -0.0582 ... -0.0222 -0.0227 -0.0234]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 374, num timesteps 768000, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.5407
         discriminator_loss   1.0234
                  gail_loss   1.0199
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0167
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0491 -0.049  -0.049  ... -0.0378 -0.0532 -0.039 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0491 -0.049  -0.049  ... -0.0378 -0.0532 -0.039 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 375, num timesteps 770048, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0067
                action_loss  -0.0047
                    bc_loss   0.0000
               dist_entropy   0.5501
         discriminator_loss   1.0376
                  gail_loss   1.0350
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0220
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0497 -0.0389 -0.039  ... -0.0572 -0.0362 -0.0579]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0497 -0.0389 -0.039  ... -0.0572 -0.0362 -0.0579]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 376, num timesteps 772096, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0092
                    bc_loss   0.0000
               dist_entropy   0.5919
         discriminator_loss   1.0199
                  gail_loss   1.0168
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0220
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0497 -0.0494 -0.0491 ... -0.0461 -0.0483 -0.0485]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0497 -0.0494 -0.0491 ... -0.0461 -0.0483 -0.0485]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 377, num timesteps 774144, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0029
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.5944
         discriminator_loss   1.0319
                  gail_loss   1.0287
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0139
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0418 -0.0408 -0.0398 ... -0.0553 -0.0565 -0.0575]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0418 -0.0408 -0.0398 ... -0.0553 -0.0565 -0.0575]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 378, num timesteps 776192, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0019
                    bc_loss   0.0000
               dist_entropy   0.5863
         discriminator_loss   1.0281
                  gail_loss   1.0248
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0117
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0559 -0.0564 -0.0564 ... -0.0531 -0.0529 -0.0528]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0559 -0.0564 -0.0564 ... -0.0531 -0.0529 -0.0528]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 379, num timesteps 778240, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0051
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.5649
         discriminator_loss   1.0262
                  gail_loss   1.0232
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0131
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0576 -0.0398 -0.0574 ... -0.0136 -0.04   -0.038 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0576 -0.0398 -0.0574 ... -0.0136 -0.04   -0.038 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 380, num timesteps 780288, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.5934
         discriminator_loss   1.0247
                  gail_loss   1.0218
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0215
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0246 -0.0227 -0.021  ... -0.0541 -0.0532 -0.0523]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0246 -0.0227 -0.021  ... -0.0541 -0.0532 -0.0523]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 381, num timesteps 782336, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0082
                    bc_loss   0.0000
               dist_entropy   0.5798
         discriminator_loss   1.0296
                  gail_loss   1.0266
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0233
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0421 -0.0414 -0.0579 ... -0.0482 -0.0514 -0.0465]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0421 -0.0414 -0.0579 ... -0.0482 -0.0514 -0.0465]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 382, num timesteps 784384, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0022
                    bc_loss   0.0000
               dist_entropy   0.6049
         discriminator_loss   1.0187
                  gail_loss   1.0156
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0174
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0514 -0.0488 -0.0498 ... -0.0528 -0.0435 -0.0426]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0514 -0.0488 -0.0498 ... -0.0528 -0.0435 -0.0426]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 383, num timesteps 786432, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.5794
         discriminator_loss   1.0288
                  gail_loss   1.0258
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0105
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0552 -0.0551 -0.0322 ... -0.0552 -0.0326 -0.0316]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0552 -0.0551 -0.0322 ... -0.0552 -0.0326 -0.0316]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 384, num timesteps 788480, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0028
                action_loss  -0.0048
                    bc_loss   0.0000
               dist_entropy   0.6229
         discriminator_loss   1.0313
                  gail_loss   1.0284
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0096
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0474 -0.0468 -0.0391 ... -0.0561 -0.0559 -0.0558]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0474 -0.0468 -0.0391 ... -0.0561 -0.0559 -0.0558]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 385, num timesteps 790528, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0051
                action_loss  -0.0038
                    bc_loss   0.0000
               dist_entropy   0.6055
         discriminator_loss   1.0292
                  gail_loss   1.0262
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0101
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0479 -0.0482 -0.0491 ... -0.0484 -0.0479 -0.0474]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0479 -0.0482 -0.0491 ... -0.0484 -0.0479 -0.0474]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 386, num timesteps 792576, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0011
                action_loss  -0.0033
                    bc_loss   0.0000
               dist_entropy   0.6294
         discriminator_loss   1.0160
                  gail_loss   1.0129
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0156
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0548 -0.029  -0.054  ... -0.0411 -0.0407 -0.0343]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0548 -0.029  -0.054  ... -0.0411 -0.0407 -0.0343]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 387, num timesteps 794624, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0080
                    bc_loss   0.0000
               dist_entropy   0.5879
         discriminator_loss   1.0281
                  gail_loss   1.0251
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0063
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0519 -0.0518 -0.0516 ... -0.038  -0.0376 -0.0373]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0519 -0.0518 -0.0516 ... -0.038  -0.0376 -0.0373]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 388, num timesteps 796672, FPS 113 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0101
                    bc_loss   0.0000
               dist_entropy   0.5908
         discriminator_loss   1.0337
                  gail_loss   1.0308
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0133
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0501 -0.0499 -0.0498 ... -0.044  -0.0437 -0.0369]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0501 -0.0499 -0.0498 ... -0.044  -0.0437 -0.0369]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 389, num timesteps 798720, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0042
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.5974
         discriminator_loss   1.0265
                  gail_loss   1.0232
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0116
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0325 -0.0511 -0.0512 ... -0.0456 -0.0459 -0.0462]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0325 -0.0511 -0.0512 ... -0.0456 -0.0459 -0.0462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 390, num timesteps 800768, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0070
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.5988
         discriminator_loss   1.0296
                  gail_loss   1.0267
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0160
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0408 -0.0329 -0.0419 ... -0.0544 -0.0545 -0.0547]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0408 -0.0329 -0.0419 ... -0.0544 -0.0545 -0.0547]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 391, num timesteps 802816, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0034
                    bc_loss   0.0000
               dist_entropy   0.6364
         discriminator_loss   1.0189
                  gail_loss   1.0160
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0226
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0474 -0.0494 -0.0477 ... -0.0447 -0.0534 -0.0532]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0474 -0.0494 -0.0477 ... -0.0447 -0.0534 -0.0532]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 392, num timesteps 804864, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0051
                    bc_loss   0.0000
               dist_entropy   0.6161
         discriminator_loss   1.0305
                  gail_loss   1.0270
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0088
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0418 -0.0535 -0.0417 ... -0.0453 -0.0455 -0.0456]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0418 -0.0535 -0.0417 ... -0.0453 -0.0455 -0.0456]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 393, num timesteps 806912, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0052
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.6218
         discriminator_loss   1.0362
                  gail_loss   1.0330
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0194
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0512 -0.0513 -0.0514 ... -0.0472 -0.0349 -0.0339]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0512 -0.0513 -0.0514 ... -0.0472 -0.0349 -0.0339]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 394, num timesteps 808960, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0028
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6469
         discriminator_loss   1.0303
                  gail_loss   1.0272
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0179
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0443 -0.0377 -0.0429 ... -0.0566 -0.0569 -0.0571]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0443 -0.0377 -0.0429 ... -0.0566 -0.0569 -0.0571]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 395, num timesteps 811008, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0018
                action_loss  -0.0052
                    bc_loss   0.0000
               dist_entropy   0.6455
         discriminator_loss   1.0175
                  gail_loss   1.0145
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0091
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0468 -0.0461 -0.0482 ... -0.0454 -0.0545 -0.042 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0468 -0.0461 -0.0482 ... -0.0454 -0.0545 -0.042 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 396, num timesteps 813056, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss  -0.0066
                    bc_loss   0.0000
               dist_entropy   0.6421
         discriminator_loss   1.0188
                  gail_loss   1.0157
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0055
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0441 -0.0438 -0.047  ... -0.0528 -0.0546 -0.0563]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0441 -0.0438 -0.047  ... -0.0528 -0.0546 -0.0563]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 397, num timesteps 815104, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0032
                action_loss  -0.0082
                    bc_loss   0.0000
               dist_entropy   0.6447
         discriminator_loss   1.0169
                  gail_loss   1.0136
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0092
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0448 -0.047  -0.0469 ... -0.0574 -0.0552 -0.0273]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0448 -0.047  -0.0469 ... -0.0574 -0.0552 -0.0273]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 398, num timesteps 817152, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0060
                action_loss  -0.0046
                    bc_loss   0.0000
               dist_entropy   0.6245
         discriminator_loss   1.0258
                  gail_loss   1.0228
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0153
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0391 -0.0366 -0.0263 ... -0.0409 -0.0395 -0.0199]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0391 -0.0366 -0.0263 ... -0.0409 -0.0395 -0.0199]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 399, num timesteps 819200, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6347
         discriminator_loss   1.0309
                  gail_loss   1.0281
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0154
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0358 -0.0501 -0.0502 ... -0.0446 -0.0447 -0.0513]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0358 -0.0501 -0.0502 ... -0.0446 -0.0447 -0.0513]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 400, num timesteps 821248, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0087
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6177
         discriminator_loss   1.0493
                  gail_loss   1.0465
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0285
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0473 -0.056  -0.0558 ... -0.0504 -0.052  -0.0527]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0473 -0.056  -0.0558 ... -0.0504 -0.052  -0.0527]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 401, num timesteps 823296, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0184
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.5973
         discriminator_loss   1.0435
                  gail_loss   1.0408
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0494
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0453 -0.0574 -0.0573 ... -0.0426 -0.032  -0.0439]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0453 -0.0574 -0.0573 ... -0.0426 -0.032  -0.0439]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 402, num timesteps 825344, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0135
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.5875
         discriminator_loss   1.0362
                  gail_loss   1.0333
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0722
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0366 -0.037  -0.038  ... -0.044  -0.0468 -0.0448]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0366 -0.037  -0.038  ... -0.044  -0.0468 -0.0448]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 403, num timesteps 827392, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0132
                action_loss  -0.0044
                    bc_loss   0.0000
               dist_entropy   0.5918
         discriminator_loss   1.0340
                  gail_loss   1.0308
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0538
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0419 -0.0431 -0.0531 ... -0.0334 -0.0493 -0.0502]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0419 -0.0431 -0.0531 ... -0.0334 -0.0493 -0.0502]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 404, num timesteps 829440, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0025
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6487
         discriminator_loss   1.0189
                  gail_loss   1.0157
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0387
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0531 -0.0538 -0.0543 ... -0.0526 -0.0518 -0.0509]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0531 -0.0538 -0.0543 ... -0.0526 -0.0518 -0.0509]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 405, num timesteps 831488, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0009
                    bc_loss   0.0000
               dist_entropy   0.6182
         discriminator_loss   1.0299
                  gail_loss   1.0272
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0224
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.056  -0.055  -0.0541 ... -0.0284 -0.0285 -0.0288]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.056  -0.055  -0.0541 ... -0.0284 -0.0285 -0.0288]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 406, num timesteps 833536, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0051
                action_loss  -0.0121
                    bc_loss   0.0000
               dist_entropy   0.6154
         discriminator_loss   1.0321
                  gail_loss   1.0290
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0287
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0163 -0.0169 -0.0085 ... -0.0496 -0.0501 -0.046 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0163 -0.0169 -0.0085 ... -0.0496 -0.0501 -0.046 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 407, num timesteps 835584, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0052
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6344
         discriminator_loss   1.0316
                  gail_loss   1.0285
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0108
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.049  -0.0486 -0.0482 ... -0.0492 -0.0479 -0.0482]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.049  -0.0486 -0.0482 ... -0.0492 -0.0479 -0.0482]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 408, num timesteps 837632, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6295
         discriminator_loss   1.0299
                  gail_loss   1.0269
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0129
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0445 -0.0557 -0.0552 ... -0.0442 -0.0445 -0.0448]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0445 -0.0557 -0.0552 ... -0.0442 -0.0445 -0.0448]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 409, num timesteps 839680, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0062
                action_loss  -0.0023
                    bc_loss   0.0000
               dist_entropy   0.6320
         discriminator_loss   1.0229
                  gail_loss   1.0199
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0210
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0534 -0.0451 -0.0544 ... -0.0555 -0.0558 -0.056 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0534 -0.0451 -0.0544 ... -0.0555 -0.0558 -0.056 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 410, num timesteps 841728, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0083
                action_loss  -0.0021
                    bc_loss   0.0000
               dist_entropy   0.6183
         discriminator_loss   1.0405
                  gail_loss   1.0376
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0196
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0497 -0.0498 -0.0499 ... -0.0513 -0.0491 -0.0499]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0497 -0.0498 -0.0499 ... -0.0513 -0.0491 -0.0499]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 411, num timesteps 843776, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0074
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6343
         discriminator_loss   1.0243
                  gail_loss   1.0213
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0258
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0404 -0.0384 -0.0366 ... -0.0443 -0.0515 -0.0457]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0404 -0.0384 -0.0366 ... -0.0443 -0.0515 -0.0457]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 412, num timesteps 845824, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0067
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.6294
         discriminator_loss   1.0385
                  gail_loss   1.0356
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0243
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0589 -0.0585 -0.0581 ... -0.0537 -0.0559 -0.0578]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0589 -0.0585 -0.0581 ... -0.0537 -0.0559 -0.0578]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 413, num timesteps 847872, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0167
                action_loss  -0.0052
                    bc_loss   0.0000
               dist_entropy   0.6315
         discriminator_loss   1.0328
                  gail_loss   1.0297
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0457
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0499 -0.0475 -0.0502 ... -0.0497 -0.0485 -0.0507]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0499 -0.0475 -0.0502 ... -0.0497 -0.0485 -0.0507]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 414, num timesteps 849920, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0108
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6097
         discriminator_loss   1.0301
                  gail_loss   1.0269
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0813
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0403 -0.0393 -0.0385 ... -0.0503 -0.0502 -0.0501]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0403 -0.0393 -0.0385 ... -0.0503 -0.0502 -0.0501]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 415, num timesteps 851968, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0067
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6186
         discriminator_loss   1.0331
                  gail_loss   1.0303
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0291
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0391 -0.0494 -0.0494 ... -0.0536 -0.0409 -0.0537]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0391 -0.0494 -0.0494 ... -0.0536 -0.0409 -0.0537]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 416, num timesteps 854016, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6484
         discriminator_loss   1.0206
                  gail_loss   1.0176
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0212
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0461 -0.0505 -0.0468 ... -0.0316 -0.052  -0.0315]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0461 -0.0505 -0.0468 ... -0.0316 -0.052  -0.0315]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 417, num timesteps 856064, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0079
                action_loss  -0.0032
                    bc_loss   0.0000
               dist_entropy   0.6137
         discriminator_loss   1.0221
                  gail_loss   1.0191
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0169
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0433 -0.0435 -0.0438 ... -0.0388 -0.0349 -0.0392]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0433 -0.0435 -0.0438 ... -0.0388 -0.0349 -0.0392]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 418, num timesteps 858112, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0020
                    bc_loss   0.0000
               dist_entropy   0.6193
         discriminator_loss   1.0273
                  gail_loss   1.0245
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0272
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0298 -0.0448 -0.0455 ... -0.0299 -0.0414 -0.0277]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0298 -0.0448 -0.0455 ... -0.0299 -0.0414 -0.0277]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 419, num timesteps 860160, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0056
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6271
         discriminator_loss   1.0323
                  gail_loss   1.0293
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0242
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.044  -0.0432 -0.025  ... -0.0481 -0.05   -0.0518]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.044  -0.0432 -0.025  ... -0.0481 -0.05   -0.0518]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 420, num timesteps 862208, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0125
                action_loss  -0.0013
                    bc_loss   0.0000
               dist_entropy   0.5684
         discriminator_loss   1.0395
                  gail_loss   1.0366
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0335
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0557 -0.057  -0.0353 ... -0.0326 -0.0322 -0.0319]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0557 -0.057  -0.0353 ... -0.0326 -0.0322 -0.0319]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 421, num timesteps 864256, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0075
                action_loss  -0.0015
                    bc_loss   0.0000
               dist_entropy   0.5956
         discriminator_loss   1.0333
                  gail_loss   1.0307
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0266
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0248 -0.0248 -0.0249 ... -0.0451 -0.0298 -0.0452]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0248 -0.0248 -0.0249 ... -0.0451 -0.0298 -0.0452]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 422, num timesteps 866304, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6139
         discriminator_loss   1.0352
                  gail_loss   1.0322
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0278
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0356 -0.0359 -0.0362 ... -0.0574 -0.0571 -0.0567]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0356 -0.0359 -0.0362 ... -0.0574 -0.0571 -0.0567]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 423, num timesteps 868352, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0093
                    bc_loss   0.0000
               dist_entropy   0.6331
         discriminator_loss   1.0259
                  gail_loss   1.0231
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0148
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0507 -0.0476 -0.052  ... -0.0138 -0.0139 -0.0142]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0507 -0.0476 -0.052  ... -0.0138 -0.0139 -0.0142]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 424, num timesteps 870400, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0147
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.5788
         discriminator_loss   1.0383
                  gail_loss   1.0352
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0402
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0461 -0.0525 -0.0462 ... -0.0459 -0.046  -0.0462]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0461 -0.0525 -0.0462 ... -0.0459 -0.046  -0.0462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 425, num timesteps 872448, FPS 114 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0046
                action_loss  -0.0026
                    bc_loss   0.0000
               dist_entropy   0.6090
         discriminator_loss   1.0315
                  gail_loss   1.0286
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0557
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0338 -0.0341 -0.0386 ... -0.0192 -0.0535 -0.0528]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0338 -0.0341 -0.0386 ... -0.0192 -0.0535 -0.0528]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 426, num timesteps 874496, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0104
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.5835
         discriminator_loss   1.0371
                  gail_loss   1.0341
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0293
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0203 -0.0486 -0.0483 ... -0.0541 -0.0537 -0.0534]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0203 -0.0486 -0.0483 ... -0.0541 -0.0537 -0.0534]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 427, num timesteps 876544, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0059
                action_loss  -0.0089
                    bc_loss   0.0000
               dist_entropy   0.6242
         discriminator_loss   1.0247
                  gail_loss   1.0219
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0317
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0535 -0.0537 -0.0442 ... -0.0353 -0.0236 -0.032 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0535 -0.0537 -0.0442 ... -0.0353 -0.0236 -0.032 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 428, num timesteps 878592, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0028
                    bc_loss   0.0000
               dist_entropy   0.6113
         discriminator_loss   1.0306
                  gail_loss   1.0271
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0199
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0142 -0.0358 -0.0348 ... -0.0477 -0.0478 -0.0431]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0142 -0.0358 -0.0348 ... -0.0477 -0.0478 -0.0431]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 429, num timesteps 880640, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0024
                action_loss  -0.0039
                    bc_loss   0.0000
               dist_entropy   0.6279
         discriminator_loss   1.0259
                  gail_loss   1.0229
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0183
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.037  -0.0572 -0.0572 ... -0.0358 -0.035  -0.0344]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.037  -0.0572 -0.0572 ... -0.0358 -0.035  -0.0344]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 430, num timesteps 882688, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0066
                action_loss  -0.0038
                    bc_loss   0.0000
               dist_entropy   0.6181
         discriminator_loss   1.0328
                  gail_loss   1.0295
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0137
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0287 -0.0466 -0.0465 ... -0.0296 -0.0469 -0.0462]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0287 -0.0466 -0.0465 ... -0.0296 -0.0469 -0.0462]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 431, num timesteps 884736, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0022
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.6522
         discriminator_loss   1.0105
                  gail_loss   1.0074
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0213
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0505 -0.0498 -0.0492 ... -0.0382 -0.0393 -0.0143]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0505 -0.0498 -0.0492 ... -0.0382 -0.0393 -0.0143]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 432, num timesteps 886784, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0095
                action_loss  -0.0068
                    bc_loss   0.0000
               dist_entropy   0.6043
         discriminator_loss   1.0329
                  gail_loss   1.0303
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0213
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0391 -0.0407 -0.0424 ... -0.051  -0.051  -0.0511]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0391 -0.0407 -0.0424 ... -0.051  -0.051  -0.0511]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 433, num timesteps 888832, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0121
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.5856
         discriminator_loss   1.0454
                  gail_loss   1.0423
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0404
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0398 -0.0548 -0.0548 ... -0.0489 -0.0485 -0.0479]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0398 -0.0548 -0.0548 ... -0.0489 -0.0485 -0.0479]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 434, num timesteps 890880, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0064
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6299
         discriminator_loss   1.0312
                  gail_loss   1.0279
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0274
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0505 -0.0496 -0.0487 ... -0.0416 -0.0412 -0.041 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0505 -0.0496 -0.0487 ... -0.0416 -0.0412 -0.041 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 435, num timesteps 892928, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.6074
         discriminator_loss   1.0275
                  gail_loss   1.0245
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0310
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0532 -0.0532 -0.045  ... -0.0314 -0.0296 -0.0278]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0532 -0.0532 -0.045  ... -0.0314 -0.0296 -0.0278]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 436, num timesteps 894976, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0079
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.5998
         discriminator_loss   1.0279
                  gail_loss   1.0248
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0263
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0295 -0.0281 -0.0267 ... -0.0545 -0.0332 -0.0537]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0295 -0.0281 -0.0267 ... -0.0545 -0.0332 -0.0537]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 437, num timesteps 897024, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0058
                action_loss  -0.0016
                    bc_loss   0.0000
               dist_entropy   0.6199
         discriminator_loss   1.0259
                  gail_loss   1.0226
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0304
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0408 -0.0439 -0.0437 ... -0.0498 -0.0467 -0.0512]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0408 -0.0439 -0.0437 ... -0.0498 -0.0467 -0.0512]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 438, num timesteps 899072, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0078
                action_loss  -0.0018
                    bc_loss   0.0000
               dist_entropy   0.6128
         discriminator_loss   1.0287
                  gail_loss   1.0255
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0308
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0498 -0.0464 -0.0472 ... -0.0502 -0.0344 -0.05  ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0498 -0.0464 -0.0472 ... -0.0502 -0.0344 -0.05  ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 439, num timesteps 901120, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6277
         discriminator_loss   1.0313
                  gail_loss   1.0281
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0223
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0368 -0.037  -0.0484 ... -0.0518 -0.0523 -0.0399]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0368 -0.037  -0.0484 ... -0.0518 -0.0523 -0.0399]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 440, num timesteps 903168, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0015
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6488
         discriminator_loss   1.0229
                  gail_loss   1.0199
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0104
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0466 -0.048  -0.0495 ... -0.04   -0.0404 -0.0538]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0466 -0.048  -0.0495 ... -0.04   -0.0404 -0.0538]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 441, num timesteps 905216, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.6305
         discriminator_loss   1.0369
                  gail_loss   1.0339
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0082
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0451 -0.0551 -0.0554 ... -0.0395 -0.0579 -0.0578]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0451 -0.0551 -0.0554 ... -0.0395 -0.0579 -0.0578]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 442, num timesteps 907264, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0031
                action_loss  -0.0030
                    bc_loss   0.0000
               dist_entropy   0.6388
         discriminator_loss   1.0167
                  gail_loss   1.0132
                  grad_loss   0.0035
                    ib_loss   0.0000
                  task_loss   0.0117
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0558 -0.0557 -0.0483 ... -0.0548 -0.0548 -0.0476]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0558 -0.0557 -0.0483 ... -0.0548 -0.0548 -0.0476]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 443, num timesteps 909312, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0138
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.5920
         discriminator_loss   1.0324
                  gail_loss   1.0294
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0286
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0567 -0.0437 -0.044  ... -0.0561 -0.0562 -0.0564]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0567 -0.0437 -0.044  ... -0.0561 -0.0562 -0.0564]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 444, num timesteps 911360, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0097
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.5980
         discriminator_loss   1.0348
                  gail_loss   1.0320
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0519
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0514 -0.0516 -0.0422 ... -0.0351 -0.0352 -0.0355]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0514 -0.0516 -0.0422 ... -0.0351 -0.0352 -0.0355]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 445, num timesteps 913408, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0017
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6441
         discriminator_loss   1.0259
                  gail_loss   1.0230
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0300
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0517 -0.0521 -0.0525 ... -0.0512 -0.0508 -0.0505]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0517 -0.0521 -0.0525 ... -0.0512 -0.0508 -0.0505]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 446, num timesteps 915456, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0043
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6232
         discriminator_loss   1.0252
                  gail_loss   1.0223
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0083
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0363 -0.0476 -0.0474 ... -0.0333 -0.0465 -0.0368]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0363 -0.0476 -0.0474 ... -0.0333 -0.0465 -0.0368]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 447, num timesteps 917504, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0046
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6208
         discriminator_loss   1.0343
                  gail_loss   1.0314
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0160
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0484 -0.0497 -0.0506 ... -0.0416 -0.0549 -0.0414]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0484 -0.0497 -0.0506 ... -0.0416 -0.0549 -0.0414]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 448, num timesteps 919552, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0035
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6309
         discriminator_loss   1.0206
                  gail_loss   1.0178
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0149
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0468 -0.0469 -0.0454 ... -0.0434 -0.0436 -0.0437]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0468 -0.0469 -0.0454 ... -0.0434 -0.0436 -0.0437]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 449, num timesteps 921600, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0055
                action_loss  -0.0062
                    bc_loss   0.0000
               dist_entropy   0.6260
         discriminator_loss   1.0241
                  gail_loss   1.0210
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0140
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0514 -0.0516 -0.0519 ... -0.0585 -0.0435 -0.0438]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0514 -0.0516 -0.0519 ... -0.0585 -0.0435 -0.0438]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 450, num timesteps 923648, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0062
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6270
         discriminator_loss   1.0184
                  gail_loss   1.0153
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0259
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0406 -0.0574 -0.0416 ... -0.0439 -0.0442 -0.0473]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0406 -0.0574 -0.0416 ... -0.0439 -0.0442 -0.0473]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 451, num timesteps 925696, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0146
                action_loss  -0.0050
                    bc_loss   0.0000
               dist_entropy   0.5999
         discriminator_loss   1.0345
                  gail_loss   1.0318
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0399
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0536 -0.0537 -0.0417 ... -0.0527 -0.038  -0.0531]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0536 -0.0537 -0.0417 ... -0.0527 -0.038  -0.0531]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 452, num timesteps 927744, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0130
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.5831
         discriminator_loss   1.0452
                  gail_loss   1.0422
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0676
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0373 -0.0528 -0.0529 ... -0.0485 -0.0495 -0.0505]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0373 -0.0528 -0.0529 ... -0.0485 -0.0495 -0.0505]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 453, num timesteps 929792, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0019
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6345
         discriminator_loss   1.0224
                  gail_loss   1.0193
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0405
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0603 -0.0613 -0.062  ... -0.0493 -0.0482 -0.0471]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0603 -0.0613 -0.062  ... -0.0493 -0.0482 -0.0471]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 454, num timesteps 931840, FPS 115 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0086
                action_loss  -0.0031
                    bc_loss   0.0000
               dist_entropy   0.5792
         discriminator_loss   1.0259
                  gail_loss   1.0225
                  grad_loss   0.0033
                    ib_loss   0.0000
                  task_loss   0.0187
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.033  -0.0321 -0.0313 ... -0.0265 -0.0488 -0.0222]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.033  -0.0321 -0.0313 ... -0.0265 -0.0488 -0.0222]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 455, num timesteps 933888, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0142
                action_loss  -0.0017
                    bc_loss   0.0000
               dist_entropy   0.5958
         discriminator_loss   1.0345
                  gail_loss   1.0315
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0385
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0446 -0.0428 -0.0411 ... -0.041  -0.0409 -0.041 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0446 -0.0428 -0.0411 ... -0.041  -0.0409 -0.041 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 456, num timesteps 935936, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0072
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.6104
         discriminator_loss   1.0333
                  gail_loss   1.0305
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0418
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0395 -0.0397 -0.0211 ... -0.0187 -0.0184 -0.0182]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0395 -0.0397 -0.0211 ... -0.0187 -0.0184 -0.0182]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 457, num timesteps 937984, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0094
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6068
         discriminator_loss   1.0326
                  gail_loss   1.0297
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0360
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0217 -0.0219 -0.0222 ... -0.0491 -0.0487 -0.0418]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0217 -0.0219 -0.0222 ... -0.0491 -0.0487 -0.0418]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 458, num timesteps 940032, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0041
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6072
         discriminator_loss   1.0229
                  gail_loss   1.0197
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0361
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0447 -0.0443 -0.0439 ... -0.0531 -0.0528 -0.0524]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0447 -0.0443 -0.0439 ... -0.0531 -0.0528 -0.0524]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 459, num timesteps 942080, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0013
                action_loss  -0.0083
                    bc_loss   0.0000
               dist_entropy   0.6535
         discriminator_loss   1.0219
                  gail_loss   1.0188
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0109
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0372 -0.0425 -0.0423 ... -0.0364 -0.0412 -0.0405]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0372 -0.0425 -0.0423 ... -0.0364 -0.0412 -0.0405]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 460, num timesteps 944128, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0087
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6018
         discriminator_loss   1.0337
                  gail_loss   1.0303
                  grad_loss   0.0034
                    ib_loss   0.0000
                  task_loss   0.0201
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0434 -0.0342 -0.0421 ... -0.0431 -0.0441 -0.0451]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0434 -0.0342 -0.0421 ... -0.0431 -0.0441 -0.0451]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 461, num timesteps 946176, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0109
                action_loss  -0.0014
                    bc_loss   0.0000
               dist_entropy   0.5834
         discriminator_loss   1.0309
                  gail_loss   1.0278
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0202
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0242 -0.0456 -0.0471 ... -0.0296 -0.0411 -0.0416]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0242 -0.0456 -0.0471 ... -0.0296 -0.0411 -0.0416]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 462, num timesteps 948224, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0114
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6126
         discriminator_loss   1.0384
                  gail_loss   1.0354
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0531
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0444 -0.026  -0.0457 ... -0.0539 -0.0539 -0.0539]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0444 -0.026  -0.0457 ... -0.0539 -0.0539 -0.0539]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 463, num timesteps 950272, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6452
         discriminator_loss   1.0278
                  gail_loss   1.0249
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0424
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0557 -0.0556 -0.0555 ... -0.052  -0.0365 -0.0393]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0557 -0.0556 -0.0555 ... -0.052  -0.0365 -0.0393]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 464, num timesteps 952320, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6311
         discriminator_loss   1.0235
                  gail_loss   1.0207
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0210
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.043  -0.0461 -0.0492 ... -0.0389 -0.0228 -0.0367]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.043  -0.0461 -0.0492 ... -0.0389 -0.0228 -0.0367]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 465, num timesteps 954368, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0066
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6275
         discriminator_loss   1.0283
                  gail_loss   1.0251
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0234
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0341 -0.0333 -0.0325 ... -0.0518 -0.0411 -0.0418]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0341 -0.0333 -0.0325 ... -0.0518 -0.0411 -0.0418]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 466, num timesteps 956416, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0023
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6522
         discriminator_loss   1.0243
                  gail_loss   1.0215
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0219
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0552 -0.0439 -0.055  ... -0.0549 -0.0548 -0.0403]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0552 -0.0439 -0.055  ... -0.0549 -0.0548 -0.0403]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 467, num timesteps 958464, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0105
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6008
         discriminator_loss   1.0393
                  gail_loss   1.0362
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0279
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0522 -0.0407 -0.0521 ... -0.0413 -0.041  -0.0409]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0522 -0.0407 -0.0521 ... -0.0413 -0.041  -0.0409]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 468, num timesteps 960512, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6190
         discriminator_loss   1.0299
                  gail_loss   1.0268
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0289
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0427 -0.0427 -0.0427 ... -0.0493 -0.052  -0.0526]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0427 -0.0427 -0.0427 ... -0.0493 -0.052  -0.0526]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 469, num timesteps 962560, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0067
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6055
         discriminator_loss   1.0394
                  gail_loss   1.0366
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0218
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.056  -0.037  -0.0353 ... -0.0247 -0.0265 -0.0284]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.056  -0.037  -0.0353 ... -0.0247 -0.0265 -0.0284]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 470, num timesteps 964608, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0061
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6184
         discriminator_loss   1.0362
                  gail_loss   1.0335
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0249
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0295 -0.0316 -0.0338 ... -0.0461 -0.0469 -0.0475]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0295 -0.0316 -0.0338 ... -0.0461 -0.0469 -0.0475]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 471, num timesteps 966656, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0158
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6010
         discriminator_loss   1.0481
                  gail_loss   1.0455
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0417
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0493 -0.0493 -0.0489 ... -0.0432 -0.0425 -0.0419]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0493 -0.0493 -0.0489 ... -0.0432 -0.0425 -0.0419]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 472, num timesteps 968704, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0033
                action_loss  -0.0010
                    bc_loss   0.0000
               dist_entropy   0.6402
         discriminator_loss   1.0130
                  gail_loss   1.0101
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0542
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0419 -0.0415 -0.0523 ... -0.0395 -0.0382 -0.0369]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0419 -0.0415 -0.0523 ... -0.0395 -0.0382 -0.0369]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 473, num timesteps 970752, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0041
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6289
         discriminator_loss   1.0372
                  gail_loss   1.0345
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0116
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0386 -0.0375 -0.0364 ... -0.0446 -0.0465 -0.0485]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0386 -0.0375 -0.0364 ... -0.0446 -0.0465 -0.0485]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 474, num timesteps 972800, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0093
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.5917
         discriminator_loss   1.0394
                  gail_loss   1.0366
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0214
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0553 -0.0553 -0.0354 ... -0.0488 -0.0487 -0.0487]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0553 -0.0553 -0.0354 ... -0.0488 -0.0487 -0.0487]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 475, num timesteps 974848, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0068
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6162
         discriminator_loss   1.0416
                  gail_loss   1.0387
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0343
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0565 -0.0565 -0.043  ... -0.0276 -0.0495 -0.0495]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0565 -0.0565 -0.043  ... -0.0276 -0.0495 -0.0495]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 476, num timesteps 976896, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0084
                action_loss  -0.0004
                    bc_loss   0.0000
               dist_entropy   0.6343
         discriminator_loss   1.0245
                  gail_loss   1.0214
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0297
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0478 -0.0321 -0.0324 ... -0.0543 -0.0412 -0.0546]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0478 -0.0321 -0.0324 ... -0.0543 -0.0412 -0.0546]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 477, num timesteps 978944, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0107
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.5993
         discriminator_loss   1.0343
                  gail_loss   1.0316
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0385
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0496 -0.0496 -0.0497 ... -0.0458 -0.0364 -0.0477]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0496 -0.0496 -0.0497 ... -0.0458 -0.0364 -0.0477]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 478, num timesteps 980992, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0009
                action_loss  -0.0012
                    bc_loss   0.0000
               dist_entropy   0.6694
         discriminator_loss   1.0186
                  gail_loss   1.0155
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0367
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0581 -0.0587 -0.0591 ... -0.0583 -0.0453 -0.0473]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0581 -0.0587 -0.0591 ... -0.0583 -0.0453 -0.0473]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 479, num timesteps 983040, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0074
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6183
         discriminator_loss   1.0269
                  gail_loss   1.0240
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0177
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0494 -0.0503 -0.045  ... -0.0157 -0.0156 -0.0157]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0494 -0.0503 -0.045  ... -0.0157 -0.0156 -0.0157]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 480, num timesteps 985088, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0114
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6053
         discriminator_loss   1.0383
                  gail_loss   1.0353
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0320
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0137 -0.0141 -0.0147 ... -0.0303 -0.0438 -0.0438]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0137 -0.0141 -0.0147 ... -0.0303 -0.0438 -0.0438]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 481, num timesteps 987136, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0128
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6083
         discriminator_loss   1.0380
                  gail_loss   1.0351
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0399
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0402 -0.0404 -0.0407 ... -0.0366 -0.0378 -0.0392]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0402 -0.0404 -0.0407 ... -0.0366 -0.0378 -0.0392]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 482, num timesteps 989184, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0044
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6245
         discriminator_loss   1.0320
                  gail_loss   1.0292
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0437
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0517 -0.0451 -0.0531 ... -0.053  -0.0531 -0.0475]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0517 -0.0451 -0.0531 ... -0.053  -0.0531 -0.0475]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 483, num timesteps 991232, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0070
                action_loss  -0.0005
                    bc_loss   0.0000
               dist_entropy   0.6228
         discriminator_loss   1.0260
                  gail_loss   1.0231
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0216
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0565 -0.0564 -0.0563 ... -0.0385 -0.0381 -0.0488]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0565 -0.0564 -0.0563 ... -0.0385 -0.0381 -0.0488]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 484, num timesteps 993280, FPS 116 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0118
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6080
         discriminator_loss   1.0371
                  gail_loss   1.0340
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0452
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0485 -0.0483 -0.0481 ... -0.0524 -0.0523 -0.0523]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0485 -0.0483 -0.0481 ... -0.0524 -0.0523 -0.0523]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 485, num timesteps 995328, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0062
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6106
         discriminator_loss   1.0367
                  gail_loss   1.0339
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0488
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0449 -0.0485 -0.045  ... -0.0427 -0.0523 -0.0533]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0449 -0.0485 -0.045  ... -0.0427 -0.0523 -0.0533]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 486, num timesteps 997376, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0020
                action_loss  -0.0007
                    bc_loss   0.0000
               dist_entropy   0.6527
         discriminator_loss   1.0319
                  gail_loss   1.0289
                  grad_loss   0.0029
                    ib_loss   0.0000
                  task_loss   0.0180
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0513 -0.052  -0.0436 ... -0.0532 -0.0396 -0.0519]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0513 -0.052  -0.0436 ... -0.0532 -0.0396 -0.0519]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 487, num timesteps 999424, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0030
                action_loss   0.0000
                    bc_loss   0.0000
               dist_entropy   0.6383
         discriminator_loss   1.0317
                  gail_loss   1.0287
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0067
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0348 -0.0472 -0.0458 ... -0.0467 -0.0475 -0.0476]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0348 -0.0472 -0.0458 ... -0.0467 -0.0475 -0.0476]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 488, num timesteps 1001472, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0037
                action_loss  -0.0002
                    bc_loss   0.0000
               dist_entropy   0.6366
         discriminator_loss   1.0334
                  gail_loss   1.0304
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0106
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0518 -0.0522 -0.0451 ... -0.0391 -0.0468 -0.0468]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0518 -0.0522 -0.0451 ... -0.0391 -0.0468 -0.0468]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 489, num timesteps 1003520, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0080
                action_loss  -0.0011
                    bc_loss   0.0000
               dist_entropy   0.6303
         discriminator_loss   1.0374
                  gail_loss   1.0348
                  grad_loss   0.0026
                    ib_loss   0.0000
                  task_loss   0.0154
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0362 -0.0561 -0.0365 ... -0.0386 -0.0573 -0.0377]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0362 -0.0561 -0.0365 ... -0.0386 -0.0573 -0.0377]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 490, num timesteps 1005568, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0046
                action_loss  -0.0006
                    bc_loss   0.0000
               dist_entropy   0.6317
         discriminator_loss   1.0239
                  gail_loss   1.0209
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0222
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0527 -0.0525 -0.0339 ... -0.0502 -0.0494 -0.0476]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0527 -0.0525 -0.0339 ... -0.0502 -0.0494 -0.0476]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 491, num timesteps 1007616, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0027
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6374
         discriminator_loss   1.0302
                  gail_loss   1.0272
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0103
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0529 -0.0427 -0.0535 ... -0.0486 -0.048  -0.0478]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0529 -0.0427 -0.0535 ... -0.0486 -0.048  -0.0478]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 492, num timesteps 1009664, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0047
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6274
         discriminator_loss   1.0341
                  gail_loss   1.0309
                  grad_loss   0.0032
                    ib_loss   0.0000
                  task_loss   0.0100
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0505 -0.0505 -0.0506 ... -0.0461 -0.0516 -0.0514]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0505 -0.0505 -0.0506 ... -0.0461 -0.0516 -0.0514]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 493, num timesteps 1011712, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0066
                action_loss  -0.0003
                    bc_loss   0.0000
               dist_entropy   0.6140
         discriminator_loss   1.0290
                  gail_loss   1.0262
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0199
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0509 -0.0523 -0.0537 ... -0.0374 -0.0579 -0.0575]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0509 -0.0523 -0.0537 ... -0.0374 -0.0579 -0.0575]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 494, num timesteps 1013760, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0073
                action_loss  -0.0008
                    bc_loss   0.0000
               dist_entropy   0.6321
         discriminator_loss   1.0318
                  gail_loss   1.0290
                  grad_loss   0.0028
                    ib_loss   0.0000
                  task_loss   0.0175
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0499 -0.0509 -0.0488 ... -0.0571 -0.0402 -0.041 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0499 -0.0509 -0.0488 ... -0.0571 -0.0402 -0.041 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 495, num timesteps 1015808, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0101
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6136
         discriminator_loss   1.0381
                  gail_loss   1.0356
                  grad_loss   0.0025
                    ib_loss   0.0000
                  task_loss   0.0318
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0551 -0.0547 -0.0459 ... -0.0405 -0.0393 -0.0381]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0551 -0.0547 -0.0459 ... -0.0405 -0.0393 -0.0381]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 496, num timesteps 1017856, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0104
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6255
         discriminator_loss   1.0397
                  gail_loss   1.0367
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0469
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.022  -0.037  -0.0363 ... -0.0433 -0.0558 -0.043 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.022  -0.037  -0.0363 ... -0.0433 -0.0558 -0.043 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 497, num timesteps 1019904, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0140
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.5887
         discriminator_loss   1.0448
                  gail_loss   1.0421
                  grad_loss   0.0027
                    ib_loss   0.0000
                  task_loss   0.0518
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0555 -0.043  -0.0432 ... -0.0281 -0.0285 -0.029 ]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0555 -0.043  -0.0432 ... -0.0281 -0.0285 -0.029 ]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 498, num timesteps 1021952, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0088
                action_loss  -0.0001
                    bc_loss   0.0000
               dist_entropy   0.6200
         discriminator_loss   1.0210
                  gail_loss   1.0179
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0393
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.0274 -0.0281 -0.029  ... -0.0539 -0.0539 -0.0537]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.0274 -0.0281 -0.029  ... -0.0539 -0.0539 -0.0537]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
===============================================================================================
 Updates 499, num timesteps 1024000, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0038
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6501
         discriminator_loss   1.0272
                  gail_loss   1.0241
                  grad_loss   0.0030
                    ib_loss   0.0000
                  task_loss   0.0270
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
-----------------------------------------------------------------------------------------------
   raw_rewards:  [[-0.0395 -0.0395 -0.0395 ... -0.0395 -0.0395 -0.0395]]
 postr_rewards:  [[0.0395 0.0395 0.0395 ... 0.0395 0.0395 0.0395]]
 discr_rewards:  [[-0.038  -0.0538 -0.0538 ... -0.0479 -0.0363 -0.0362]]
 task_rewards:  [[0. 0. 0. ... 0. 0. 0.]]
 final_rewards:  [[-0.038  -0.0538 -0.0538 ... -0.0479 -0.0363 -0.0362]]
-----------------------------------------------------------------------------------------------
 Evaluation using 10 episodes: mean reward -400.00000
 Evaluation using 100 episodes: mean reward -400.00000
===============================================================================================
 Updates 500, num timesteps 1026048, FPS 117 
 Last 10 training episodes: mean/median reward -400.0/-400.0, min/max reward -400.0/-400.0
 Last rollout:   value_loss   0.0085
                action_loss  -0.0000
                    bc_loss   0.0000
               dist_entropy   0.6314
         discriminator_loss   1.0305
                  gail_loss   1.0275
                  grad_loss   0.0031
                    ib_loss   0.0000
                  task_loss   0.0223
                       beta   0.0000
             posterior_loss   0.0000
===============================================================================================
(multi-task) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ [K(multi-task) ]0;jinwoo@jinwoo-desktop: /media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-devjinwoo@jinwoo-desktop:/media/jinwoo/Disk/Research_linux/Workspace/Research/Compound_task/furniture-dev$ python main.py --env-name "MountainToyCar-v1" --algo ppo --use-gae --log-interval 1 --num-steps 20048 --num-processes 1 --lr 3e-4 --entropy-coef 0 --value-loss-coef 0.5 --num-mini-batch 32 --gamma 0.99 --gae-lambda 0.95 --num-env-steps 1026048 --use-linear-lr-decay --use-proper-time-limits --gail --extract-obs ---gail-algo standard --expert-algo ppo --use-latent --latent-dim 1 --hierarchical-policy --task-transition --posterior --save-date 200706 --eval-interval 1 --task-curiosity-reward --latent-space discrete[A[A[K
[K
[K[A[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C